<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-13T17:02:33-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nick Shu. A Fool in the Making</title><subtitle>Nick&apos;s Personal Website
</subtitle><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><entry><title type="html">Github and SSH Keys</title><link href="http://localhost:4000/github_ssh.html" rel="alternate" type="text/html" title="Github and SSH Keys" /><published>2022-01-13T00:00:00-05:00</published><updated>2022-01-13T00:00:00-05:00</updated><id>http://localhost:4000/github-ssh</id><content type="html" xml:base="http://localhost:4000/github_ssh.html"><![CDATA[<p>This is a guide to add a SSH key to your Github account.</p>

<p>A good guide is done by Antonio Medeiros <a href="https://linuxkamarada.com/en/2019/07/14/using-git-with-ssh-keys/">here</a> and <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent">here</a>.</p>

<h1 id="existing-ssh-keys">Existing SSH Keys</h1>

<p>First, check and see what are the current SSH keys. This can be done with</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-lah</span> ~/.ssh

total 28K
drwx------  2 nickshu nickshu 4.0K Dec 21 00:45 <span class="nb">.</span>
drwx------ 45 nickshu nickshu 4.0K Jan 13 15:00 ..
<span class="nt">-rw-------</span>  1 nickshu nickshu 4.2K Dec 29 21:14 known_hosts
<span class="nt">-rw-------</span>  1 nickshu nickshu 3.5K Dec 21 00:39 known_hosts.old
</code></pre></div></div>

<h1 id="generate-a-new-ssh-key">Generate a new SSH Key</h1>

<p>Next, you need to generate a new key. If you check the <code class="language-plaintext highlighter-rouge">man ssh-keygen</code>, you’ll see that the <code class="language-plaintext highlighter-rouge">-t</code> tag has 6 different types of keys you can choose from.</p>

<ul>
  <li>DSA</li>
  <li>EcDSA</li>
  <li>EcDSA-SK</li>
  <li>Ed25519</li>
  <li>Ed25519-SK</li>
  <li>RSA</li>
</ul>

<p>For more information on some of these types, you may visit https://goteleport.com/blog/comparing-ssh-keys/</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ssh-keygen <span class="nt">-t</span> ed25519 <span class="nt">-C</span> <span class="s2">"username@email.com"</span>
Generating public/private ed25519 key pair.
Enter file <span class="k">in </span>which to save the key <span class="o">(</span>/home/username/.ssh/id_ed25519<span class="o">)</span>: 
</code></pre></div></div>

<p>Here you may either enter a specific path for your key pair, or you may use the default location. Finally, you will 
be prompted to enter a password.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
</code></pre></div></div>

<p>This will create a key pair: a private and a public key on your desired location. The public key will have the extension <code class="language-plaintext highlighter-rouge">.pub</code>, whereas the private key will not have an extension. Do not share your private key.</p>

<h1 id="add-the-key-to-github">Add the Key to Github</h1>

<p>On Github, go to your <code class="language-plaintext highlighter-rouge">Settings</code> &gt; <code class="language-plaintext highlighter-rouge">SSH and GPG keys</code>, where you will see a list of your SSH keys.</p>

<p>Press on <code class="language-plaintext highlighter-rouge">New SSH key</code> and copy and paste your SSH public key (e.g. /home/username/.ssh/mykey.pub).</p>

<p>At this point, the key has been added to your Github account. Now you need to add it to the <code class="language-plaintext highlighter-rouge">ssh-agent</code>.</p>

<h1 id="add-your-ssh-key-to-the-ssh-agent">Add your SSH Key to the ssh-agent</h1>

<h2 id="one-time-use">One-Time Use</h2>

<p>Your SSH agent will help you so that you are not having to add your passphrase every time. First start the ssh-agent in the background.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>ssh-agent <span class="nt">-s</span><span class="si">)</span><span class="s2">"</span>
</code></pre></div></div>

<p>Next, add the SSH private key to the ssh-agent.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ssh-add ~/.ssh/path/to/ssh/private/key
</code></pre></div></div>

<h2 id="permanent-use">Permanent Use</h2>

<p>So, the easiest way to do so is to force the keys to be always kept. This can be done by adding to the <code class="language-plaintext highlighter-rouge">~/.ssh/config</code> file. If your file does not exist, then simply create it and add the private keys</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IdentityFile ~/.ssh/github_priv_key
IdentityFile ~/.ssh/server_priv_key
</code></pre></div></div>

<p>And then change the permissions to 600</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> ~/.ssh
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span>
...
<span class="nt">-rw-r--r--</span> 1 nickshu nickshu   58 Jan 13 16:01 config
...

<span class="nv">$ </span><span class="nb">chmod </span>600 ~/.ssh/config
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-la</span>
...
<span class="nt">-rw-------</span> 1 nickshu nickshu   58 Jan 13 16:01 config
...
</code></pre></div></div>

<p>Alternatively, if you’d like to map a specific key to a specific host, you may use the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Host github.com
    User git
    IdentityFile ~/.ssh/github_priv_key
</code></pre></div></div>

<p>Finally, from this point on, you won’t have to add the SSH key to the SSH agent every time. A more thorough answer can be found <a href="https://stackoverflow.com/a/4246809">here</a></p>

<h1 id="test-your-ssh-connection">Test your SSH connection</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ssh -T git@github.com
Hi username! You've successfully authenticated, but GitHub does not provide shell access.
</code></pre></div></div>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><summary type="html"><![CDATA[This is a guide to add a SSH key to your Github account. A good guide is done by Antonio Medeiros here and here. Existing SSH Keys First, check and see what are the current SSH keys. This can be done with $ ls -lah ~/.ssh total 28K drwx------ 2 nickshu nickshu 4.0K Dec 21 00:45 . drwx------ 45 nickshu nickshu 4.0K Jan 13 15:00 .. -rw------- 1 nickshu nickshu 4.2K Dec 29 21:14 known_hosts -rw------- 1 nickshu nickshu 3.5K Dec 21 00:39 known_hosts.old Generate a new SSH Key Next, you need to generate a new key. If you check the man ssh-keygen, you’ll see that the -t tag has 6 different types of keys you can choose from. DSA EcDSA EcDSA-SK Ed25519 Ed25519-SK RSA For more information on some of these types, you may visit https://goteleport.com/blog/comparing-ssh-keys/ $ ssh-keygen -t ed25519 -C "username@email.com" Generating public/private ed25519 key pair. Enter file in which to save the key (/home/username/.ssh/id_ed25519): Here you may either enter a specific path for your key pair, or you may use the default location. Finally, you will be prompted to enter a password. Enter passphrase (empty for no passphrase): Enter same passphrase again: This will create a key pair: a private and a public key on your desired location. The public key will have the extension .pub, whereas the private key will not have an extension. Do not share your private key. Add the Key to Github On Github, go to your Settings &gt; SSH and GPG keys, where you will see a list of your SSH keys. Press on New SSH key and copy and paste your SSH public key (e.g. /home/username/.ssh/mykey.pub). At this point, the key has been added to your Github account. Now you need to add it to the ssh-agent. Add your SSH Key to the ssh-agent One-Time Use Your SSH agent will help you so that you are not having to add your passphrase every time. First start the ssh-agent in the background. $ eval "$(ssh-agent -s)" Next, add the SSH private key to the ssh-agent. $ ssh-add ~/.ssh/path/to/ssh/private/key Permanent Use So, the easiest way to do so is to force the keys to be always kept. This can be done by adding to the ~/.ssh/config file. If your file does not exist, then simply create it and add the private keys IdentityFile ~/.ssh/github_priv_key IdentityFile ~/.ssh/server_priv_key And then change the permissions to 600 $ cd ~/.ssh $ ls -la ... -rw-r--r-- 1 nickshu nickshu 58 Jan 13 16:01 config ... $ chmod 600 ~/.ssh/config $ ls -la ... -rw------- 1 nickshu nickshu 58 Jan 13 16:01 config ... Alternatively, if you’d like to map a specific key to a specific host, you may use the following: Host github.com User git IdentityFile ~/.ssh/github_priv_key Finally, from this point on, you won’t have to add the SSH key to the SSH agent every time. A more thorough answer can be found here Test your SSH connection $ ssh -T git@github.com Hi username! You've successfully authenticated, but GitHub does not provide shell access.]]></summary></entry><entry><title type="html">Audio Cheatsheet</title><link href="http://localhost:4000/audio_cheatsheet.html" rel="alternate" type="text/html" title="Audio Cheatsheet" /><published>2021-04-12T00:00:00-04:00</published><updated>2021-04-12T00:00:00-04:00</updated><id>http://localhost:4000/audio-cheatsheet</id><content type="html" xml:base="http://localhost:4000/audio_cheatsheet.html"><![CDATA[<p>Here is a couple of things to remember about dealing with audio files in Python</p>

<h2 id="loading-audios">Loading Audios</h2>

<h3 id="scipy">Scipy</h3>
<p>There are multiple packages where one can use to load audio files. With respect to <code class="language-plaintext highlighter-rouge">.wav</code> files, the immediate one that people consider using is from <code class="language-plaintext highlighter-rouge">scipy.io</code>. More specifically, one may use the following template to load an audio data along with its sampling rate:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.io.wavfile</span> <span class="k">as</span> <span class="n">wavfile</span>
<span class="n">fs</span><span class="p">,</span><span class="n">audio</span> <span class="o">=</span> <span class="n">wavfile</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="s">"audio1.wav"</span><span class="p">)</span>
</code></pre></div></div>

<p>Then if one plots it, one will see the following</p>

<p><img src="../assets/images/audio_cheatsheet/scipy_wavfile_read.png" alt="" /></p>

<p>However, if there’s anything that I could tell someone is to NOT use Scipy’s Wavfile module to read in <code class="language-plaintext highlighter-rouge">.wav</code> files, because it imports data as integers. This is a problem because we may not know exactly the bit resolution and thus we don’t know how to properly scale it. Usually, it’s best for us to keep the values between -1 and 1.</p>

<h3 id="soundfile">Soundfile</h3>

<p>Alternatively, one may use one of the following</p>
<ul>
  <li><a href="https://pysoundfile.readthedocs.io/en/latest/">SoundFile</a>’s <a href="https://pysoundfile.readthedocs.io/en/latest/#soundfile.read"><code class="language-plaintext highlighter-rouge">read</code></a> function</li>
  <li><a href="https://librosa.org/doc/latest/index.html">Librosa</a>’s <a href="https://librosa.org/doc/main/generated/librosa.load.html"><code class="language-plaintext highlighter-rouge">load</code></a> function</li>
</ul>

<p>If one uses <code class="language-plaintext highlighter-rouge">soundfile</code>, one may use the following template:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">soundfile</span> <span class="k">as</span> <span class="n">sf</span>
<span class="n">audio</span><span class="p">,</span><span class="n">fs</span> <span class="o">=</span> <span class="n">sf</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="s">"audio1.wav"</span><span class="p">)</span>
</code></pre></div></div>

<p>And if one plots it, one will observe the following plot</p>

<p><img src="../assets/images/audio_cheatsheet/soundfile_read.png" alt="" /></p>

<p>One can clearly see that the amplitude values range between -1 and 1, where as for Scipy, it tries to keep the values as integers. This instead then comes properly scaled.</p>

<h3 id="librosa">Librosa</h3>

<p>As a third option, and what I actually prefer, is to use <code class="language-plaintext highlighter-rouge">librosa</code>. Below is a template to load audio files.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">librosa</span>
<span class="n">audio</span><span class="p">,</span><span class="n">fs</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"audio1.wav"</span><span class="p">,</span><span class="n">sr</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>And if one is to plot the result, one would see the following plot:</p>

<p><img src="../assets/images/audio_cheatsheet/librosa_load.png" alt="" /></p>

<p>I actually prefer <code class="language-plaintext highlighter-rouge">librosa</code> because the folks who built this library wrote it to be adaptable so that it takes almost every audio file type. For example, it even works with <code class="language-plaintext highlighter-rouge">.mp3</code> files.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">audio</span><span class="p">,</span> <span class="n">fs</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"audio2.mp3"</span><span class="p">,</span><span class="n">sr</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that I put the sampling rate parameter <code class="language-plaintext highlighter-rouge">sr</code> as <code class="language-plaintext highlighter-rouge">None</code>. The reason for that is that <code class="language-plaintext highlighter-rouge">librosa</code> was built to work super well with its entire ecosystem, and it forces the sampling rate to be 22050 Hz by default. In order to load the audio to be its original value, one needs to pass the <code class="language-plaintext highlighter-rouge">None</code> value to the sampling rate parameter.</p>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><summary type="html"><![CDATA[Here is a couple of things to remember about dealing with audio files in Python Loading Audios Scipy There are multiple packages where one can use to load audio files. With respect to .wav files, the immediate one that people consider using is from scipy.io. More specifically, one may use the following template to load an audio data along with its sampling rate: import scipy.io.wavfile as wavfile fs,audio = wavfile.read("audio1.wav") Then if one plots it, one will see the following However, if there’s anything that I could tell someone is to NOT use Scipy’s Wavfile module to read in .wav files, because it imports data as integers. This is a problem because we may not know exactly the bit resolution and thus we don’t know how to properly scale it. Usually, it’s best for us to keep the values between -1 and 1. Soundfile Alternatively, one may use one of the following SoundFile’s read function Librosa’s load function If one uses soundfile, one may use the following template: import soundfile as sf audio,fs = sf.read("audio1.wav") And if one plots it, one will observe the following plot One can clearly see that the amplitude values range between -1 and 1, where as for Scipy, it tries to keep the values as integers. This instead then comes properly scaled. Librosa As a third option, and what I actually prefer, is to use librosa. Below is a template to load audio files. import librosa audio,fs = librosa.load("audio1.wav",sr=None) And if one is to plot the result, one would see the following plot: I actually prefer librosa because the folks who built this library wrote it to be adaptable so that it takes almost every audio file type. For example, it even works with .mp3 files. audio, fs = librosa.load("audio2.mp3",sr=None) Note that I put the sampling rate parameter sr as None. The reason for that is that librosa was built to work super well with its entire ecosystem, and it forces the sampling rate to be 22050 Hz by default. In order to load the audio to be its original value, one needs to pass the None value to the sampling rate parameter.]]></summary></entry><entry><title type="html">Tmux Cheatsheet</title><link href="http://localhost:4000/tmux_cheatsheet.html" rel="alternate" type="text/html" title="Tmux Cheatsheet" /><published>2021-03-26T00:00:00-04:00</published><updated>2021-03-26T00:00:00-04:00</updated><id>http://localhost:4000/tmux</id><content type="html" xml:base="http://localhost:4000/tmux_cheatsheet.html"><![CDATA[<p>The prefix key is <code class="language-plaintext highlighter-rouge">C-b</code> (<code class="language-plaintext highlighter-rouge">Ctrl+b</code>). Here we’ll use <code class="language-plaintext highlighter-rouge">arrow</code> to represent any of the arrow keys (<code class="language-plaintext highlighter-rouge">left</code>,<code class="language-plaintext highlighter-rouge">right</code>,<code class="language-plaintext highlighter-rouge">up</code>,<code class="language-plaintext highlighter-rouge">down</code>)</p>

<h1 id="shortcuts">Shortcuts</h1>
<h2 id="panes">Panes</h2>
<p><code class="language-plaintext highlighter-rouge">"</code> - Splits plane vertically
<code class="language-plaintext highlighter-rouge">%</code> - Splits plane horizontally
<code class="language-plaintext highlighter-rouge">arrow</code> - Navigate between panes
<code class="language-plaintext highlighter-rouge">Ctrl</code>+<code class="language-plaintext highlighter-rouge">arrow</code> - Resize current pane
<code class="language-plaintext highlighter-rouge">q</code> - Show the pane numbers</p>

<h2 id="windows">Windows</h2>
<p><code class="language-plaintext highlighter-rouge">c</code> - Create new window
<code class="language-plaintext highlighter-rouge">,</code> - Rename window
<code class="language-plaintext highlighter-rouge">p</code> - Move to previous window
<code class="language-plaintext highlighter-rouge">n</code> - Move to next window</p>

<h2 id="sessions">Sessions</h2>
<p><code class="language-plaintext highlighter-rouge">(</code> - Move to previous session
<code class="language-plaintext highlighter-rouge">)</code> - Move to next session
<code class="language-plaintext highlighter-rouge">d</code> - Detach from session
<code class="language-plaintext highlighter-rouge">b</code> - Rename session</p>

<h1 id="scripting">Scripting</h1>
<p>If you wish to create multiple panes and launch different things from the command line, you can create a script.</p>

<p>In order to initiate a new session and detach from it from the session</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux new-session <span class="nt">-d</span>
</code></pre></div></div>

<p>Then you can split the window into two panes with <code class="language-plaintext highlighter-rouge">tmux split-window</code>. Here, you can split the window either vertically with the <code class="language-plaintext highlighter-rouge">-v</code> flag, or horizontally, with the <code class="language-plaintext highlighter-rouge">-h</code> flag. If none are specified, then <code class="language-plaintext highlighter-rouge">-v</code> is assumed.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux split-window <span class="nt">-v</span>
</code></pre></div></div>

<p>Next you can send commands to the new pane with <code class="language-plaintext highlighter-rouge">tmux send</code>. Here, simply send a string followed by <code class="language-plaintext highlighter-rouge">ENTER</code> to send the command</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux send <span class="s1">'ls'</span> ENTER
</code></pre></div></div>

<p>If you wish to switch panes, you first need to determine the number. Commonly, using <code class="language-plaintext highlighter-rouge">Ctrl</code>+<code class="language-plaintext highlighter-rouge">b</code> +<code class="language-plaintext highlighter-rouge">q</code> will quickly show you the pane numbers. Then by using <code class="language-plaintext highlighter-rouge">tmux select-pane</code>, you can choose the pane to move to.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux <span class="k">select</span><span class="nt">-pane</span> <span class="nt">-t</span> 0
</code></pre></div></div>

<p>Finally, if you wish to view the session, you can use <code class="language-plaintext highlighter-rouge">tmux attach-session</code>. A full working example is shown below:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># launch.sh</span>
tmux new-session <span class="nt">-d</span>
tmux split-window <span class="nt">-v</span>
tmux send <span class="s1">'conda activate myenv'</span> ENTER
tmux send <span class="s1">'python'</span> ENTER
tmux send <span class="s1">'x=1'</span> ENTER
tmux send <span class="s1">'print(x)'</span> ENTER
tmux split-window <span class="nt">-v</span>
tmux selected-pane <span class="nt">-t</span> 0
tmux send <span class="s1">'node'</span> ENTER
tmux attach-session
</code></pre></div></div>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Linux" /><summary type="html"><![CDATA[The prefix key is C-b (Ctrl+b). Here we’ll use arrow to represent any of the arrow keys (left,right,up,down) Shortcuts Panes " - Splits plane vertically % - Splits plane horizontally arrow - Navigate between panes Ctrl+arrow - Resize current pane q - Show the pane numbers Windows c - Create new window , - Rename window p - Move to previous window n - Move to next window Sessions ( - Move to previous session ) - Move to next session d - Detach from session b - Rename session Scripting If you wish to create multiple panes and launch different things from the command line, you can create a script. In order to initiate a new session and detach from it from the session tmux new-session -d Then you can split the window into two panes with tmux split-window. Here, you can split the window either vertically with the -v flag, or horizontally, with the -h flag. If none are specified, then -v is assumed. tmux split-window -v Next you can send commands to the new pane with tmux send. Here, simply send a string followed by ENTER to send the command tmux send 'ls' ENTER If you wish to switch panes, you first need to determine the number. Commonly, using Ctrl+b +q will quickly show you the pane numbers. Then by using tmux select-pane, you can choose the pane to move to. tmux select-pane -t 0 Finally, if you wish to view the session, you can use tmux attach-session. A full working example is shown below: # launch.sh tmux new-session -d tmux split-window -v tmux send 'conda activate myenv' ENTER tmux send 'python' ENTER tmux send 'x=1' ENTER tmux send 'print(x)' ENTER tmux split-window -v tmux selected-pane -t 0 tmux send 'node' ENTER tmux attach-session]]></summary></entry><entry><title type="html">Sliding Window Cheatsheet</title><link href="http://localhost:4000/sliding_window.html" rel="alternate" type="text/html" title="Sliding Window Cheatsheet" /><published>2021-02-26T00:00:00-05:00</published><updated>2021-02-26T00:00:00-05:00</updated><id>http://localhost:4000/sliding_window</id><content type="html" xml:base="http://localhost:4000/sliding_window.html"><![CDATA[<p>Alright, this is something that I am sick of having to figure out over and over again. Sliding window.</p>

<h2 id="case-1-stride-length--window-length">Case 1: Stride Length = Window Length</h2>

<p><img src="../assets/images/sliding_window/case1.png" alt="" /></p>

<p>Let us first consider the simplest case: where are trying to pass a window through some data, and your stride length equals the window length. If we were to know <em>a priori</em> the number of times we’d like to iterate through (e.g. 10 iterations), then this is the simplest case. We can simply let it iterate through integers, and ad a factor of the length of the window</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">window_length</span> <span class="o">=</span> <span class="mi">15</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">window_length</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">window_length</span>
    <span class="k">print</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">)</span>

<span class="c1"># Output 
# 0 15
# 15 30
# 30 45
# 45 60
# 60 75
# 75 90
# 90 105
# 105 120
# 120 135
# 135 150
</span></code></pre></div></div>

<p>If we however have a finite array with $N$ elements, we need to know the number of segments $n_{frames}$ we will need. With this, we can determine it by simply compute it by dividing the number of elements by the window length and flooring it</p>

\[n_{frames} = \bigg\lfloor \frac{N}{window\_length} \bigg\rfloor\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">window_length</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_frames</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">window_length</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_frames</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">window_length</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">window_length</span>
    <span class="k">print</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">]))</span>

<span class="c1"># Output
</span>
<span class="c1">#  0 16     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
# 16 32     [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31]
# 32 48     [32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]
# 48 64     [48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63]
# 64 80     [64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79]
# 80 96     [80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95]
</span></code></pre></div></div>

<h2 id="case-2-infinite-data">Case 2: Infinite Data</h2>

<p><img src="../assets/images/sliding_window/case2.png" alt="" /></p>

<p>Let us consider the case we now do not have the window length to be equal to the stride length. Let us also consider that we either know <em>a priori</em> the number of iterations we wish to go through, or we have infinite data. Let $k$ be the iteration number. We now change the start and end indices to</p>

\[\begin{align}
    start &amp;= k \cdot stride\_length \\ 
    stop &amp;= k \cdot stride\_length  + window\_length
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stride_length</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">window_length</span> <span class="o">=</span> <span class="mi">16</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">stride_length</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">stride_length</span> <span class="o">+</span> <span class="n">window_length</span>
    <span class="k">print</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">stop</span><span class="p">)</span>

<span class="c1"># Output
</span>
<span class="c1">#  0 16
# 10 26
# 20 36
# 30 46
# 40 56
# 50 66
# 60 76
# 70 86
# 80 96
# 90 106
</span></code></pre></div></div>

<h2 id="case-3-finite-data">Case 3: Finite Data</h2>

<p><img src="../assets/images/sliding_window/case3.png" alt="" /></p>

<p>Now, let us consider the case that we have a finite data with differing window lengths and stride lengths. In such case, we first need to determine the number of frames we’ll need. Let $N$ represent the number of data points you possess. This can be computed by</p>

\[n_{frames} = \bigg \lfloor \frac{N - window\_length}{stride\_length} \bigg \rfloor + 1\]

<p>We can the iterate it with the same formulation above, where $k$ represents the iteration number</p>

\[\begin{align}
    start &amp;= k \cdot stride\_length \\ 
    stop &amp;= k \cdot stride\_length  + window\_length
\end{align}\]

<p>Thus, its implementation is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stride_length</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">window_length</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_frames</span> <span class="o">=</span> <span class="p">((</span><span class="n">N</span> <span class="o">-</span> <span class="n">window_length</span><span class="p">)</span>  <span class="o">//</span> <span class="n">stride_length</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_frames</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">stride_length</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">stride_length</span> <span class="o">+</span> <span class="n">window_length</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"({:&gt;2}) {:&gt;2} {:&gt;2}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span><span class="n">stop</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">])</span>

<span class="c1"># Output
</span>
<span class="c1"># ( 0)  0 13	 [ 0  1  2  3  4  5  6  7  8  9 10 11 12]
# ( 1)  7 20	 [ 7  8  9 10 11 12 13 14 15 16 17 18 19]
# ( 2) 14 27	 [14 15 16 17 18 19 20 21 22 23 24 25 26]
# ( 3) 21 34	 [21 22 23 24 25 26 27 28 29 30 31 32 33]
# ( 4) 28 41	 [28 29 30 31 32 33 34 35 36 37 38 39 40]
# ( 5) 35 48	 [35 36 37 38 39 40 41 42 43 44 45 46 47]
# ( 6) 42 55	 [42 43 44 45 46 47 48 49 50 51 52 53 54]
# ( 7) 49 62	 [49 50 51 52 53 54 55 56 57 58 59 60 61]
# ( 8) 56 69	 [56 57 58 59 60 61 62 63 64 65 66 67 68]
# ( 9) 63 76	 [63 64 65 66 67 68 69 70 71 72 73 74 75]
# (10) 70 83	 [70 71 72 73 74 75 76 77 78 79 80 81 82]
# (11) 77 90	 [77 78 79 80 81 82 83 84 85 86 87 88 89]
# (12) 84 97	 [84 85 86 87 88 89 90 91 92 93 94 95 96]
</span></code></pre></div></div>

<p>Done. Get to work.</p>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Machine Learning" /><category term="Python" /><summary type="html"><![CDATA[Alright, this is something that I am sick of having to figure out over and over again. Sliding window. Case 1: Stride Length = Window Length Let us first consider the simplest case: where are trying to pass a window through some data, and your stride length equals the window length. If we were to know a priori the number of times we’d like to iterate through (e.g. 10 iterations), then this is the simplest case. We can simply let it iterate through integers, and ad a factor of the length of the window window_length = 15 for k in range(10): start = k * window_length stop = (k+1) * window_length print(start, stop) # Output # 0 15 # 15 30 # 30 45 # 45 60 # 60 75 # 75 90 # 90 105 # 105 120 # 120 135 # 135 150 If we however have a finite array with $N$ elements, we need to know the number of segments $n_{frames}$ we will need. With this, we can determine it by simply compute it by dividing the number of elements by the window length and flooring it \[n_{frames} = \bigg\lfloor \frac{N}{window\_length} \bigg\rfloor\] data = np.arange(100) window_length = 16 N = data.shape[0] n_frames = N // window_length for k in range(n_frames): start = k * window_length stop = (k+1) * window_length print(start, stop, "\t", len(data[start:stop])) # Output # 0 16 [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] # 16 32 [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31] # 32 48 [32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47] # 48 64 [48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63] # 64 80 [64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79] # 80 96 [80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95] Case 2: Infinite Data Let us consider the case we now do not have the window length to be equal to the stride length. Let us also consider that we either know a priori the number of iterations we wish to go through, or we have infinite data. Let $k$ be the iteration number. We now change the start and end indices to \[\begin{align} start &amp;= k \cdot stride\_length \\ stop &amp;= k \cdot stride\_length + window\_length \end{align}\] stride_length = 10 window_length = 16 for k in range(10): start = k * stride_length stop = k * stride_length + window_length print(start,stop) # Output # 0 16 # 10 26 # 20 36 # 30 46 # 40 56 # 50 66 # 60 76 # 70 86 # 80 96 # 90 106 Case 3: Finite Data Now, let us consider the case that we have a finite data with differing window lengths and stride lengths. In such case, we first need to determine the number of frames we’ll need. Let $N$ represent the number of data points you possess. This can be computed by \[n_{frames} = \bigg \lfloor \frac{N - window\_length}{stride\_length} \bigg \rfloor + 1\] We can the iterate it with the same formulation above, where $k$ represents the iteration number \[\begin{align} start &amp;= k \cdot stride\_length \\ stop &amp;= k \cdot stride\_length + window\_length \end{align}\] Thus, its implementation is stride_length = 4 window_length = 6 data = np.arange(100) N = data.shape[0] n_frames = ((N - window_length) // stride_length) + 1 for k in range(n_frames): start = k * stride_length stop = k * stride_length + window_length print("({:&gt;2}) {:&gt;2} {:&gt;2}".format(k, start,stop), data[start:stop]) # Output # ( 0) 0 13 [ 0 1 2 3 4 5 6 7 8 9 10 11 12] # ( 1) 7 20 [ 7 8 9 10 11 12 13 14 15 16 17 18 19] # ( 2) 14 27 [14 15 16 17 18 19 20 21 22 23 24 25 26] # ( 3) 21 34 [21 22 23 24 25 26 27 28 29 30 31 32 33] # ( 4) 28 41 [28 29 30 31 32 33 34 35 36 37 38 39 40] # ( 5) 35 48 [35 36 37 38 39 40 41 42 43 44 45 46 47] # ( 6) 42 55 [42 43 44 45 46 47 48 49 50 51 52 53 54] # ( 7) 49 62 [49 50 51 52 53 54 55 56 57 58 59 60 61] # ( 8) 56 69 [56 57 58 59 60 61 62 63 64 65 66 67 68] # ( 9) 63 76 [63 64 65 66 67 68 69 70 71 72 73 74 75] # (10) 70 83 [70 71 72 73 74 75 76 77 78 79 80 81 82] # (11) 77 90 [77 78 79 80 81 82 83 84 85 86 87 88 89] # (12) 84 97 [84 85 86 87 88 89 90 91 92 93 94 95 96] Done. Get to work.]]></summary></entry><entry><title type="html">Jupyter Cheatsheet</title><link href="http://localhost:4000/jupyter_cheatsheet.html" rel="alternate" type="text/html" title="Jupyter Cheatsheet" /><published>2021-02-13T00:00:00-05:00</published><updated>2021-02-13T00:00:00-05:00</updated><id>http://localhost:4000/jupyter-cheatsheet</id><content type="html" xml:base="http://localhost:4000/jupyter_cheatsheet.html"><![CDATA[<h2 id="autoreload">Autoreload</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
</code></pre></div></div>

<h2 id="ipython">IPython</h2>

<h3 id="display-audio">Display Audio</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="n">ipd</span>

<span class="n">audio</span><span class="p">,</span> <span class="n">fs</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"path/to/audio"</span><span class="p">)</span>
<span class="n">ipd</span><span class="p">.</span><span class="n">Audio</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">fs</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><summary type="html"><![CDATA[Autoreload %load_ext autoreload %autoreload 2 IPython Display Audio import IPython.display as ipd audio, fs = librosa.load("path/to/audio") ipd.Audio(data=audio, rate=fs)]]></summary></entry><entry><title type="html">Tensorflow Cheatsheet</title><link href="http://localhost:4000/tf_cheatsheet.html" rel="alternate" type="text/html" title="Tensorflow Cheatsheet" /><published>2021-02-13T00:00:00-05:00</published><updated>2021-02-13T00:00:00-05:00</updated><id>http://localhost:4000/suppress_tensorflow</id><content type="html" xml:base="http://localhost:4000/tf_cheatsheet.html"><![CDATA[<h2 id="suppress-tensorflow-warnings">Suppress Tensorflow Warnings</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'TF_CPP_MIN_LOG_LEVEL'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'2'</span>
</code></pre></div></div>

<h2 id="allow-for-gpu-memory-growth">Allow for GPU Memory Growth</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">physical_devices</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span> 
<span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">physical_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Machine Learning" /><category term="Python" /><summary type="html"><![CDATA[Suppress Tensorflow Warnings import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' Allow for GPU Memory Growth import tensorflow as tf physical_devices = tf.config.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(physical_devices[0], True)]]></summary></entry><entry><title type="html">Installing h5web on Jupyter Lab</title><link href="http://localhost:4000/jupyter_h5web.html" rel="alternate" type="text/html" title="Installing h5web on Jupyter Lab" /><published>2021-02-12T00:00:00-05:00</published><updated>2021-02-12T00:00:00-05:00</updated><id>http://localhost:4000/h5web</id><content type="html" xml:base="http://localhost:4000/jupyter_h5web.html"><![CDATA[<p>Sometimes in different scientific fields, one may end up using a lot of <a href="https://en.wikipedia.org/wiki/Binary_large_object">BLOB</a> data. Thus, there are different ways to store the data. One of the ways that one may wish to install their data, such as hyperspectral images, is to store it in HDF5. With such, one may sometimes wish to view their data, since it will likely be saved in a <code class="language-plaintext highlighter-rouge">.h5</code> format. Loic Huder created a tool, which he presented with the <a href="https://youtu.be/3GNOOdDR-YU">YouTube hdf5 channel</a>, where he’s been keeping his repository on GitHub <a href="https://github.com/silx-kit/h5web">here</a>. At the moment of writing this post, I still have not yet figured out how to install it properly. However, an alternative is to use it within Jupyter Lab (<a href="https://github.com/silx-kit/jupyterlab-h5web">source</a>).</p>

<p>In order to do so, you may use a Jupyter Lab version greater than 2.2.5, e.g.:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install jupyterlab==2.2.8
</code></pre></div></div>

<p>Then to install the extension,</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>jupyterlab_h5web 

<span class="c"># OR for blosc and bitshuffle files</span>
<span class="c"># pip install jupyterlab_h5web[full]</span>

jupyter lab build
</code></pre></div></div>

<p>Finally, you will be ablet open the HDF5 files by going to Jupyter Lab and viewing the files. As an example, <a href="../assets/images/h5web/example-data.h5">here</a> is a sample HDF5 file. Below are a few examples of how it’d look like</p>

<p><img src="../assets/images/h5web/line_graph.png" alt="" />
<img src="../assets/images/h5web/text.png" alt="" />
<img src="../assets/images/h5web/heatmap.png" alt="" /></p>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Linux" /><category term="Machine Learning" /><summary type="html"><![CDATA[Sometimes in different scientific fields, one may end up using a lot of BLOB data. Thus, there are different ways to store the data. One of the ways that one may wish to install their data, such as hyperspectral images, is to store it in HDF5. With such, one may sometimes wish to view their data, since it will likely be saved in a .h5 format. Loic Huder created a tool, which he presented with the YouTube hdf5 channel, where he’s been keeping his repository on GitHub here. At the moment of writing this post, I still have not yet figured out how to install it properly. However, an alternative is to use it within Jupyter Lab (source). In order to do so, you may use a Jupyter Lab version greater than 2.2.5, e.g.: pip install jupyterlab==2.2.8 Then to install the extension, pip install jupyterlab_h5web # OR for blosc and bitshuffle files # pip install jupyterlab_h5web[full] jupyter lab build Finally, you will be ablet open the HDF5 files by going to Jupyter Lab and viewing the files. As an example, here is a sample HDF5 file. Below are a few examples of how it’d look like]]></summary></entry><entry><title type="html">Bayesian Inference on Gaussian Distributions</title><link href="http://localhost:4000/bayesian_inference_for_gaussian.html" rel="alternate" type="text/html" title="Bayesian Inference on Gaussian Distributions" /><published>2021-02-06T00:00:00-05:00</published><updated>2021-02-06T00:00:00-05:00</updated><id>http://localhost:4000/bayesian-inference-gaussian</id><content type="html" xml:base="http://localhost:4000/bayesian_inference_for_gaussian.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Here we will try to better understand how Bayesian inference works for Gaussian distributions.</p>

<h2 id="likelihood">Likelihood</h2>

<p>Let’s say that we have a dataset $\boldsymbol{X} = { x_1, \cdots, x_N }$, where $x_i \in \mathbb{R}$ and $x_i \sim \mathcal{N(\mu,\sigma)}$. In other words, we have $N$ examples from an univariate Gaussian distribution which has the mean $\mu$ and the variance $\sigma$ for the distribution. Let us also assume that all of the $N$ examples are sampled independently from each other. Therefore, the likelihood of the dataset is defined as the probability of obtaining the data given $\mu$, as a function of $\mu$. Note that this all makes sense in a diverging connection in a Bayesian network, where the mean node diverges to $N$ nodes, and if $\mu$ is known, then all of the samples are independent.</p>

\[\bbox[teal,4pt]{p(\boldsymbol{X}|\mu) = \prod_{i=1}^N p(x_i|\mu) = \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right]}\]

<p>Although this is defined as the likelihood, it would be best to determine a single equation that does not have have a factorized notation (i.e. $\prod$). Now, let us define the empirical mean $\bar{x}$ and empirical variance $s^2$.</p>

\[\begin{align}
\bar{x} &amp;= \frac{1}{N}\sum_{i=1}^N x_i \\ 
s^2 &amp;= \frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2
\end{align}\]

<p>Therefore, we can rewrite</p>

\[\begin{align}
\sum_{i=1}^N (x_i - \mu)^2 &amp;= \sum_{i=1}^N (x_i - \bar{x} - \mu + \bar{x})^2 \\ 
&amp;= \sum_{i=1}^N [(x_i - \bar{x}) - (\mu - \bar{x})]^2 \\ 
&amp;= \underbrace{\sum_{i=1}^N (x_i - \bar{x})^2}_{ns^2} + \sum_{i=1}^N (\bar{x} - \mu)^2 - 2 \sum_{i=1}^N (x_i - \bar{x})(\mu - \bar{x}) \\ 
&amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( \left(\sum_{i=1}^N x_i \right)-N\bar{x} \right) (\mu - \bar{x})\right] \\ 
&amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( N\bar{x}-N\bar{x} \right) (\mu - \bar{x})\right] \\ 
&amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 - 0 \\ 
&amp;= Ns^2 + \sum_{i=1}^N \underbrace{(\bar{x} - \mu)^2}_{\text{indep of $i$}} \\
&amp;= Ns^2 + N (\bar{x} - \mu)^2
\end{align}\]

<p>Therefore, by putting this into the likelihood</p>

\[\begin{align}
p(\boldsymbol{X}|\mu) &amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right] \\ 
&amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} [Ns^2 + N (\bar{x} - \mu)^2]\right] \\ 
&amp;\propto (\sigma^2)^{N/2} \exp\left( -\frac{Ns^2}{2\sigma^2} \right) \exp \left( -\frac{N}{2\sigma^2} (\bar{x}-\mu)^2\right)
\end{align}\]

<p>Now, if $\sigma^2$ is constant, that means that all of the data examples have the same variance, and if this is constant, then we can drop the constant factors from the likelihood, thus obtaining that the likelihood of the dataset is</p>

\[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{N}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\]

<h2 id="prior">Prior</h2>

<p>Now let’s compute the prior. Often, for different distributions, a natural conjugate prior is a prior which leads to the posterior being in the same distribution as the prior.  On the dataset, $\sigma^2$ is the variance of the observation noise, and $\mu$ is the mean of the observations. So, since the likelihood has the form</p>

\[p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{n}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{n} \right)\]

<p>Then the natural conjugate prior $p(\mu)$ has the form</p>

\[\bbox[teal,4pt]{p(\mu) \propto \exp\left[ - \frac{(\mu - \mu_0)^2}{2 \sigma_0^2}  \right] \propto \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\]

<p>where the mean of the prior is $\mu_0$, and the variance of the prior is $\sigma_0^2$.</p>

<h2 id="posterior">Posterior</h2>

<p>Now, we need to compute the posterior $p(\mu \vert \boldsymbol{X})$. The posterior distribution is given by</p>

\[p(\mu \vert \boldsymbol{X}) \propto p(\boldsymbol{X} \vert \mu) p (\mu)\]

<p>By expanding it, we will have</p>

\[\begin{align}
p(\mu | \boldsymbol{X}) &amp;\propto p(\boldsymbol{X} | \mu) p (\mu) \\ 
&amp;\propto \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right) \cdot \exp \left( -\frac{1}{2\sigma_0^2} (\mu - \mu_0)^2 \right) \\ 
&amp;= \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i^2 + \mu^2 - 2x_i \mu) + \frac{-1}{2\sigma_0^2} (\mu^2 + \mu_0 - 2 \mu_0 \mu)\right] 
\end{align}\]

<p>We know that the product of two Gaussians yield another Gaussian. So we can write</p>

\[\begin{align} p(\mu | \boldsymbol{X}) &amp;\propto \exp \left[  \color{cyan}{-\frac{\sum_i x_i^2}{2\sigma^2}}  \color{green}{- \frac{N \mu^2}{2\sigma^2}} \color{magenta}{+\frac{\sum_i x_i \mu}{\sigma^2}} \color{green}{- \frac{\mu^2}{2\sigma_0^2}} \color{cyan}{- \frac{\mu_0}{2\sigma_0^2}} \color{magenta}{+ \frac{\mu_0\mu}{\sigma_0^2}} \right] \\ 
&amp;= \exp \left[ \color{green}{- \frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2} \right)} \color{magenta}{+\mu \left( \frac{\sum_i x_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)} \color{cyan}{- \left( \frac{\sum_i x^2}{2 \sigma^2} + \frac{\mu_0}{2\sigma_0} \right)} \right] \\ 
&amp;\triangleq \exp \left[ -\frac{1}{2\sigma_N^2} (\mu - \mu_N)^2 \right] \\ 
&amp;= \exp \left[ -\frac{1}{2\sigma_N^2} \left( \color{green}{\mu^2} \color{magenta}{-2\mu_N \mu} \color{cyan}{+\mu_N^2} \right)\right]
\end{align}\]

<p>By matching the different coefficients, we can determine the posterior variance $\sigma_N^2$ and the posterior mean $\mu_N$. By matching the $\mu^2$ first, we can determine the $\sigma_N^2$:</p>

\[\begin{align} 
-\frac{\mu^2}{2\sigma_N^2} &amp;= -\frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2}\right) \\ 
\frac{1}{\sigma_N^2} &amp;= \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = \frac{N\sigma_0^2 + \sigma^2}{\sigma^2\sigma_0^2}\\
\sigma_N^2 &amp;= \left( \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} \right)^{-1} = \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}
\end{align}\]

<p>By matching the $\mu$ coefficients, we can determine the posterior mean $\mu_N$</p>

\[\begin{align} 
\frac{-2\mu \mu_N}{-2\sigma_N^2} &amp;= \mu \left( \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right) \\ 
\frac{\mu_N}{\sigma_N^2} &amp;= \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ 
&amp;= \frac{N \bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ 
&amp;= \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
\mu_N &amp;= \sigma_N^2 \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
&amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \cdot \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
&amp;= \frac{\sigma_0^2 n\bar{x} + \sigma^2 \mu_0}{N\sigma_0^2 + \sigma^2} \\ 
&amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\bar{x} \\ 
&amp;= \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right)
\end{align}\]

<p>So, the two parameters for the posterior Gaussian are:</p>

\[\bbox[teal,4pt]{\begin{align}
\mu_N &amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \underbrace{\bar{x}}_{\mu_{ML}} = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \\
\sigma_N^2 &amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}
\end{align}}\]

<p>Note that $\bar{x} = \mu_{ML} = N^{-1} \sum_{i=1}^N x_i$ as the maximum likelihood solution for $\mu$. Note that you can represent the variance as the <strong>precision</strong> of a Gaussian, which should be the inverse of the variance.</p>

\[\lambda = \frac{1}{\sigma^2} \quad \quad \lambda_0 = \frac{1}{\sigma_0^2} \quad \quad \lambda_N = \frac{1}{\sigma_N^2}\]

<p>We can write the precision of the posterior Gaussian as</p>

\[\lambda_N = \frac{1}{\sigma_N^2} = \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = N\lambda + \lambda_0\]

<p>And for the mean of the posterior using the precision,</p>

\[\mu_N = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2} \right) = \frac{\mu_0 \lambda_0 + n\bar{x} \lambda}{\lambda_N}\]

<p>Let us look at how all of these equations behave when you add more data.</p>

<blockquote>
  <p>For each of the following, when you have $N\rightarrow \infty$, then:</p>

  <ul>
    <li>Mean: $\displaystyle \lim_{\color{green}{N}\rightarrow \infty} \mu_N = \lim_{\color{green}{N}\rightarrow \infty} \frac{\sigma^2}{\color{green}{N}\sigma_0^2 + \sigma^2}\mu_0 + \frac{\color{green}{N}\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} \mu_{ML} = \mu_{ML}$</li>
    <li>Variance: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \sigma_{\color{green}{N}}^2 = \lim_{\color{green}{N\rightarrow \infty}} \frac{\sigma^2\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} = 0$</li>
    <li>Precision: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \lambda_{\color{green}{N}} = \lim_{\color{green}{N\rightarrow \infty}} = \color{green}{N}\lambda + \lambda_0 = \infty$</li>
  </ul>

  <p>This all starts to make sense intuitively. As we increase the number of examples, the mean progressively starts to get closer to the mean of the maximum likelihood of the Gaussian data. Meanwhile, the variance steadily starts to shrink to infinitesimally small values, whereas the precision starts to become infinitely high.</p>
</blockquote>

<h2 id="posterior-predictive">Posterior Predictive</h2>

<p>Now, we come to compute the posterior predictive. As a recap (Bishop Eq. 2.113), if we have a marginal Gaussian distribution $p(x)$ and a conditional Gaussian distribution $p(y \vert x)$</p>

\[\begin{align} p(x) &amp;= \texttip{\mathcal{N} (x | \mu, \color{magenta}{\Lambda^{-1}})}{Marginal} \\ p(y|x) &amp;= \mathcal{N} (y | \color{purple}{Ax + b}, \color{yellow}{L^{-1}})\end{align}\]

<p>Then to compute the marginal distribution $p(y)$ is</p>

\[p(y) = \mathcal{N} (y|\color{purple}{A\mu+b}, \color{yellow}{L^{-1}} + A\color{magenta}{\Lambda^{-1}} A^T)\]

<p>So, the posterior predictive is given by</p>

\[\begin{align} 
p(x|\boldsymbol{X}) &amp;= \int p(x|\mu) \cdot p(\mu|\boldsymbol{X}) d\mu \\ 
&amp;= \int \mathcal{N(x|\mu, \sigma^2) \cdot \mathcal{N}(\mu | \color{purple}{\mu_N}, \color{yellow}{\sigma_N^2}) d\mu } 
\end{align}\]

\[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\color{purple}{\mu_N}, \color{yellow}{\sigma_N^2} + \color{magenta}{\sigma^2})}\]

<p>[Alternate Proof here]</p>

<h2 id="marginal-likelihood">Marginal Likelihood</h2>

<p>The marginal likelihood $l$ can be defined as</p>

\[\bbox[4pt,teal]{\begin{align}
l &amp;= p(\boldsymbol{X}) = \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\
&amp;=\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]
\end{align}}\]

<h3 id="proof">Proof</h3>

<blockquote>
  <p>In order to compute the marginal likelihood, let us call $m=\mu_0$ and $\tau^2 = \sigma_0^2$ as our hyperparameters for simplicity</p>

\[\begin{align}
l &amp;= p(\boldsymbol{X}) \\
&amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\
&amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | m, \tau^2) d\mu \\
&amp;= \frac{1}{(\sigma \sqrt{2\pi})^n (\tau \sqrt{2\pi})} \int \exp \left( - \frac{1}{2\sigma^2 \sum_i (x_i - \mu)^2 - \frac{1}{2\tau^2} (\mu-m)^2 }\right) d\mu
\end{align}\]

  <p>Let us now define $S^2 = \frac{1}{\sigma^2}$ and $T^2 = \frac{1}{\tau^2}$
 \(\begin{align}
p(\boldsymbol{X}) &amp;= \frac{1}{(S^{-1}\sqrt{2\pi})^n (T^{-1} \sqrt{2\pi})} \int \exp \left[ -\frac{S^2}{2} \left( \sum_i x_i^2 + n \mu^2 - 2\mu \sum_i x_i\right) - \frac{T^2}{2} \left(\mu^2 +m^2-2m\mu\right)\right] \\
&amp;= \color{purple}{\frac{1}{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}} \int \color{purple}{e^{-\frac{S^2 \sum_i x_i^2}{2}}} e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} \color{purple}{e^{-\frac{T^2}{2}m^2}} e^{T^2m\mu} d\mu \\
&amp;= \underbrace{\color{purple}{\frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}}}_{c} \int e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu} d\mu \\
&amp;= c \int \color{violet}{e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu}} d\mu \\
&amp;= c \int \color{violet}{\exp \left[ -\frac{1}{2} \left( S^2N\mu^2 - 2S^2\mu \sum_i x_i +T^2 \mu^2 -2T^2 m \mu\right) \right]}  d\mu \\
&amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \underbrace{\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} }_{\alpha}\right)\right] d\mu \\
&amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha\right)\right] d\mu \\
&amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha \color{violet}{+ \alpha^2 - \alpha^2} \right) \right] d\mu \\
&amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \color{purple}{+ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 }\right] d\mu \\
&amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} d\mu \\
&amp;= c  \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] d\mu \\
&amp;= c  \exp \left[ \frac{1}{2} \color{red}{\left( S^2N + T^2 \right) \left( \frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} \right)^2} \right] \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu \\ 
&amp;= c  \exp \left[ \frac{1}{2}  \color{red}{\frac{(S^2\sum_i x_i +T^2 m)^2}{S^2 N + T^2}} \right] \underbrace{\color{yellow}{\int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu}}_{\cdots = \frac{1.25331 erf(\frac{0.7071 -T^2m + s^2 N(\mu-\bar{x})+T^2\mu }{\sqrt{NS^2 + T^2}})}{\sqrt{NS^2 + T^2}} = \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} }  \\
&amp;= c  \exp \left[ \frac{1}{2}  \frac{(S^2 \color{brown}{\sum_i x_i} +T^2 m)^2}{S^2 N + T^2} \right] \color{yellow}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ 
&amp;= c  \exp \left[ \frac{1}{2}  \frac{(S^2 \color{brown}{N \bar{x}}+T^2 m)^2}{S^2 N + T^2} \right] \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} \\ 
&amp;= \frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}}  \exp \left[ \color{violet}{ \frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\
&amp;= \frac{1 }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2 \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}}
\end{align}\)</p>

  <p>Now, let us work on the constant and the exponent portion:</p>

\[\color{lime}{\frac{\sqrt{2\pi}}{T^{-1}\sqrt{2\pi}\sqrt{S^2N + T^2}} = \frac{1}{\tau\sqrt{\frac{N}{\sigma^2} + \frac{1}{\tau^2}}}\frac{\sigma}{\sigma} = \frac{\sigma}{\sqrt{\sigma^2\tau^2 \left( \frac{n}{\sigma^2}+ \frac{1}{\tau^2} \right)}} = \frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} }\]

\[\color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} = \frac{\left( \frac{1}{\sigma^2} n\bar{x} + \frac{1}{\tau^2} m \right)^2}{2 \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2}\right)} = \frac{\left(\frac{N \bar{x} \tau^2+ \sigma^2 m}{\tau^2 \sigma^2} \right)^2}{2 \left( \frac{\tau^2 N +\sigma^2}{\tau^2 \sigma^2} \right)} = \frac{(N\bar{x}\tau^2 + \sigma^2 m)^2}{2\tau^2 \sigma^2 (\tau^2 N + \sigma^2)}}\]

\[\color{violet}{= \frac{N^2 \bar{x} \tau^4 + \sigma^4 m^2 + 2 \sigma^2 \tau^2 m N \bar{x}}{2 \sigma^2 \tau^2 (\tau^2 N + \sigma^2)} = \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)}}\]

  <p>Therefore, bringing it all together, we obtain</p>

\[\begin{align}
p(\boldsymbol{X}) &amp;=  \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{(T^{-1}\sqrt{2\pi})\sqrt{S^2N + T^2}}} \\
&amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} }\right] \color{lime}{\frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} } \\
&amp;= \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\tau^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} \right] 
\end{align}\]

  <p>By substituting the $\tau$ and $m$ terms back again, we obtain</p>

\[\bbox[teal,4pt]{p(\boldsymbol{X}) = \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\sigma_0^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 \mu_0^2}{\sigma_0^2} + 2 \mu_0 N\bar{x}}{2 (\sigma_0^2 N + \sigma)} \right] }\]
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>Bringing it all together, if one is to have a dataset $\boldsymbol{X}$ with $N$ examples, which are all univariate, the <strong>likelihood</strong> of obtaining the dataset given some mean $\mu$, is</p>

\[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) = \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\]

<p>The <strong>prior distribution</strong> for the mean $\mu$ has a prior mean $\mu_0$ and variance $\sigma_0^2$ being represented as</p>

\[\bbox[teal,4pt]{p(\mu) = \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\]

<p>The <strong>posterior distribution</strong> for the mean given the dataset can be described as</p>

\[\bbox[teal,4pt]{p(\mu | \boldsymbol{X} ) = \mathcal{N} \left( \mu \bigg| \underbrace{\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \bar{x}}_{\mu_N}, \underbrace{\frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}}_{\sigma^2_N} \right) }\]

<p>And in order to identify the probability of a probe sample $x$ given the dataset $\boldsymbol{X}$ can be described as</p>

\[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)}\]

<p>Finally, to obtain the marginal likelihood of the dataset, one can compute as</p>

\[\bbox[4pt,teal]{p(\boldsymbol{X}) =\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]}\]

<h2 id="sources">Sources</h2>

<ul>
  <li>Kevin P. Murphy <a href="https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf">“Conjugate Bayesian Analysis of the Gaussian Distribution”</a> 3 Oct 2007</li>
  <li>Christopher Bishop - Pattern Recognition</li>
</ul>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Machine Learning" /><category term="Math" /><summary type="html"><![CDATA[Introduction Here we will try to better understand how Bayesian inference works for Gaussian distributions. Likelihood Let’s say that we have a dataset $\boldsymbol{X} = { x_1, \cdots, x_N }$, where $x_i \in \mathbb{R}$ and $x_i \sim \mathcal{N(\mu,\sigma)}$. In other words, we have $N$ examples from an univariate Gaussian distribution which has the mean $\mu$ and the variance $\sigma$ for the distribution. Let us also assume that all of the $N$ examples are sampled independently from each other. Therefore, the likelihood of the dataset is defined as the probability of obtaining the data given $\mu$, as a function of $\mu$. Note that this all makes sense in a diverging connection in a Bayesian network, where the mean node diverges to $N$ nodes, and if $\mu$ is known, then all of the samples are independent. \[\bbox[teal,4pt]{p(\boldsymbol{X}|\mu) = \prod_{i=1}^N p(x_i|\mu) = \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right]}\] Although this is defined as the likelihood, it would be best to determine a single equation that does not have have a factorized notation (i.e. $\prod$). Now, let us define the empirical mean $\bar{x}$ and empirical variance $s^2$. \[\begin{align} \bar{x} &amp;= \frac{1}{N}\sum_{i=1}^N x_i \\ s^2 &amp;= \frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2 \end{align}\] Therefore, we can rewrite \[\begin{align} \sum_{i=1}^N (x_i - \mu)^2 &amp;= \sum_{i=1}^N (x_i - \bar{x} - \mu + \bar{x})^2 \\ &amp;= \sum_{i=1}^N [(x_i - \bar{x}) - (\mu - \bar{x})]^2 \\ &amp;= \underbrace{\sum_{i=1}^N (x_i - \bar{x})^2}_{ns^2} + \sum_{i=1}^N (\bar{x} - \mu)^2 - 2 \sum_{i=1}^N (x_i - \bar{x})(\mu - \bar{x}) \\ &amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( \left(\sum_{i=1}^N x_i \right)-N\bar{x} \right) (\mu - \bar{x})\right] \\ &amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( N\bar{x}-N\bar{x} \right) (\mu - \bar{x})\right] \\ &amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 - 0 \\ &amp;= Ns^2 + \sum_{i=1}^N \underbrace{(\bar{x} - \mu)^2}_{\text{indep of $i$}} \\ &amp;= Ns^2 + N (\bar{x} - \mu)^2 \end{align}\] Therefore, by putting this into the likelihood \[\begin{align} p(\boldsymbol{X}|\mu) &amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right] \\ &amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} [Ns^2 + N (\bar{x} - \mu)^2]\right] \\ &amp;\propto (\sigma^2)^{N/2} \exp\left( -\frac{Ns^2}{2\sigma^2} \right) \exp \left( -\frac{N}{2\sigma^2} (\bar{x}-\mu)^2\right) \end{align}\] Now, if $\sigma^2$ is constant, that means that all of the data examples have the same variance, and if this is constant, then we can drop the constant factors from the likelihood, thus obtaining that the likelihood of the dataset is \[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{N}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\] Prior Now let’s compute the prior. Often, for different distributions, a natural conjugate prior is a prior which leads to the posterior being in the same distribution as the prior. On the dataset, $\sigma^2$ is the variance of the observation noise, and $\mu$ is the mean of the observations. So, since the likelihood has the form \[p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{n}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{n} \right)\] Then the natural conjugate prior $p(\mu)$ has the form \[\bbox[teal,4pt]{p(\mu) \propto \exp\left[ - \frac{(\mu - \mu_0)^2}{2 \sigma_0^2} \right] \propto \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\] where the mean of the prior is $\mu_0$, and the variance of the prior is $\sigma_0^2$. Posterior Now, we need to compute the posterior $p(\mu \vert \boldsymbol{X})$. The posterior distribution is given by \[p(\mu \vert \boldsymbol{X}) \propto p(\boldsymbol{X} \vert \mu) p (\mu)\] By expanding it, we will have \[\begin{align} p(\mu | \boldsymbol{X}) &amp;\propto p(\boldsymbol{X} | \mu) p (\mu) \\ &amp;\propto \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right) \cdot \exp \left( -\frac{1}{2\sigma_0^2} (\mu - \mu_0)^2 \right) \\ &amp;= \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i^2 + \mu^2 - 2x_i \mu) + \frac{-1}{2\sigma_0^2} (\mu^2 + \mu_0 - 2 \mu_0 \mu)\right] \end{align}\] We know that the product of two Gaussians yield another Gaussian. So we can write \[\begin{align} p(\mu | \boldsymbol{X}) &amp;\propto \exp \left[ \color{cyan}{-\frac{\sum_i x_i^2}{2\sigma^2}} \color{green}{- \frac{N \mu^2}{2\sigma^2}} \color{magenta}{+\frac{\sum_i x_i \mu}{\sigma^2}} \color{green}{- \frac{\mu^2}{2\sigma_0^2}} \color{cyan}{- \frac{\mu_0}{2\sigma_0^2}} \color{magenta}{+ \frac{\mu_0\mu}{\sigma_0^2}} \right] \\ &amp;= \exp \left[ \color{green}{- \frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2} \right)} \color{magenta}{+\mu \left( \frac{\sum_i x_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)} \color{cyan}{- \left( \frac{\sum_i x^2}{2 \sigma^2} + \frac{\mu_0}{2\sigma_0} \right)} \right] \\ &amp;\triangleq \exp \left[ -\frac{1}{2\sigma_N^2} (\mu - \mu_N)^2 \right] \\ &amp;= \exp \left[ -\frac{1}{2\sigma_N^2} \left( \color{green}{\mu^2} \color{magenta}{-2\mu_N \mu} \color{cyan}{+\mu_N^2} \right)\right] \end{align}\] By matching the different coefficients, we can determine the posterior variance $\sigma_N^2$ and the posterior mean $\mu_N$. By matching the $\mu^2$ first, we can determine the $\sigma_N^2$: \[\begin{align} -\frac{\mu^2}{2\sigma_N^2} &amp;= -\frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2}\right) \\ \frac{1}{\sigma_N^2} &amp;= \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = \frac{N\sigma_0^2 + \sigma^2}{\sigma^2\sigma_0^2}\\ \sigma_N^2 &amp;= \left( \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} \right)^{-1} = \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \end{align}\] By matching the $\mu$ coefficients, we can determine the posterior mean $\mu_N$ \[\begin{align} \frac{-2\mu \mu_N}{-2\sigma_N^2} &amp;= \mu \left( \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right) \\ \frac{\mu_N}{\sigma_N^2} &amp;= \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ &amp;= \frac{N \bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ &amp;= \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ \mu_N &amp;= \sigma_N^2 \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ &amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \cdot \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ &amp;= \frac{\sigma_0^2 n\bar{x} + \sigma^2 \mu_0}{N\sigma_0^2 + \sigma^2} \\ &amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\bar{x} \\ &amp;= \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \end{align}\] So, the two parameters for the posterior Gaussian are: \[\bbox[teal,4pt]{\begin{align} \mu_N &amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \underbrace{\bar{x}}_{\mu_{ML}} = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \\ \sigma_N^2 &amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \end{align}}\] Note that $\bar{x} = \mu_{ML} = N^{-1} \sum_{i=1}^N x_i$ as the maximum likelihood solution for $\mu$. Note that you can represent the variance as the precision of a Gaussian, which should be the inverse of the variance. \[\lambda = \frac{1}{\sigma^2} \quad \quad \lambda_0 = \frac{1}{\sigma_0^2} \quad \quad \lambda_N = \frac{1}{\sigma_N^2}\] We can write the precision of the posterior Gaussian as \[\lambda_N = \frac{1}{\sigma_N^2} = \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = N\lambda + \lambda_0\] And for the mean of the posterior using the precision, \[\mu_N = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2} \right) = \frac{\mu_0 \lambda_0 + n\bar{x} \lambda}{\lambda_N}\] Let us look at how all of these equations behave when you add more data. For each of the following, when you have $N\rightarrow \infty$, then: Mean: $\displaystyle \lim_{\color{green}{N}\rightarrow \infty} \mu_N = \lim_{\color{green}{N}\rightarrow \infty} \frac{\sigma^2}{\color{green}{N}\sigma_0^2 + \sigma^2}\mu_0 + \frac{\color{green}{N}\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} \mu_{ML} = \mu_{ML}$ Variance: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \sigma_{\color{green}{N}}^2 = \lim_{\color{green}{N\rightarrow \infty}} \frac{\sigma^2\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} = 0$ Precision: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \lambda_{\color{green}{N}} = \lim_{\color{green}{N\rightarrow \infty}} = \color{green}{N}\lambda + \lambda_0 = \infty$ This all starts to make sense intuitively. As we increase the number of examples, the mean progressively starts to get closer to the mean of the maximum likelihood of the Gaussian data. Meanwhile, the variance steadily starts to shrink to infinitesimally small values, whereas the precision starts to become infinitely high. Posterior Predictive Now, we come to compute the posterior predictive. As a recap (Bishop Eq. 2.113), if we have a marginal Gaussian distribution $p(x)$ and a conditional Gaussian distribution $p(y \vert x)$ \[\begin{align} p(x) &amp;= \texttip{\mathcal{N} (x | \mu, \color{magenta}{\Lambda^{-1}})}{Marginal} \\ p(y|x) &amp;= \mathcal{N} (y | \color{purple}{Ax + b}, \color{yellow}{L^{-1}})\end{align}\] Then to compute the marginal distribution $p(y)$ is \[p(y) = \mathcal{N} (y|\color{purple}{A\mu+b}, \color{yellow}{L^{-1}} + A\color{magenta}{\Lambda^{-1}} A^T)\] So, the posterior predictive is given by \[\begin{align} p(x|\boldsymbol{X}) &amp;= \int p(x|\mu) \cdot p(\mu|\boldsymbol{X}) d\mu \\ &amp;= \int \mathcal{N(x|\mu, \sigma^2) \cdot \mathcal{N}(\mu | \color{purple}{\mu_N}, \color{yellow}{\sigma_N^2}) d\mu } \end{align}\] \[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\color{purple}{\mu_N}, \color{yellow}{\sigma_N^2} + \color{magenta}{\sigma^2})}\] [Alternate Proof here] Marginal Likelihood The marginal likelihood $l$ can be defined as \[\bbox[4pt,teal]{\begin{align} l &amp;= p(\boldsymbol{X}) = \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\ &amp;=\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right] \end{align}}\] Proof In order to compute the marginal likelihood, let us call $m=\mu_0$ and $\tau^2 = \sigma_0^2$ as our hyperparameters for simplicity \[\begin{align} l &amp;= p(\boldsymbol{X}) \\ &amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\ &amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | m, \tau^2) d\mu \\ &amp;= \frac{1}{(\sigma \sqrt{2\pi})^n (\tau \sqrt{2\pi})} \int \exp \left( - \frac{1}{2\sigma^2 \sum_i (x_i - \mu)^2 - \frac{1}{2\tau^2} (\mu-m)^2 }\right) d\mu \end{align}\] Let us now define $S^2 = \frac{1}{\sigma^2}$ and $T^2 = \frac{1}{\tau^2}$ \(\begin{align} p(\boldsymbol{X}) &amp;= \frac{1}{(S^{-1}\sqrt{2\pi})^n (T^{-1} \sqrt{2\pi})} \int \exp \left[ -\frac{S^2}{2} \left( \sum_i x_i^2 + n \mu^2 - 2\mu \sum_i x_i\right) - \frac{T^2}{2} \left(\mu^2 +m^2-2m\mu\right)\right] \\ &amp;= \color{purple}{\frac{1}{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}} \int \color{purple}{e^{-\frac{S^2 \sum_i x_i^2}{2}}} e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} \color{purple}{e^{-\frac{T^2}{2}m^2}} e^{T^2m\mu} d\mu \\ &amp;= \underbrace{\color{purple}{\frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}}}_{c} \int e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu} d\mu \\ &amp;= c \int \color{violet}{e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu}} d\mu \\ &amp;= c \int \color{violet}{\exp \left[ -\frac{1}{2} \left( S^2N\mu^2 - 2S^2\mu \sum_i x_i +T^2 \mu^2 -2T^2 m \mu\right) \right]} d\mu \\ &amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \underbrace{\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} }_{\alpha}\right)\right] d\mu \\ &amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha\right)\right] d\mu \\ &amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha \color{violet}{+ \alpha^2 - \alpha^2} \right) \right] d\mu \\ &amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \color{purple}{+ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 }\right] d\mu \\ &amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} d\mu \\ &amp;= c \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] d\mu \\ &amp;= c \exp \left[ \frac{1}{2} \color{red}{\left( S^2N + T^2 \right) \left( \frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} \right)^2} \right] \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu \\ &amp;= c \exp \left[ \frac{1}{2} \color{red}{\frac{(S^2\sum_i x_i +T^2 m)^2}{S^2 N + T^2}} \right] \underbrace{\color{yellow}{\int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu}}_{\cdots = \frac{1.25331 erf(\frac{0.7071 -T^2m + s^2 N(\mu-\bar{x})+T^2\mu }{\sqrt{NS^2 + T^2}})}{\sqrt{NS^2 + T^2}} = \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} } \\ &amp;= c \exp \left[ \frac{1}{2} \frac{(S^2 \color{brown}{\sum_i x_i} +T^2 m)^2}{S^2 N + T^2} \right] \color{yellow}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ &amp;= c \exp \left[ \frac{1}{2} \frac{(S^2 \color{brown}{N \bar{x}}+T^2 m)^2}{S^2 N + T^2} \right] \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} \\ &amp;= \frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[ \color{violet}{ \frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ &amp;= \frac{1 }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2 \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \end{align}\) Now, let us work on the constant and the exponent portion: \[\color{lime}{\frac{\sqrt{2\pi}}{T^{-1}\sqrt{2\pi}\sqrt{S^2N + T^2}} = \frac{1}{\tau\sqrt{\frac{N}{\sigma^2} + \frac{1}{\tau^2}}}\frac{\sigma}{\sigma} = \frac{\sigma}{\sqrt{\sigma^2\tau^2 \left( \frac{n}{\sigma^2}+ \frac{1}{\tau^2} \right)}} = \frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} }\] \[\color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} = \frac{\left( \frac{1}{\sigma^2} n\bar{x} + \frac{1}{\tau^2} m \right)^2}{2 \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2}\right)} = \frac{\left(\frac{N \bar{x} \tau^2+ \sigma^2 m}{\tau^2 \sigma^2} \right)^2}{2 \left( \frac{\tau^2 N +\sigma^2}{\tau^2 \sigma^2} \right)} = \frac{(N\bar{x}\tau^2 + \sigma^2 m)^2}{2\tau^2 \sigma^2 (\tau^2 N + \sigma^2)}}\] \[\color{violet}{= \frac{N^2 \bar{x} \tau^4 + \sigma^4 m^2 + 2 \sigma^2 \tau^2 m N \bar{x}}{2 \sigma^2 \tau^2 (\tau^2 N + \sigma^2)} = \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)}}\] Therefore, bringing it all together, we obtain \[\begin{align} p(\boldsymbol{X}) &amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{(T^{-1}\sqrt{2\pi})\sqrt{S^2N + T^2}}} \\ &amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} }\right] \color{lime}{\frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} } \\ &amp;= \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\tau^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} \right] \end{align}\] By substituting the $\tau$ and $m$ terms back again, we obtain \[\bbox[teal,4pt]{p(\boldsymbol{X}) = \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\sigma_0^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 \mu_0^2}{\sigma_0^2} + 2 \mu_0 N\bar{x}}{2 (\sigma_0^2 N + \sigma)} \right] }\] Conclusion Bringing it all together, if one is to have a dataset $\boldsymbol{X}$ with $N$ examples, which are all univariate, the likelihood of obtaining the dataset given some mean $\mu$, is \[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) = \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\] The prior distribution for the mean $\mu$ has a prior mean $\mu_0$ and variance $\sigma_0^2$ being represented as \[\bbox[teal,4pt]{p(\mu) = \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\] The posterior distribution for the mean given the dataset can be described as \[\bbox[teal,4pt]{p(\mu | \boldsymbol{X} ) = \mathcal{N} \left( \mu \bigg| \underbrace{\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \bar{x}}_{\mu_N}, \underbrace{\frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}}_{\sigma^2_N} \right) }\] And in order to identify the probability of a probe sample $x$ given the dataset $\boldsymbol{X}$ can be described as \[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)}\] Finally, to obtain the marginal likelihood of the dataset, one can compute as \[\bbox[4pt,teal]{p(\boldsymbol{X}) =\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]}\] Sources Kevin P. Murphy “Conjugate Bayesian Analysis of the Gaussian Distribution” 3 Oct 2007 Christopher Bishop - Pattern Recognition]]></summary></entry><entry><title type="html">Probabilistic Linear Discriminant Analysis</title><link href="http://localhost:4000/plda_python.html" rel="alternate" type="text/html" title="Probabilistic Linear Discriminant Analysis" /><published>2021-01-30T00:00:00-05:00</published><updated>2021-01-30T00:00:00-05:00</updated><id>http://localhost:4000/plda</id><content type="html" xml:base="http://localhost:4000/plda_python.html"><![CDATA[<p>I need to note that a lot of this post was inspired by RaviSoji’s <a href="https://github.com/RaviSoji/plda">PLDA implementation</a></p>

<p>Let us say that we have a training dataset $X \in \mathbb{R}^{N\times F}$ and $y \in \mathbb{R}^N$, where $N$ is the number of examples in the whole dataset, $F$ is the number of features in the dataset, $K$ specifies the number of classes that there are, and $\boldsymbol{m} \in \mathbb{R}^F$ represents the mean vector of the entire dataset $X$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">"/home/data/MNIST"</span><span class="p">)</span>
<span class="n">ds</span><span class="p">.</span><span class="n">gz</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="c1">#.reshape(-1,28,28)
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">F</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>For a classification problem we will create several subspaces:</p>
<ul>
  <li>$\mathcal{D}$ represents the space where the data lives in</li>
  <li>$\mathcal{X}$ represents the subspace found via PCA where the data in $\mathcal{D}$ gets transformed to the subspace $\mathcal{X}$. Here, you may consider $\mathcal{X}$ to be the space of the preprocessed state</li>
  <li>$\mathcal{U}$ represents the subspace found via PLDA, where the data will be ultimately transformed to, which is described in detail by Ioffe 2006</li>
  <li>$\mathcal{U}_{model}$ represents the subspace within $\mathcal{U}$ which contains only the dimensions that are relevant to the problem</li>
</ul>

<p>So, in general, the data will first flow in the following fashion
\(\mathcal{D} \leftrightarrow \mathcal{X} \leftrightarrow \mathcal{U} \leftrightarrow \mathcal{U}_{model}\)</p>

<p>First, for a classification problem, we perform a PCA to reduce the dimensionality, and get rid of features that may not be very important. In other words, we will bring the data from $\mathcal{D}$ to $\mathcal{X}$. For this one needs to determine the number of components that one would want to take into account for in its subspace, and set that as the matrix rank. This can be predefined or not. If it is not defined, we compute the covariance matrices for each of the classes (i.e. a between-class covariance matrix and a within-class covariance matrix per class $k$). In other words</p>

<p>For each class</p>
<ol>
  <li>Compute the mean vectors $m_k \in \mathbb{R}^F$</li>
  <li>Compute the covariance matrix $\sigma_k \in \mathbb{R}^{F\times F}$</li>
  <li>Compute the between-class covariance matrix per class $S_{b,k} \in \mathbb{R}^{F\times F}$
 \(S_{b,k} = \frac{n_k}{N}\underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times 1}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{1\times F}}\)</li>
  <li>Compute the within-class covariance matrix per class $S_{w,k} \in \mathbb{R}^{F\times F}$
 \(S_{w,k} = \frac{n_k-1}{N} \odot \sigma_{k}\)</li>
</ol>

<p>Then compute the within-class and between-class covariance matrices, $S_w \in \mathbb{R}^{F\times F}$ and $S_b \in \mathbb{R}^{F\times F}$ respectively. If one was to set \(\boldsymbol{m}_{ks} \in \mathbb{R}^{K\times F}\) as the matrix representing all of the mean vectors, $\boldsymbol{\sigma_{ks} \in \mathbb{R}^{K\times F \times F}}$ the tensor representing all of the class covariances, and $n_{ks}\in \mathbb{R}^K$ a vector representing all of the number of examples in each class, it is possible to vectorize it all as</p>

\[\begin{align}
    S_b &amp;= \underbrace{\frac{n_{ks}}{N} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times K}}}_{\mathbb{R}^{F\times K}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{K\times F}} \\
    S_w &amp;= \sum_{k} S_{w,k} \\ 
    &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \odot \underbrace{\frac{n_k-1}{N}}_{\mathbb{R}^K} \\
    &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \cdot \underbrace{\frac{n_k-1}{N}\text{[:, None, None]}}_{\mathbb{R}^{K\times 1 \times 1}}
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_principal_components</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># This might overfit
</span>
<span class="k">if</span> <span class="n">n_principal_components</span><span class="p">:</span>
    <span class="n">matrix_rank</span> <span class="o">=</span> <span class="n">n_principal_components</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">m_ks</span><span class="p">,</span> <span class="n">sigma_ks</span><span class="p">,</span> <span class="n">n_ks</span> <span class="o">=</span> <span class="p">[],[],[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Get only the data associated with class k
</span>        <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
        
        <span class="c1"># Compute the mean, number of samples, and class covariance
</span>        <span class="n">m_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>
        <span class="n">sigma_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Append them all
</span>        <span class="n">m_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">m_k</span><span class="p">)</span>
        <span class="n">n_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_k</span><span class="p">)</span>
        <span class="n">sigma_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_k</span><span class="p">)</span>
    <span class="n">m_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">m_ks</span><span class="p">)</span>
    <span class="n">n_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_ks</span><span class="p">)</span>
    <span class="n">sigma_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sigma_ks</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">m_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">n_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">sigma_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    
    <span class="n">S_b</span> <span class="o">=</span>  <span class="p">((</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">T</span> <span class="o">*</span> <span class="n">n_ks</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>  <span class="o">@</span> <span class="p">(</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">S_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_ks</span> <span class="o">*</span> <span class="p">((</span><span class="n">n_ks</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">matrix_rank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">S_w</span><span class="p">)</span>

<span class="k">if</span> <span class="n">F</span> <span class="o">!=</span> <span class="n">matrix_rank</span><span class="p">:</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">matrix_rank</span><span class="p">)</span>
    <span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, there are going to be several transformations:</p>
<ul>
  <li>$\mathcal{D} \rightarrow \mathcal{X}$</li>
</ul>

<blockquote>
  <p>Here, there are two case scenarios. If PCA was defined in order to reduce the dimensions, then the data in $\mathcal{D}$ will be transformed via PCA. Otherwise, you can return the data itself</p>
</blockquote>

<ul>
  <li>$\mathcal{X} \rightarrow \mathcal{D}$</li>
</ul>

<blockquote>
  <p>In this case, it is very similar to the converse. If PCA was defined, in order to bring it back to the original data space $\mathcal{D}$, you need to inverse transform the data. Otherwise, just return the data itself</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_D_to_X</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">pca</span>
    <span class="k">if</span> <span class="n">pca</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">transform_from_X_to_D</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">pca</span>
    <span class="k">if</span> <span class="n">pca</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>So, at this point, we convert the training data from $\mathcal{D}$ to the space $\mathcal{X}$, having the data be represented as $X_{pca}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_pca</span> <span class="o">=</span> <span class="n">transform_from_D_to_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of X_pca ="</span><span class="p">,</span><span class="n">X_pca</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Shape of X_pca = (60000, 712)
</span></code></pre></div></div>

<p>In the PLDA, we use a Gaussian mixture model, where $\boldsymbol{x}$ retpresents a sample in the mixture, and $\mathcal{y}$ represents the center of a mixture component. In general, the class-conditional distributions is represented by</p>

\[P(\boldsymbol{x} | \boldsymbol{y}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{y}, \Phi_w)\]

<p>where all of the class-conditional distributions share one common covariance 
[I DON’T KNOW WHY THEY ALL SHARE THE SAME COVARIANCE]</p>

<p>If we recall, the <strong>LDA formulation</strong> is the result if we were to set $\mu_k$ values to be constrainted to be in a lower dimension, and perform the likelihood maximization with respect to $\mu_k$, $\pi_k$, and $\Phi_w$, where the priord of the class variable $\boldsymbol{y}$ is set to put a probability mass on each of the points</p>

\[P_{LDA}(\boldsymbol{y}) = \sum_{k=1}^{K} \pi_k \delta(\boldsymbol{y}-\mu_k)\]

<p>But that won’t be the case in the <strong>PLDA formulation</strong>. Instead, PLDA sets it so taht the prior is not to be within a discrete set of values, but instead, sampled from a Gaussian prior.</p>

\[P_{PLDA}(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{m}, \Phi_b)\]

<p>Note that this normal distribution $P_{PLDA}(y)$ uses the mean of the full dataset. This formulation makes it such that $\Phi_w$ is positive definite, and $\Phi_b$ is positive semi-definite. Theoretically, it is possible to find a transformation $V$ which can simultaneously diagonalize $\Phi_b$ and $\Phi_w$</p>

\[\begin{align}
    V^T \Phi_b V &amp;= \Psi \\
    V^T \Phi_w V &amp;= I
\end{align}\]

<p>We can define $A = V^{-T} = \text{inv}(V^T)$, resulting in</p>

\[\begin{align}
    \Phi_w &amp;= AA^T \\
    \Phi_b &amp;= A\Psi A^T
\end{align}\]

<p>Thus, the <strong>PLDA model</strong> is defined as:</p>

\[\bbox[teal, 4pt]{\begin{align}
\boldsymbol{x} &amp;= \boldsymbol{m} + A \boldsymbol{u} \quad \text{where} \\
&amp; \boldsymbol{u} \sim \mathcal{N}(\cdot | \boldsymbol{v}, I) \\
&amp; \boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)
\end{align}}\]

<p><img src="assets/images/lda_plda/v_u_x.png" alt="drawing" align="middle" style="width:300px;" /></p>

<p>Here, $\boldsymbol{u}$ represents the sample data representation of $\boldsymbol{x}$, but projected in the lated projected space $\mathcal{U}$, and $\boldsymbol{v}$ represents the sample label in the lated projected space. These transformations can be computed via</p>

\[\boldsymbol{x} = \boldsymbol{m} + A \boldsymbol{u} \quad \leftrightarrow \quad \boldsymbol{u} = V^T (\boldsymbol{x} - \boldsymbol{m})\]

\[\boldsymbol{y} = \boldsymbol{m} + A \boldsymbol{v} \quad \leftrightarrow \quad \boldsymbol{v} = V^T (\boldsymbol{y} - \boldsymbol{m})\]

<p>And from this point on, we determine the optimal $\boldsymbol{m}$, $A$, and $\Psi$ are.</p>

<p>S. Ioffe: “<em>In the training data, the grouping of examples into clusters is given, and we learn the model parameters by maximizing the likelihood. If, instead, the model parameters are fixed, likelihood maximization with respect to the class assignment labels solves a clustering problem</em>”</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_X_to_U</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">m</span><span class="p">,</span> <span class="n">A</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">m</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transform_from_U_to_X</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">m</span><span class="p">,</span> <span class="n">A</span>
    <span class="k">return</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>But, at this point, we don’t know what the parameters $A$ or $\Psi$ are, so we can’t use these functions yet.</p>

<h3 id="learning-the-model-parameters-boldsymbolm-psi-a">Learning the Model Parameters ($\boldsymbol{m}, \Psi, A$)</h3>
<p>The loading matrix $A$ is essentially finding the variances $\Phi_b$ and $\Phi_w$, and all of the parameters can be defined using a maximum likelihood framework. Let us say that $D_k$ represents the dataset which contains only samples from the $k^{th}$ class, and $\boldsymbol{x}_k^i$ represents the $i^{th}$ sample from $D_k$, and it belongs to the $k^{th}$ class. Given $N$ training examples separated into $K$ classes, and assuming that they are all independently drawm from their respective class, the log likelihood is</p>

\[l(\boldsymbol{x}^{1 \cdots N}) = \sum_{k=1}^K \ln P(\boldsymbol{x}^i : i \in D_k) = \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k)\]

<p>where the joint probability distribution of a set of $n$ patterns (assuming all these $n$ patterns belong to the same class $k$) is:</p>

\[\begin{align}
P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) &amp;= \int \color{red}{P(\boldsymbol{x}^1_k | \boldsymbol{y})} \cdots \color{cyan}{P(\boldsymbol{x}^n_k | \boldsymbol{y})} \color{magenta}{P(\boldsymbol{y})} d\boldsymbol{y} \\
&amp;= \int \color{red}{\mathcal{N}(\boldsymbol{x}^1_k | \boldsymbol{y}, \Phi_w)} \cdots \color{cyan}{\mathcal{N}(\boldsymbol{x}^n_k | \boldsymbol{y}, \Phi_w)} \color{magenta}{\mathcal{N}(\boldsymbol{y} | 0, \Phi_b)} d\boldsymbol{y}
\end{align}\]

<p>By computing the integral, we obtain</p>

\[\color{red}{MAGIC}\]

\[\ln P(\boldsymbol{x}^1_k,\cdots, \boldsymbol{x}^n_k) = C - \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} (\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T\right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} ( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)\right)\right]\]

<p>where \(\bar{\boldsymbol{x}}_k = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}^i_k\), and $C$ is a constant that can be ignored</p>

<p>At this point, as a “hack”, it sets the number of examples for each class to be $n$. In other words, every class ends up having exactly $n$ examples to learn from. Now, if one were to maximize the equation $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\boldsymbol{m}$, one would obtain $\boldsymbol{m}^* = \frac{1}{N} \sum_i \boldsymbol{x}^i$. If one substitutes it back, one would get</p>

\[\begin{align}
l(\boldsymbol{x}^{1\cdots N})
&amp;= \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) \\
&amp;= -\sum_{k=1}^K \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} \color{cyan}{(\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T}  \right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} \color{red}{( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)} \right)  \right] \\
&amp;= \cdots \\
&amp;= \color{red}{\text{MAGIC}} \\
&amp;= \cdots \\
&amp;= - \frac{c}{2} \left[ \ln |\Phi_b + \frac{\Phi_w}{n} | + \text{tr} \left( (\Phi_b + \frac{\Phi_w}{n})^{-1} \color{cyan}{S_b} \right) + (n-1) \ln |\Phi_w | + n \text{tr} (\Phi_w^{-1} \color{red}{S_w}) \right]
\end{align}\]

<p>Now, we need to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Phi_b$ and $\Phi_w$ subject to $\Phi_w$ being p.d. and $\Phi_b$ being p.s.d. <strong>Without</strong> these constraints, we would obtain</p>

\[\Phi_w = \frac{n}{n-1} S_w \quad \text{and} \quad \Phi_b = S_b - \frac{1}{n-1} S_w\]

<p>Therefore, if $S_w$ and $S_b$ are diagonal, then the covariances $\Phi_w$ and $\Phi_b$ will also be diagonal, and the diagonalization property holds as long as the contraints above are satisfied.</p>

<p>As we have previously stated, we know that</p>

\[\Phi_b = A \Psi A^T\]

<p>If you fix $\Psi$ and maximize $l(\boldsymbol{x}^{1\cdots N})$ via unconstrained optimization with respect to $A^{-1}$ will make $A^{-1}S_b A^{-T}$ and $A^{-1}S_w A^{-T}$, making the $A^{-T}$ to be the solution of the generalized eigenvector problem involving $S_b$ and $S_w$, where $S_b \boldsymbol{v} = \lambda S_w \boldsymbol{v}$. Then, the projection of the data to the latent space with the LDA projection. [REVIEW]</p>

<p>Then, if you were to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Psi$ subject to $\Psi \geq 0$ and $\text{rank}(\Psi) \leq \hat{F}$, then we’ll get the method to optimize the model [REVIEW]</p>

<p>Ioffe 2006: “<em>Our method was derived for the case where each class in the training data is represented by the same number $n$ of examples. This may not be true in practice, in which case, we can resample the data to make the number of examples the same, use EM (as shown in section 5), or use approximations. We took the latter approach, using the closed-form solution in Fig. 2, where $n$ was taken to be the average number of examples per class</em>”</p>

<h2 id="algorithm-plda-optimization"><strong>Algorithm</strong>: PLDA Optimization</h2>
<blockquote>
  <p><strong><em>Input</em>:</strong> Training $N$ examples from $K$ classes, with $n = N/K$ per class<br />
<strong><em>Output</em>:</strong> Parameters $\boldsymbol{m}, A, \Psi$, maximizing the likelihood of the PLDA model</p>
</blockquote>

<blockquote>
  <ol>
    <li>Compute the covariance matrices $S_b$, and $S_w$</li>
    <li>Compute the transformation matrix $W$ such that $S_b \boldsymbol{w} = \lambda S_w \boldsymbol{w}$ (i.e. $eig(S_w^{-1}S_b)$)</li>
    <li>Compute the covariance matrices in the latent space
      <ul>
        <li>$\Lambda_b = W^T S_b W$</li>
        <li>$\Lambda_w = W^T S_w W$</li>
      </ul>
    </li>
    <li>Determine the following parameters
      <ul>
        <li>$\boldsymbol{m} = \frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$</li>
        <li>$A = W^{-T} \left( \frac{n}{n-1} \Lambda_w \right)^{1/2}$</li>
        <li>$\Psi = \max \left( 0, \frac{n-1}{n} (\Lambda_b / \Lambda_w) - \frac{1}{n} \right)$</li>
      </ul>
    </li>
    <li>Reduce the dimensionality to $\hat{F}$ by keeping the largest elements of $\Psi$, while setting the rest to zero.</li>
    <li>In the latent space $\boldsymbol{u} = A^{-1}(\boldsymbol{x}-\boldsymbol{m})$, only the features for non-zero entries are needed for recognition</li>
  </ol>
</blockquote>

<p>Note:</p>
<blockquote>
  <p>Scipy’s <code class="language-plaintext highlighter-rouge">eigh(A,B)</code> function in <code class="language-plaintext highlighter-rouge">linalg</code> solves the generalized eigenvalue problem for a complex Hermitian or a real symmetric matrix, so that $A\boldsymbol{v} = \lambda B \boldsymbol{v}$. Otherwise, if <code class="language-plaintext highlighter-rouge">B</code> is omitted, then it is assumed that $B=I$<br />
Also note that <code class="language-plaintext highlighter-rouge">scipy.linalg.eigh != np.linalg.eigh</code></p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_Sb_Sw</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span><span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">m_ks</span><span class="p">,</span> <span class="n">sigma_ks</span><span class="p">,</span> <span class="n">n_ks</span> <span class="o">=</span> <span class="p">[],[],[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Get only the data associated with class k
</span>        <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
        
        <span class="c1"># Compute the mean, number of samples, and class covariance
</span>        <span class="n">m_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>
        <span class="n">sigma_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Append them all
</span>        <span class="n">m_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">m_k</span><span class="p">)</span>
        <span class="n">n_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_k</span><span class="p">)</span>
        <span class="n">sigma_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_k</span><span class="p">)</span>
    <span class="n">m_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">m_ks</span><span class="p">)</span>
    <span class="n">n_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_ks</span><span class="p">)</span>
    <span class="n">sigma_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sigma_ks</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">m_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">n_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">sigma_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    
    <span class="n">S_b</span> <span class="o">=</span>  <span class="p">((</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">T</span> <span class="o">*</span> <span class="n">n_ks</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>  <span class="o">@</span> <span class="p">(</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">S_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_ks</span> <span class="o">*</span> <span class="p">((</span><span class="n">n_ks</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">N</span><span class="o">/</span><span class="n">K</span>
<span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span> <span class="o">=</span> <span class="n">compute_Sb_Sw</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute W
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">eigvecs</span>

<span class="c1"># Compute Lambdas
</span><span class="n">Lambda_b</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_b</span> <span class="o">@</span> <span class="n">W</span>
<span class="n">Lambda_w</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_w</span> <span class="o">@</span> <span class="n">W</span>

<span class="c1"># Compute A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Lambda_w</span><span class="p">))</span><span class="o">**</span><span class="mf">0.5</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Compute Psi
</span><span class="n">diag_Lambda_w</span> <span class="o">=</span> <span class="n">Lambda_w</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span>
<span class="n">diag_Lambda_b</span> <span class="o">=</span> <span class="n">Lambda_b</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span>

<span class="n">Psi</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">diag_Lambda_b</span><span class="o">/</span><span class="n">diag_Lambda_w</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="n">Psi</span><span class="p">[</span> <span class="n">Psi</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Psi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Psi</span><span class="p">)</span>
</code></pre></div></div>

<p>From this point you can transform the data that is in the PCA subspace $\mathcal{X}$ to the $\mathcal{U}$ subspace, having the data be represented as $U$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span> <span class="o">=</span> <span class="n">transform_from_X_to_U</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, again, not every dimension will be relevant in the $\mathcal{U}$ subspace, and that is why we reduce the $\mathcal{U}$ to \(\mathcal{U}_{model}\) , which only contains the relevant dimensions of $\mathcal{U}$. Therefore, in order to go back and forth in between the two subspaces, simply drop them the irrelevant dimensions or add the relevant dimensions to a zero matrix. This new data will be represented as $U_{model}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_U_to_Umodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dims</span><span class="p">):</span>
    <span class="n">u_model</span> <span class="o">=</span> <span class="n">u</span><span class="p">[...,</span><span class="n">dims</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">u_model</span> 

<span class="k">def</span> <span class="nf">transform_from_Umodel_to_U</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dims</span><span class="p">,</span><span class="n">u_dim</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">u_dim</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">u</span><span class="p">[...,</span> <span class="n">dims</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">u</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute the relevant dimensions of Psi
</span><span class="n">relevant_dims</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Psi</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="n">relevant_dims</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">relevant_dims</span> <span class="o">=</span> <span class="n">relevant_dims</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

<span class="n">U_model</span> <span class="o">=</span> <span class="n">transform_from_U_to_Umodel</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span><span class="n">relevant_dims</span><span class="p">)</span>
</code></pre></div></div>

<p>Great! Now we we have the data in the $\mathcal{U}_{model}$ space. Now, all that there is left is to understand how to perform inference, and in order to do that, one needs to find the prior parameters for $\boldsymbol{v}$, the posterior parameters, and the posterior predictive parameters.  Let’s discuss now how to find the probability parameters.</p>

<h2 id="inference-on-the-latent-space">Inference on the Latent Space</h2>

<p>First, if you need some review on Bayesian inference for Gaussian distributions, you may check the other <a href="https://nicolasshu.com/bayesian_inference_for_gaussian.html">post</a> to understand priors, posteriors, posterior predictives, and marginal probability. For this problem, note that the different dimensions have been decorrelated (i.e. the covariances have been decorrelated), thus the different dimensions could be treated as univariate problems.</p>

<p>We need to determing the prior parameters of $\boldsymbol{v}$, which leads to the probability distribution $P(\boldsymbol{v})$, the posterior parameters, which leads to \(P(\boldsymbol{v} \vert \boldsymbol{u})\) , and the posterior predictive parameters, for \(P(\boldsymbol{u}^p \vert \boldsymbol{u}^g_{1\cdots n})\)</p>

<h3 id="prior-distribution">Prior Distribution</h3>

<p>The easiest to determine right off the bat are the prior parameters. For the prior parameters, as one may recall in the model formulation</p>

\[\boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)\]

<p>which, in turn, are simple to compute. Here we’ll call $\mu_{prior}$ the prior mean, and $\Sigma_{prior}$ the prior covariance, making</p>

\[\bbox[teal, 4pt]{\begin{align}
\mu^{prior} &amp;= \boldsymbol{0} \\
\Sigma^{prior} &amp;= \Psi_{\forall d \in D}
\end{align}}\]

<p>where $D$ represents all of the relevant dimensions, which are all that the variances are not zero. Then from this point, we’ll use the notation setting $\hat{\Psi} = \Psi_{\forall d \in D}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"mean"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">relevant_dims</span><span class="p">),</span>
    <span class="s">"cov"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Psi</span><span class="p">)[</span><span class="n">relevant_dims</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now, the more involved ones are the posteriors parameters.</p>

<p><strong>One advantage that PLDA has is that it allows one to make inferences about classes not present during training.</strong> Let us consider the following case of classification first. We are given a set of data to learn from, which Ioffe refers to as a “gallery”. This set ${ \boldsymbol{x}^1, \cdots,\boldsymbol{x}^k, \cdots, \boldsymbol{x}^K  }$ contains $K$ examples, with one example from each of the $K$ classes. We are also given a probe example $\boldsymbol{x}^p$, and assume that it belongs to one of the $K$ classes. If we are to determine to which class it belongs, maximizing the likelihood will do the job. This can be more easily accomplised in the lated space by performing the trnasformation $\boldsymbol{u} = A^{-1}(\boldsymbol{x} - \boldsymbol{m})$, since it will decorrelate the data. For this example, $\boldsymbol{x}^p$ will be transformed to $\boldsymbol{u}^p$.</p>

<p>Next we will see how to compute the posterior distribution and the posterior predictive distribution. We will, however look at two different cases. We will first look in the case where we have a single training example per class (in deep learning, known as $k$-way-one-shot learning), and then we’ll look at the case where we see multiple examples per class (also known as $k$-way-few-shot learning).</p>

<h3 id="case-single-training-example-per-class">Case: Single Training Example per Class</h3>
<h4 id="posterior-distribution">Posterior Distribution</h4>
<p>Let us consider an example $\boldsymbol{u}^g$ from the training set (i.e. gallery), where, again, it belongs to some class between $1 \cdots K$ The probability that the probe $\boldsymbol{u}^p$ belongs to the same class as $\boldsymbol{u}^g$ is defined by the probability $P(\boldsymbol{u}^p | \boldsymbol{u}^g)$.</p>

<p>So, the posterior probability will provide a way to perform inference on the class variable $\boldsymbol{v}$ (i.e. the transformed version of $\boldsymbol{y}$). As we may remember, the parameters for a posterior Gaussian are</p>

\[\begin{align}
\mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Now, since we only have a single sample $\bar{x} = \boldsymbol{u}$. Additionally, we know that the mean for the prior $\mu^{prior}$ is all zeros (since the data has been centralized), then</p>

\[\begin{align}
\mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Additionally, since we would be looking at a single class at this point and the covariances have been diagonalized, the within-class covariance is an identity matrix, making the $(\sigma^{prior})^2 = 1$. Therefore the parameters turn into</p>

\[\begin{align}
\mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{1(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1}
\end{align}\]

<p>And, once again, since we are dealing with a single sample, $N=1$</p>

\[\begin{align}
\mu^{post} &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1}
\end{align}\]

<p>The posterior for a single training example can then be defined as</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}) = \mathcal{N}\left(\boldsymbol{v} \bigg| \frac{\hat{\Psi}}{\hat{\Psi} + I}\boldsymbol{u}, \frac{\hat{\Psi}}{\hat{\Psi} + I}\right)}\]

<p>Now, if we see how this flows, the class variable $\boldsymbol{v}$ will be used to determine $\boldsymbol{u}$ examples, which are then used to determine the data $\boldsymbol{x}$.</p>

<h4 id="posterior-predictive-distribution">Posterior Predictive Distribution</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">()</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'v'</span><span class="p">,</span><span class="s">"v"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'1'</span><span class="p">,</span><span class="s">"u1"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'2'</span><span class="p">,</span><span class="s">"u2"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">"a"</span><span class="p">,</span><span class="s">"x1"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span><span class="s">"x2"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">edges</span><span class="p">([</span><span class="s">'v1'</span><span class="p">,</span><span class="s">"v2"</span><span class="p">,</span><span class="s">"1a"</span><span class="p">,</span><span class="s">"2b"</span><span class="p">])</span>
<span class="n">dot</span>
</code></pre></div></div>
<p><img src="assets/images/lda_plda/v_u_x.png" alt="drawing" align="middle" style="width:300px;" /></p>

<p>From probabilistic graphical models, if we observe $\boldsymbol{v}$ (i.e. $\boldsymbol{v}$ is given), then $\boldsymbol{u}^p$ and $\boldsymbol{u}^g$ are conditionally independent. We also know that the posterior predictive probability is</p>

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu^{post}, (\sigma^{post})^2 + \sigma^2)\]

<p>And since the within class variance has been diagonalized to an identity matrix ($\sigma^2 = 1$), we’ll obtain</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}^g) = \mathcal{N}\left(\boldsymbol{u}^p \bigg| \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \boldsymbol{u}^g, I + \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \right)}\]

<p>In order to classify $\boldsymbol{u}^p$, then we compute $P(\boldsymbol{u}^p \vert \boldsymbol{u}^g) \forall g \in {1,\cdots,M }$, and pick the maximum.</p>

<p>Ioffe 2006: “<em>With PLDA, we were able to combine the knowledge about the general structure of the data, obtained during training, and the examples of new classes, yielding a principled way to perform classification</em>”</p>

<p>Now that we’ve seen the single example case, let’s expand this to the multiple example case.</p>

<h3 id="case-multiple-training-examples-per-class">Case: Multiple Training Examples Per Class</h3>
<h4 id="posterior-distribution-1">Posterior Distribution</h4>
<p>We can improve the recognition performance by using more examples. Let us say that we have $n_k$ examples from class $k$, making</p>

\[n_k = |U_{model,k}|\]

<p>These examples are all independent examples $\boldsymbol{u}_{1\cdots n}^g$. Just as before, we know that, here, we are looking at a single class $k$. We have previously diagonalized all of the covariance matrices, making all of the dimensions (i.e. features) decorrelated, thus they can be worked on individually as if they were univariate features. This also means that the within-class covariance is an identity matrix, making $\sigma^2 = 1$. As we may remember, the parameters for a posterior Gaussian are</p>

\[\begin{align}
\mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Again, since our model estimates that the prior mean $\mu^{prior}$ is all zeros, then</p>

\[\begin{align}
\mu^{post} &amp;=  \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Since we’re looking at the class $k$, then $N=n_k$ and $\bar{x} = \bar{\boldsymbol{u}}_k$. Thus the posterior for multiple samples is</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}_k^{1\cdots n_k} ) = \mathcal{N}\left(\boldsymbol{v} \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi}+I}\bar{\boldsymbol{u}}_k, \dfrac{\hat{\Psi}}{n_k \hat{\Psi}+I} \right) }\]

<p>Therefore, in order to compute the posterior parameters, where $\mu_k^{post}$ and $\Sigma_k^{post}$ are the mean posterior and the covariance posterior for each class $k$, we have</p>

\[\bbox[teal, 4pt]{\begin{align}
\Sigma^{post}_k &amp;= \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = \Sigma^{prior} \odot \frac{1}{1 + n_k \cdot \Sigma^{prior}} \\
\mu_k^{post} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \underbrace{\left( \sum_{\boldsymbol{u} \in U_{model,k}} \boldsymbol{u} \right)}_{n_k \bar{\boldsymbol{u}}_k} \cdot \Sigma^{post}_k 
\end{align}}\]

<p>where we can recall that $\Sigma^{prior} = \hat{\Psi}$, and $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posterior_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">u_model_k</span> <span class="o">=</span> <span class="n">u_model</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_model_k</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">n_k</span> <span class="o">*</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">])</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u_model_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cov</span>
    <span class="n">posterior_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s">"mean"</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s">"cov"</span><span class="p">:</span><span class="n">cov</span><span class="p">}</span>
</code></pre></div></div>

<h4 id="posterior-predictive-distribution-1">Posterior Predictive Distribution</h4>

<p>As per page 535 by Ioffe 2006, if one sets multiple examples of a class to a single model, assuming we have $n_k$ independent examples \(\{ \boldsymbol{u}_k^i \}_{i=1}^{n_k}\), then the probability of obtaining a sample $\boldsymbol{u}^p$, given the set above, can be obtained from the posterior predictive</p>

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)\]

<p>Once again, since the within-class covariance has been diagonalized to an identity matrix, then $\sigma^2 = 1$</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}_k^{1\cdots n_k}) = P(\boldsymbol{u}^p | \boldsymbol{u}_k^1, \cdots, \boldsymbol{u}_k^{n_k}) = \mathcal{N} \left( \boldsymbol{u}^p \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k, I + \dfrac{\hat{\Psi}}{n_k \hat{\Psi} + I} \right) }\]

<p>where $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$</p>

<p>This means that, you may compute the predictive parameters as copying the posterior parameters ($\mu_k^{postpred}$, $\Sigma_k^{postpred}$), and adding an identity matrix to the covariance</p>

\[\bbox[teal, 4pt]{\begin{align}
\mu_k^{postpred} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \mu_k^{post} \\
\Sigma_k^{postpred} &amp;= I + \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = I + \Sigma_k^{post}
\end{align}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">post_pred_params</span> <span class="o">=</span> <span class="n">posterior_params</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">params</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<h3 id="summary-and-inference">Summary and Inference</h3>

<p>Great! This is all that we need to precompute before performing any inference! At this point, you have determined all of the parameters which describe you data. As a review, in order to prepare this algorithm, we have received a training dataset $X$ and its corresponding labels $y$. Then, we have decided on a certain number of components to be used for a PCA. This PCA will bring the data from the space $\mathcal{D}$ to the space $\mathcal{X}$, obtaining the data $X_{pca}$. Then we found the parameters $\boldsymbol{m}$, $A$, and $\hat{\Psi}$, which optimize the PLDA formulation. These would then allow us to bring the data from the $\mathcal{X}$ subspace to the $\mathcal{U}$ latent space, obtaining the data $U$. Then, we reduced the dimensions of $U$ by discarding any dimensions which had a zero variance in $\hat{\Psi}$, yielding the data $U_{model}$. Finally, using these same parameters, we obtain the prior parameters ($\mu^{prior}$ and $\Sigma^{prior}$), the posterior parameters ($\mu^{post}_k$ and $\Sigma^{post}_k$), and the posterior predictive parameters ($\mu^{postpred}_k$ and $\Sigma^{postpred}_k$)</p>

<p>Now, we’re ready to do the inference on the latent space</p>

<p>We have previously established that we have the classes (i.e. categories) $1,\cdots, K$. What we will do is to iterate through each of the possible classes, and compute its posterior probabilities by using the parameters computed for the posterior predictive probabilities and creating a Gaussian distribution. Finally, we can obtain the probability on that Gaussian at each of those locations. We can also use the log probability for each of the samples on each of the classes</p>

\[\bbox[teal, 4pt]{\begin{align}
P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) &amp;= \mathcal{N}(\boldsymbol{u}_{model} | \mu_k^{postpred}, \Sigma_k^{postpred}) \\ 
\boldsymbol{y}^* &amp;= \arg\max_{k} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k)
\end{align}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span> <span class="k">as</span> <span class="n">gaussian</span>
<span class="n">log_prob_post</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">mean</span><span class="p">,</span><span class="n">cov</span> <span class="o">=</span> <span class="n">param</span><span class="p">[</span><span class="s">"mean"</span><span class="p">],</span> <span class="n">param</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span>
    <span class="n">log_probs_k</span> <span class="o">=</span> <span class="n">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">)).</span><span class="n">logpdf</span><span class="p">(</span><span class="n">U_model</span><span class="p">)</span>
    <span class="n">log_prob_post</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_probs_k</span><span class="p">)</span>
<span class="n">log_prob_post</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_prob_post</span><span class="p">).</span><span class="n">T</span>
</code></pre></div></div>

<p>Here, you may choose to normalize the probabilities</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob_post</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_prob_post</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_prob_post</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categories</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">keys</span><span class="p">()])</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">categories</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">count</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">title</span> <span class="o">=</span> <span class="s">"True: {} | Pred: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">count</span><span class="p">],</span><span class="n">predictions</span><span class="p">[</span><span class="n">count</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">count</span><span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>If you wanted to extract the LDA features, you could simply use the transformation functions to convert the data from some space to another space.</p>

<p>So this is the case when we have a classification problem where the probes are assumed to belong to one of the trained classes. Now, let us look at the case where the probes belong to classes not yet seen.</p>

<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<p>Here, we try to determine whether two samples belong to the same class or not. For that, we can compute the following likelihoods</p>

\[\begin{align}
P(\boldsymbol{u}^p)P(\boldsymbol{u}^g) &amp;= \text{likelihood of examples belonging to different classes} \\
P(\boldsymbol{u}^p, \boldsymbol{u}^g) &amp;= \int P(\boldsymbol{u}^p | \boldsymbol{v}) P(\boldsymbol{u}^g|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ 
&amp;= \text{likelihood of examples belonging to the same class}
\end{align}\]

<p>As a generalized formulation where there are multiple examples, the likelihood ratio is</p>

\[\begin{align}
R(\{\boldsymbol{u}^{1\cdots m}_p\},\{\boldsymbol{u}^{1\cdots n}_g\}) &amp;= \frac{\text{likelihood(same)}}{\text{likelihood(diff)}} = \frac{P(\boldsymbol{u}^{1\cdots m}_p,\boldsymbol{u}^{1\cdots n}_g)}{P(\boldsymbol{u}^{1\cdots m}_p)P(\boldsymbol{u}^{1\cdots n}_g)} \\
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;= \int P(\boldsymbol{u}^1 | \boldsymbol{v}) \cdots P(\boldsymbol{u}^n|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\
&amp;= \prod_{t=1}^d \frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} \exp \left( - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} - \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} \right)
\end{align}\]

<p>where</p>

\[\bar{u}_t = \frac{1}{n}\sum_{i=1}^n u_t^i\]

<p>For the priors $\pi_{\text{same}}$ and $\pi_{\text{diff}}$, the probability that all of the examples are in the same class is</p>

\[\left(1 + \dfrac{\pi_{\text{diff}} / \pi_{\text{same}} }{R} \right)^{-1} = \dfrac{R}{R+\pi_{\text{diff}} / \pi_{\text{same}}}\]

<p><span style="color:red">
If $R &gt; \pi_{\text{diff}} / \pi_{\text{same}}$, the two groups of examples belong to the same class; otherwise they do not.</span></p>

<p>The between-class feature variances $\psi_t$ indicate how discriminative the features are. For example, if $\psi=0$, then it is a completely non-discriminative feature.</p>

<p>Therefore, we can compute the marginal likelihoods for each of those possibilities:</p>
<ul>
  <li>Marginal probability of them being from same class, i.e. $P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n})$
    <blockquote>
      <p>Note that, for the marginal probability that they are from the same class, we will are treating it as a single marginal probability
\(P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n}) =P(\underbrace{\boldsymbol{u}_p^1, \cdots, \boldsymbol{u}_p^m}_{\boldsymbol{u}_p^{1\cdots m}},\underbrace{\boldsymbol{u}_g^1, \cdots, \boldsymbol{u}_g^n}_{\boldsymbol{u}_g^{1\cdots n}})\)</p>
    </blockquote>
  </li>
  <li>Marginal probability of them being from different classes, i.e. $P(\boldsymbol{u}_p^{1\cdots m}) \cdot P(\boldsymbol{u}_g^{1\cdots n})$</li>
</ul>

<p>Based on the post about <a href="https://nicolasshu.com/bayesian_inference_for_gaussian.html">Bayesian inference on Gaussian distributions</a>, we saw that in order to obtain the marginal likelihood on a dataset $\boldsymbol{X}$ is</p>

\[P(\boldsymbol{X}) = P(\boldsymbol{x}^1,\cdots,\boldsymbol{x}^{N}) = \frac{\sigma}{(2\pi \sigma^2)^{N/2} (N\sigma_0^2 + \sigma^2)^{1/2}} \exp \left[ -\frac{\sum_i \boldsymbol{x}^i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{1}{2(N\sigma_0^2 +\sigma^2)} \cdot \left(\frac{\sigma_0^2 N^2 \bar{x}^2}{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x}\mu_0 \right) \right]\]

<p>where $N$ represents the number of examples in $\boldsymbol{X}$,  $\sigma^2$ represents the within-class variance, $\bar{x}$ is the mean of the $\boldsymbol{X}$ over all of the $N$ samples, and the $\boldsymbol{x}^i$ represents the $i^{th}$ example in $\boldsymbol{X}$. Now, rewriting this for our example, let us first write this for a single class $k$, considering it to be univariate (remember, we have decorrelated all of the dimensions/features, thus each feature behaves as if it was its on univariate feature)</p>

\[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) = \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \frac{\mu_{0,k}^2}{2\sigma_{0,k}^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \frac{\sigma_k^2\mu_{0,k}^2}{\sigma_{0,k}^2} + 2n_k\bar{u}_k\mu_{0,k} \right) \right]\]

<p>where, $\sigma_{0,k}^2$  represents the prior variance for the $k^{th}$ class, $\mu_{0,k}$ represents the prior mean for the $k^{th}$ class, and $\sigma_k^2$ represents the within-class variance for the $k^{th}$ class. Do remember, however that we have set the prior means $\mu_{0,k}=0$ for all classes. Therefore</p>

\[\begin{align} 
P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) &amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \frac{\color{violet}{\mu_{0,k}}^2}{2\sigma_{0,k}^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \frac{\sigma_k^2\color{violet}{\mu_{0,k}}^2}{\sigma_{0,k}^2} + 2n_k\bar{u}_k\color{violet}{\mu_{0,k}} \right) \right] \\
&amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \color{violet}{0} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \color{violet}{0} + \color{violet}{0} \right) \right] \\
&amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2}  \right) \right]
\end{align}\]

<p>We have also diagonalized our covariance matrices so that the within-class covariance would be the identity matrix. Therefore $\sigma_k^2 = 1$, yielding</p>

\[\begin{align}
P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) &amp;= \frac{\color{lime}{\sigma_k}}{(2\pi \color{lime}{\sigma^2_k})^{n_k/2} (n_k\sigma_{0,k}^2 + \color{lime}{\sigma_k^2})^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2 \color{lime}{\sigma_k^2}} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 + \color{lime}{\sigma_k^2})} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\color{lime}{\sigma_k^2}}  \right) \right] \\
&amp;= \frac{\color{lime}{1}}{(2\pi  \cdot \color{lime}{1})^{n_k/2} (n_k\sigma_{0,k}^2 + \color{lime}{1})^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2 \cdot \color{lime}{1}} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 + \color{lime}{1})} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\color{lime}{1}}  \right) \right] \\
&amp;= \frac{1}{(2\pi  )^{n_k/2} (n_k\sigma_{0,k}^2 + 1)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2} \right] \exp \left[ \frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{2(n_k \sigma_{0,k}^2 + 1)} \right] \end{align}\]

<p>But, if we are to have multivariate examples, with $\hat{F}$ features, then we rewrite the formulation above to multiply over the feature dimension $f$</p>

\[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) = \prod_{f=1}^{\hat{F}} \frac{1}{(2\pi  )^{n_k/2} (n_k\sigma_{0,k,f}^2 + 1)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i_f}{2} \right] \exp \left[ \frac{\sigma_{0,k,f}^2 n_k^2 \bar{u}_{k,f}^2}{2(n_k \sigma_{0,k}^2 + 1)} \right]\]

<p>By taking the natural logarithm of it, we can more easily deal with infinitesimal probabilities, and it helps us make multiplications into additions</p>

\[\log[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k)] = \sum_{f=1}^{\hat{F}} -\frac{n_k}{2} \log(2\pi) - \frac{1}{2} \log(n_k \sigma^2_{0,k,f} + 1) -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i_f}{2} + \frac{\sigma_{0,k,f}^2 n_k^2 \bar{u}_{k,f}^2}{2(n_k\sigma_{0,k}^2 + 1)}\]

<p>Since $\sigma_0^2$ represents the prior variance, if we are given a dataset $U_{model}$ with size $n=\vert U_{model}\vert$, we can write the probability of all of those samples occurring as</p>

\[\log[P(\boldsymbol{u}^1,\cdots,\boldsymbol{u}^{n})] = \sum_{f=1}^{\hat{F}} \underbrace{-\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n \Sigma^{prior} + I)}_{\log(C)} \underbrace{-\frac{\sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}}{2}}_{E_2} \underbrace{+ \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2(n\Sigma^{prior} + I)}}_{E_1}\]

<!-- By taking the logarithmic, we can can more easily deal with infinitesimal probabilities and helps us make multiplications into additions. Therefore, let us consider

$$\begin{align}
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &= \prod_{t=1}^d \underbrace{\frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} }_{C} \exp \left( \underbrace{- \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})}}_{E_1} \underbrace{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }_{E_2} \right) \\
&= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
\log (P(\boldsymbol{u}^{1\cdots n})) &= \sum_{t=1}^d \color{red}{\log (C)} +\color{cyan}{ \log(e^{E_1})} + \color{magenta}{\log(e^{E_2})} \\
&= \sum_{t=1}^d \color{red}{\log (C)} + \color{cyan}{E_1} + \color{magenta}{E_2} \\
&= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(\psi_t + \frac{1}{n} \right)} \color{cyan}{ - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} } \\
&= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(n\psi_t + 1 \right) + \frac{1}{2} \log(n)} \color{cyan}{ - \frac{n \bar{u}_t^2}{2(n\psi_t + 1)} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }
\end{align}$$

> $$\begin{align}
> P(\boldsymbol{u}^{1\cdots n}) &= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
> \log (C) &= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n\Sigma^{prior} + I) \\
> \log (e^{E_1}) &= E_1 = \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2 (n\Sigma^{prior} + I)} \\
> \log (e^{E_2}) &= E_2 = -\frac{1}{2} \sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}^2 \\
> \log(P(\boldsymbol{u}^{1\cdots n})) &= \sum_{t=1}^d \log(C) + E_1 + E_2 \\ 
> \end{align}$$

Now, note that because the data in $U_{model}$ has been normalized $mean(U_{model}) = \boldsymbol{0}$ -->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">marginal_logprob</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">S_prior</span> <span class="o">=</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span>
    <span class="n">log_C</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">S_prior</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">E1</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">S_prior</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">S_prior</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">E2</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">U</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">logP_t</span> <span class="o">=</span> <span class="n">log_C</span> <span class="o">+</span> <span class="n">E1</span> <span class="o">+</span> <span class="n">E2</span> 
    <span class="n">logP</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">logP_t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logP</span>
</code></pre></div></div>

<p>This way, if we were to receive two sets of data</p>

\[\begin{align}
\boldsymbol{u}_p &amp;\in \mathbb{R}^{m\times \hat{F}} \\
\boldsymbol{u}_g &amp;\in \mathbb{R}^{n\times \hat{F}}
\end{align}\]

<p>We can set a set by concatenating both of them
\(\boldsymbol{u}_{pg} \in \mathbb{R}^{(m+n) \times \hat{F}}\)</p>

<p>And then we can pass them through the computation above</p>

\[\begin{align}
\log[P(\boldsymbol{u}_p^{1\cdots m})] &amp;= \text{log likelihood for probe set} \\ 
\log[P(\boldsymbol{u}_g^{1\cdots n})] &amp;= \text{log likelihood for gallery set} \\ 
\log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] &amp;= \text{log likelihood for combined set}\\ 
\end{align}\]

<p>Finally, instead of the ratio, we compute the log of the ratio</p>

\[\log(R) = \log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] - [\log(P(\boldsymbol{u}_p^{1\cdots m}) + \log(P(\boldsymbol{u}_g^{1\cdots n})]\]

<p>In such case, since we are dealing with log ratios, negative values mean that the model believes the two datapoints are from different categories, where as positive values indicate that the model believes that the two data points are from the same category.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logprob_p</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">u_p</span><span class="p">)</span>
<span class="n">logprob_g</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">u_g</span><span class="p">)</span>
<span class="n">logprob_pg</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">u_p</span><span class="p">,</span><span class="n">u_g</span><span class="p">]))</span>
<span class="n">log_Ratio</span> <span class="o">=</span> <span class="n">logprob_pg</span> <span class="o">-</span> <span class="p">(</span><span class="n">logprob_p</span> <span class="o">+</span> <span class="n">logprob_g</span><span class="p">)</span>

<span class="k">if</span> <span class="n">log_Ratio</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Belong to the same class"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Belong to different classes"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><category term="Machine Learning" /><summary type="html"><![CDATA[I need to note that a lot of this post was inspired by RaviSoji’s PLDA implementation Let us say that we have a training dataset $X \in \mathbb{R}^{N\times F}$ and $y \in \mathbb{R}^N$, where $N$ is the number of examples in the whole dataset, $F$ is the number of features in the dataset, $K$ specifies the number of classes that there are, and $\boldsymbol{m} \in \mathbb{R}^F$ represents the mean vector of the entire dataset $X$. from mnist import MNIST from sklearn.decomposition import PCA import scipy ds = MNIST("/home/data/MNIST") ds.gz = True images, labels = ds.load_training() X = np.array(images)#.reshape(-1,28,28) y = np.array(labels) K = len(np.unique(y)) F = len(X[0]) m = np.mean(X,axis=0) For a classification problem we will create several subspaces: $\mathcal{D}$ represents the space where the data lives in $\mathcal{X}$ represents the subspace found via PCA where the data in $\mathcal{D}$ gets transformed to the subspace $\mathcal{X}$. Here, you may consider $\mathcal{X}$ to be the space of the preprocessed state $\mathcal{U}$ represents the subspace found via PLDA, where the data will be ultimately transformed to, which is described in detail by Ioffe 2006 $\mathcal{U}_{model}$ represents the subspace within $\mathcal{U}$ which contains only the dimensions that are relevant to the problem So, in general, the data will first flow in the following fashion \(\mathcal{D} \leftrightarrow \mathcal{X} \leftrightarrow \mathcal{U} \leftrightarrow \mathcal{U}_{model}\) First, for a classification problem, we perform a PCA to reduce the dimensionality, and get rid of features that may not be very important. In other words, we will bring the data from $\mathcal{D}$ to $\mathcal{X}$. For this one needs to determine the number of components that one would want to take into account for in its subspace, and set that as the matrix rank. This can be predefined or not. If it is not defined, we compute the covariance matrices for each of the classes (i.e. a between-class covariance matrix and a within-class covariance matrix per class $k$). In other words For each class Compute the mean vectors $m_k \in \mathbb{R}^F$ Compute the covariance matrix $\sigma_k \in \mathbb{R}^{F\times F}$ Compute the between-class covariance matrix per class $S_{b,k} \in \mathbb{R}^{F\times F}$ \(S_{b,k} = \frac{n_k}{N}\underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times 1}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{1\times F}}\) Compute the within-class covariance matrix per class $S_{w,k} \in \mathbb{R}^{F\times F}$ \(S_{w,k} = \frac{n_k-1}{N} \odot \sigma_{k}\) Then compute the within-class and between-class covariance matrices, $S_w \in \mathbb{R}^{F\times F}$ and $S_b \in \mathbb{R}^{F\times F}$ respectively. If one was to set \(\boldsymbol{m}_{ks} \in \mathbb{R}^{K\times F}\) as the matrix representing all of the mean vectors, $\boldsymbol{\sigma_{ks} \in \mathbb{R}^{K\times F \times F}}$ the tensor representing all of the class covariances, and $n_{ks}\in \mathbb{R}^K$ a vector representing all of the number of examples in each class, it is possible to vectorize it all as \[\begin{align} S_b &amp;= \underbrace{\frac{n_{ks}}{N} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times K}}}_{\mathbb{R}^{F\times K}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{K\times F}} \\ S_w &amp;= \sum_{k} S_{w,k} \\ &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \odot \underbrace{\frac{n_k-1}{N}}_{\mathbb{R}^K} \\ &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \cdot \underbrace{\frac{n_k-1}{N}\text{[:, None, None]}}_{\mathbb{R}^{K\times 1 \times 1}} \end{align}\] n_principal_components = None # This might overfit if n_principal_components: matrix_rank = n_principal_components else: m_ks, sigma_ks, n_ks = [],[],[] for k in range(K): # Get only the data associated with class k X_k = X[y==k] # Compute the mean, number of samples, and class covariance m_k = np.mean(X_k,axis=0) n_k = len(X_k) sigma_k = np.cov(X_k.T) # Append them all m_ks.append(m_k) n_ks.append(n_k) sigma_ks.append(sigma_k) m_ks = np.array(m_ks) n_ks = np.array(n_ks) sigma_ks = np.array(sigma_ks) assert m_ks.shape == (K,F) assert n_ks.shape == (K,) assert sigma_ks.shape == (K,F,F) S_b = ((m_ks - m).T * n_ks/N) @ (m_ks - m) S_w = np.sum(sigma_ks * ((n_ks-1)/N).reshape(-1,1,1), axis=0) matrix_rank = np.linalg.matrix_rank(S_w) if F != matrix_rank: pca = PCA(n_components = matrix_rank) pca.fit(X) Now, there are going to be several transformations: $\mathcal{D} \rightarrow \mathcal{X}$ Here, there are two case scenarios. If PCA was defined in order to reduce the dimensions, then the data in $\mathcal{D}$ will be transformed via PCA. Otherwise, you can return the data itself $\mathcal{X} \rightarrow \mathcal{D}$ In this case, it is very similar to the converse. If PCA was defined, in order to bring it back to the original data space $\mathcal{D}$, you need to inverse transform the data. Otherwise, just return the data itself def transform_from_D_to_X(x): global pca if pca: return pca.transform(x) else: return x def transform_from_X_to_D(x): global pca if pca: return pca.inverse_transform(x) else: return x So, at this point, we convert the training data from $\mathcal{D}$ to the space $\mathcal{X}$, having the data be represented as $X_{pca}$ X_pca = transform_from_D_to_X(X) print("Shape of X_pca =",X_pca.shape) # Shape of X_pca = (60000, 712) In the PLDA, we use a Gaussian mixture model, where $\boldsymbol{x}$ retpresents a sample in the mixture, and $\mathcal{y}$ represents the center of a mixture component. In general, the class-conditional distributions is represented by \[P(\boldsymbol{x} | \boldsymbol{y}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{y}, \Phi_w)\] where all of the class-conditional distributions share one common covariance [I DON’T KNOW WHY THEY ALL SHARE THE SAME COVARIANCE] If we recall, the LDA formulation is the result if we were to set $\mu_k$ values to be constrainted to be in a lower dimension, and perform the likelihood maximization with respect to $\mu_k$, $\pi_k$, and $\Phi_w$, where the priord of the class variable $\boldsymbol{y}$ is set to put a probability mass on each of the points \[P_{LDA}(\boldsymbol{y}) = \sum_{k=1}^{K} \pi_k \delta(\boldsymbol{y}-\mu_k)\] But that won’t be the case in the PLDA formulation. Instead, PLDA sets it so taht the prior is not to be within a discrete set of values, but instead, sampled from a Gaussian prior. \[P_{PLDA}(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{m}, \Phi_b)\] Note that this normal distribution $P_{PLDA}(y)$ uses the mean of the full dataset. This formulation makes it such that $\Phi_w$ is positive definite, and $\Phi_b$ is positive semi-definite. Theoretically, it is possible to find a transformation $V$ which can simultaneously diagonalize $\Phi_b$ and $\Phi_w$ \[\begin{align} V^T \Phi_b V &amp;= \Psi \\ V^T \Phi_w V &amp;= I \end{align}\] We can define $A = V^{-T} = \text{inv}(V^T)$, resulting in \[\begin{align} \Phi_w &amp;= AA^T \\ \Phi_b &amp;= A\Psi A^T \end{align}\] Thus, the PLDA model is defined as: \[\bbox[teal, 4pt]{\begin{align} \boldsymbol{x} &amp;= \boldsymbol{m} + A \boldsymbol{u} \quad \text{where} \\ &amp; \boldsymbol{u} \sim \mathcal{N}(\cdot | \boldsymbol{v}, I) \\ &amp; \boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi) \end{align}}\] Here, $\boldsymbol{u}$ represents the sample data representation of $\boldsymbol{x}$, but projected in the lated projected space $\mathcal{U}$, and $\boldsymbol{v}$ represents the sample label in the lated projected space. These transformations can be computed via \[\boldsymbol{x} = \boldsymbol{m} + A \boldsymbol{u} \quad \leftrightarrow \quad \boldsymbol{u} = V^T (\boldsymbol{x} - \boldsymbol{m})\] \[\boldsymbol{y} = \boldsymbol{m} + A \boldsymbol{v} \quad \leftrightarrow \quad \boldsymbol{v} = V^T (\boldsymbol{y} - \boldsymbol{m})\] And from this point on, we determine the optimal $\boldsymbol{m}$, $A$, and $\Psi$ are. S. Ioffe: “In the training data, the grouping of examples into clusters is given, and we learn the model parameters by maximizing the likelihood. If, instead, the model parameters are fixed, likelihood maximization with respect to the class assignment labels solves a clustering problem” def transform_from_X_to_U(x): global m, A return (x-m) @ np.linalg.inv(A) def transform_from_U_to_X(x): global m, A return m + (x @ A.T) But, at this point, we don’t know what the parameters $A$ or $\Psi$ are, so we can’t use these functions yet. Learning the Model Parameters ($\boldsymbol{m}, \Psi, A$) The loading matrix $A$ is essentially finding the variances $\Phi_b$ and $\Phi_w$, and all of the parameters can be defined using a maximum likelihood framework. Let us say that $D_k$ represents the dataset which contains only samples from the $k^{th}$ class, and $\boldsymbol{x}_k^i$ represents the $i^{th}$ sample from $D_k$, and it belongs to the $k^{th}$ class. Given $N$ training examples separated into $K$ classes, and assuming that they are all independently drawm from their respective class, the log likelihood is \[l(\boldsymbol{x}^{1 \cdots N}) = \sum_{k=1}^K \ln P(\boldsymbol{x}^i : i \in D_k) = \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k)\] where the joint probability distribution of a set of $n$ patterns (assuming all these $n$ patterns belong to the same class $k$) is: \[\begin{align} P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) &amp;= \int \color{red}{P(\boldsymbol{x}^1_k | \boldsymbol{y})} \cdots \color{cyan}{P(\boldsymbol{x}^n_k | \boldsymbol{y})} \color{magenta}{P(\boldsymbol{y})} d\boldsymbol{y} \\ &amp;= \int \color{red}{\mathcal{N}(\boldsymbol{x}^1_k | \boldsymbol{y}, \Phi_w)} \cdots \color{cyan}{\mathcal{N}(\boldsymbol{x}^n_k | \boldsymbol{y}, \Phi_w)} \color{magenta}{\mathcal{N}(\boldsymbol{y} | 0, \Phi_b)} d\boldsymbol{y} \end{align}\] By computing the integral, we obtain \[\color{red}{MAGIC}\] \[\ln P(\boldsymbol{x}^1_k,\cdots, \boldsymbol{x}^n_k) = C - \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} (\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T\right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} ( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)\right)\right]\] where \(\bar{\boldsymbol{x}}_k = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}^i_k\), and $C$ is a constant that can be ignored At this point, as a “hack”, it sets the number of examples for each class to be $n$. In other words, every class ends up having exactly $n$ examples to learn from. Now, if one were to maximize the equation $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\boldsymbol{m}$, one would obtain $\boldsymbol{m}^* = \frac{1}{N} \sum_i \boldsymbol{x}^i$. If one substitutes it back, one would get \[\begin{align} l(\boldsymbol{x}^{1\cdots N}) &amp;= \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) \\ &amp;= -\sum_{k=1}^K \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} \color{cyan}{(\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T} \right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} \color{red}{( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)} \right) \right] \\ &amp;= \cdots \\ &amp;= \color{red}{\text{MAGIC}} \\ &amp;= \cdots \\ &amp;= - \frac{c}{2} \left[ \ln |\Phi_b + \frac{\Phi_w}{n} | + \text{tr} \left( (\Phi_b + \frac{\Phi_w}{n})^{-1} \color{cyan}{S_b} \right) + (n-1) \ln |\Phi_w | + n \text{tr} (\Phi_w^{-1} \color{red}{S_w}) \right] \end{align}\] Now, we need to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Phi_b$ and $\Phi_w$ subject to $\Phi_w$ being p.d. and $\Phi_b$ being p.s.d. Without these constraints, we would obtain \[\Phi_w = \frac{n}{n-1} S_w \quad \text{and} \quad \Phi_b = S_b - \frac{1}{n-1} S_w\] Therefore, if $S_w$ and $S_b$ are diagonal, then the covariances $\Phi_w$ and $\Phi_b$ will also be diagonal, and the diagonalization property holds as long as the contraints above are satisfied. As we have previously stated, we know that \[\Phi_b = A \Psi A^T\] If you fix $\Psi$ and maximize $l(\boldsymbol{x}^{1\cdots N})$ via unconstrained optimization with respect to $A^{-1}$ will make $A^{-1}S_b A^{-T}$ and $A^{-1}S_w A^{-T}$, making the $A^{-T}$ to be the solution of the generalized eigenvector problem involving $S_b$ and $S_w$, where $S_b \boldsymbol{v} = \lambda S_w \boldsymbol{v}$. Then, the projection of the data to the latent space with the LDA projection. [REVIEW] Then, if you were to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Psi$ subject to $\Psi \geq 0$ and $\text{rank}(\Psi) \leq \hat{F}$, then we’ll get the method to optimize the model [REVIEW] Ioffe 2006: “Our method was derived for the case where each class in the training data is represented by the same number $n$ of examples. This may not be true in practice, in which case, we can resample the data to make the number of examples the same, use EM (as shown in section 5), or use approximations. We took the latter approach, using the closed-form solution in Fig. 2, where $n$ was taken to be the average number of examples per class” Algorithm: PLDA Optimization Input: Training $N$ examples from $K$ classes, with $n = N/K$ per class Output: Parameters $\boldsymbol{m}, A, \Psi$, maximizing the likelihood of the PLDA model Compute the covariance matrices $S_b$, and $S_w$ Compute the transformation matrix $W$ such that $S_b \boldsymbol{w} = \lambda S_w \boldsymbol{w}$ (i.e. $eig(S_w^{-1}S_b)$) Compute the covariance matrices in the latent space $\Lambda_b = W^T S_b W$ $\Lambda_w = W^T S_w W$ Determine the following parameters $\boldsymbol{m} = \frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$ $A = W^{-T} \left( \frac{n}{n-1} \Lambda_w \right)^{1/2}$ $\Psi = \max \left( 0, \frac{n-1}{n} (\Lambda_b / \Lambda_w) - \frac{1}{n} \right)$ Reduce the dimensionality to $\hat{F}$ by keeping the largest elements of $\Psi$, while setting the rest to zero. In the latent space $\boldsymbol{u} = A^{-1}(\boldsymbol{x}-\boldsymbol{m})$, only the features for non-zero entries are needed for recognition Note: Scipy’s eigh(A,B) function in linalg solves the generalized eigenvalue problem for a complex Hermitian or a real symmetric matrix, so that $A\boldsymbol{v} = \lambda B \boldsymbol{v}$. Otherwise, if B is omitted, then it is assumed that $B=I$ Also note that scipy.linalg.eigh != np.linalg.eigh def compute_Sb_Sw(X,y): m = np.mean(X,axis=0) _,F = X.shape m_ks, sigma_ks, n_ks = [],[],[] for k in range(K): # Get only the data associated with class k X_k = X[y==k] # Compute the mean, number of samples, and class covariance m_k = np.mean(X_k,axis=0) n_k = len(X_k) sigma_k = np.cov(X_k.T) # Append them all m_ks.append(m_k) n_ks.append(n_k) sigma_ks.append(sigma_k) m_ks = np.array(m_ks) n_ks = np.array(n_ks) sigma_ks = np.array(sigma_ks) assert m_ks.shape == (K,F) assert n_ks.shape == (K,) assert sigma_ks.shape == (K,F,F) S_b = ((m_ks - m).T * n_ks/N) @ (m_ks - m) S_w = np.sum(sigma_ks * ((n_ks-1)/N).reshape(-1,1,1), axis=0) return S_b, S_w assert X.shape[0] == y.shape[0] m = X_pca.mean(axis=0) n = N/K S_b, S_w = compute_Sb_Sw(X_pca,y) # Compute W eigvals, eigvecs = scipy.linalg.eigh(S_b, S_w) W = eigvecs # Compute Lambdas Lambda_b = W.T @ S_b @ W Lambda_w = W.T @ S_w @ W # Compute A A = np.linalg.inv(W.T) * (n / (n-1) * np.diag(Lambda_w))**0.5 print(A.shape) # Compute Psi diag_Lambda_w = Lambda_w.diagonal() diag_Lambda_b = Lambda_b.diagonal() Psi = (n - 1)*(diag_Lambda_b/diag_Lambda_w) - (1/n) Psi[ Psi &lt;= 0 ] = 0 Psi = np.diag(Psi) From this point you can transform the data that is in the PCA subspace $\mathcal{X}$ to the $\mathcal{U}$ subspace, having the data be represented as $U$ u = transform_from_X_to_U(X_pca) Now, again, not every dimension will be relevant in the $\mathcal{U}$ subspace, and that is why we reduce the $\mathcal{U}$ to \(\mathcal{U}_{model}\) , which only contains the relevant dimensions of $\mathcal{U}$. Therefore, in order to go back and forth in between the two subspaces, simply drop them the irrelevant dimensions or add the relevant dimensions to a zero matrix. This new data will be represented as $U_{model}$ def transform_from_U_to_Umodel(x,dims): u_model = u[...,dims] return u_model def transform_from_Umodel_to_U(x,dims,u_dim): shape = (*x.shape[:-1], u_dim) u = np.zeros(shape) u[..., dims] = x return u # Compute the relevant dimensions of Psi relevant_dims = np.squeeze(np.argwhere(Psi.diagonal() != 0))[0] if relevant_dims.ndim == 0: relevant_dims = relevant_dims.reshape(1,) U_model = transform_from_U_to_Umodel(X_pca,relevant_dims) Great! Now we we have the data in the $\mathcal{U}_{model}$ space. Now, all that there is left is to understand how to perform inference, and in order to do that, one needs to find the prior parameters for $\boldsymbol{v}$, the posterior parameters, and the posterior predictive parameters. Let’s discuss now how to find the probability parameters. Inference on the Latent Space First, if you need some review on Bayesian inference for Gaussian distributions, you may check the other post to understand priors, posteriors, posterior predictives, and marginal probability. For this problem, note that the different dimensions have been decorrelated (i.e. the covariances have been decorrelated), thus the different dimensions could be treated as univariate problems. We need to determing the prior parameters of $\boldsymbol{v}$, which leads to the probability distribution $P(\boldsymbol{v})$, the posterior parameters, which leads to \(P(\boldsymbol{v} \vert \boldsymbol{u})\) , and the posterior predictive parameters, for \(P(\boldsymbol{u}^p \vert \boldsymbol{u}^g_{1\cdots n})\) Prior Distribution The easiest to determine right off the bat are the prior parameters. For the prior parameters, as one may recall in the model formulation \[\boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)\] which, in turn, are simple to compute. Here we’ll call $\mu_{prior}$ the prior mean, and $\Sigma_{prior}$ the prior covariance, making \[\bbox[teal, 4pt]{\begin{align} \mu^{prior} &amp;= \boldsymbol{0} \\ \Sigma^{prior} &amp;= \Psi_{\forall d \in D} \end{align}}\] where $D$ represents all of the relevant dimensions, which are all that the variances are not zero. Then from this point, we’ll use the notation setting $\hat{\Psi} = \Psi_{\forall d \in D}$ prior_params = { "mean": np.zeros(relevant_dims), "cov": np.diag(Psi)[relevant_dims] } Now, the more involved ones are the posteriors parameters. One advantage that PLDA has is that it allows one to make inferences about classes not present during training. Let us consider the following case of classification first. We are given a set of data to learn from, which Ioffe refers to as a “gallery”. This set ${ \boldsymbol{x}^1, \cdots,\boldsymbol{x}^k, \cdots, \boldsymbol{x}^K }$ contains $K$ examples, with one example from each of the $K$ classes. We are also given a probe example $\boldsymbol{x}^p$, and assume that it belongs to one of the $K$ classes. If we are to determine to which class it belongs, maximizing the likelihood will do the job. This can be more easily accomplised in the lated space by performing the trnasformation $\boldsymbol{u} = A^{-1}(\boldsymbol{x} - \boldsymbol{m})$, since it will decorrelate the data. For this example, $\boldsymbol{x}^p$ will be transformed to $\boldsymbol{u}^p$. Next we will see how to compute the posterior distribution and the posterior predictive distribution. We will, however look at two different cases. We will first look in the case where we have a single training example per class (in deep learning, known as $k$-way-one-shot learning), and then we’ll look at the case where we see multiple examples per class (also known as $k$-way-few-shot learning). Case: Single Training Example per Class Posterior Distribution Let us consider an example $\boldsymbol{u}^g$ from the training set (i.e. gallery), where, again, it belongs to some class between $1 \cdots K$ The probability that the probe $\boldsymbol{u}^p$ belongs to the same class as $\boldsymbol{u}^g$ is defined by the probability $P(\boldsymbol{u}^p | \boldsymbol{u}^g)$. So, the posterior probability will provide a way to perform inference on the class variable $\boldsymbol{v}$ (i.e. the transformed version of $\boldsymbol{y}$). As we may remember, the parameters for a posterior Gaussian are \[\begin{align} \mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Now, since we only have a single sample $\bar{x} = \boldsymbol{u}$. Additionally, we know that the mean for the prior $\mu^{prior}$ is all zeros (since the data has been centralized), then \[\begin{align} \mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Additionally, since we would be looking at a single class at this point and the covariances have been diagonalized, the within-class covariance is an identity matrix, making the $(\sigma^{prior})^2 = 1$. Therefore the parameters turn into \[\begin{align} \mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;= \frac{1(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \end{align}\] And, once again, since we are dealing with a single sample, $N=1$ \[\begin{align} \mu^{post} &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \end{align}\] The posterior for a single training example can then be defined as \[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}) = \mathcal{N}\left(\boldsymbol{v} \bigg| \frac{\hat{\Psi}}{\hat{\Psi} + I}\boldsymbol{u}, \frac{\hat{\Psi}}{\hat{\Psi} + I}\right)}\] Now, if we see how this flows, the class variable $\boldsymbol{v}$ will be used to determine $\boldsymbol{u}$ examples, which are then used to determine the data $\boldsymbol{x}$. Posterior Predictive Distribution from graphviz import Digraph dot = Digraph() dot.node('v',"v") dot.node('1',"u1") dot.node('2',"u2") dot.node("a","x1") dot.node("b","x2") dot.edges(['v1',"v2","1a","2b"]) dot From probabilistic graphical models, if we observe $\boldsymbol{v}$ (i.e. $\boldsymbol{v}$ is given), then $\boldsymbol{u}^p$ and $\boldsymbol{u}^g$ are conditionally independent. We also know that the posterior predictive probability is \[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu^{post}, (\sigma^{post})^2 + \sigma^2)\] And since the within class variance has been diagonalized to an identity matrix ($\sigma^2 = 1$), we’ll obtain \[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}^g) = \mathcal{N}\left(\boldsymbol{u}^p \bigg| \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \boldsymbol{u}^g, I + \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \right)}\] In order to classify $\boldsymbol{u}^p$, then we compute $P(\boldsymbol{u}^p \vert \boldsymbol{u}^g) \forall g \in {1,\cdots,M }$, and pick the maximum. Ioffe 2006: “With PLDA, we were able to combine the knowledge about the general structure of the data, obtained during training, and the examples of new classes, yielding a principled way to perform classification” Now that we’ve seen the single example case, let’s expand this to the multiple example case. Case: Multiple Training Examples Per Class Posterior Distribution We can improve the recognition performance by using more examples. Let us say that we have $n_k$ examples from class $k$, making \[n_k = |U_{model,k}|\] These examples are all independent examples $\boldsymbol{u}_{1\cdots n}^g$. Just as before, we know that, here, we are looking at a single class $k$. We have previously diagonalized all of the covariance matrices, making all of the dimensions (i.e. features) decorrelated, thus they can be worked on individually as if they were univariate features. This also means that the within-class covariance is an identity matrix, making $\sigma^2 = 1$. As we may remember, the parameters for a posterior Gaussian are \[\begin{align} \mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Again, since our model estimates that the prior mean $\mu^{prior}$ is all zeros, then \[\begin{align} \mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Since we’re looking at the class $k$, then $N=n_k$ and $\bar{x} = \bar{\boldsymbol{u}}_k$. Thus the posterior for multiple samples is \[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}_k^{1\cdots n_k} ) = \mathcal{N}\left(\boldsymbol{v} \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi}+I}\bar{\boldsymbol{u}}_k, \dfrac{\hat{\Psi}}{n_k \hat{\Psi}+I} \right) }\] Therefore, in order to compute the posterior parameters, where $\mu_k^{post}$ and $\Sigma_k^{post}$ are the mean posterior and the covariance posterior for each class $k$, we have \[\bbox[teal, 4pt]{\begin{align} \Sigma^{post}_k &amp;= \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = \Sigma^{prior} \odot \frac{1}{1 + n_k \cdot \Sigma^{prior}} \\ \mu_k^{post} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \underbrace{\left( \sum_{\boldsymbol{u} \in U_{model,k}} \boldsymbol{u} \right)}_{n_k \bar{\boldsymbol{u}}_k} \cdot \Sigma^{post}_k \end{align}}\] where we can recall that $\Sigma^{prior} = \hat{\Psi}$, and $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$ posterior_params = {} for k in np.unique(y): u_model_k = u_model[y==k] n_k = len(u_model_k) cov = prior_params["cov"] / (1 + n_k * prior_params["cov"]) mean = np.sum(u_model_k, axis=0) * cov posterior_params[k] = {"mean": mean, "cov":cov} Posterior Predictive Distribution As per page 535 by Ioffe 2006, if one sets multiple examples of a class to a single model, assuming we have $n_k$ independent examples \(\{ \boldsymbol{u}_k^i \}_{i=1}^{n_k}\), then the probability of obtaining a sample $\boldsymbol{u}^p$, given the set above, can be obtained from the posterior predictive \[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)\] Once again, since the within-class covariance has been diagonalized to an identity matrix, then $\sigma^2 = 1$ \[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}_k^{1\cdots n_k}) = P(\boldsymbol{u}^p | \boldsymbol{u}_k^1, \cdots, \boldsymbol{u}_k^{n_k}) = \mathcal{N} \left( \boldsymbol{u}^p \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k, I + \dfrac{\hat{\Psi}}{n_k \hat{\Psi} + I} \right) }\] where $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$ This means that, you may compute the predictive parameters as copying the posterior parameters ($\mu_k^{postpred}$, $\Sigma_k^{postpred}$), and adding an identity matrix to the covariance \[\bbox[teal, 4pt]{\begin{align} \mu_k^{postpred} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \mu_k^{post} \\ \Sigma_k^{postpred} &amp;= I + \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = I + \Sigma_k^{post} \end{align}}\] post_pred_params = posterior_params.copy() for k,params in post_pred_params.items(): params["cov"] += 1 Summary and Inference Great! This is all that we need to precompute before performing any inference! At this point, you have determined all of the parameters which describe you data. As a review, in order to prepare this algorithm, we have received a training dataset $X$ and its corresponding labels $y$. Then, we have decided on a certain number of components to be used for a PCA. This PCA will bring the data from the space $\mathcal{D}$ to the space $\mathcal{X}$, obtaining the data $X_{pca}$. Then we found the parameters $\boldsymbol{m}$, $A$, and $\hat{\Psi}$, which optimize the PLDA formulation. These would then allow us to bring the data from the $\mathcal{X}$ subspace to the $\mathcal{U}$ latent space, obtaining the data $U$. Then, we reduced the dimensions of $U$ by discarding any dimensions which had a zero variance in $\hat{\Psi}$, yielding the data $U_{model}$. Finally, using these same parameters, we obtain the prior parameters ($\mu^{prior}$ and $\Sigma^{prior}$), the posterior parameters ($\mu^{post}_k$ and $\Sigma^{post}_k$), and the posterior predictive parameters ($\mu^{postpred}_k$ and $\Sigma^{postpred}_k$) Now, we’re ready to do the inference on the latent space We have previously established that we have the classes (i.e. categories) $1,\cdots, K$. What we will do is to iterate through each of the possible classes, and compute its posterior probabilities by using the parameters computed for the posterior predictive probabilities and creating a Gaussian distribution. Finally, we can obtain the probability on that Gaussian at each of those locations. We can also use the log probability for each of the samples on each of the classes \[\bbox[teal, 4pt]{\begin{align} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) &amp;= \mathcal{N}(\boldsymbol{u}_{model} | \mu_k^{postpred}, \Sigma_k^{postpred}) \\ \boldsymbol{y}^* &amp;= \arg\max_{k} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) \end{align}}\] from scipy.stats import multivariate_normal as gaussian log_prob_post = [] for k, param in post_pred_params.items(): mean,cov = param["mean"], param["cov"] log_probs_k = gaussian(mean,np.diag(cov)).logpdf(U_model) log_prob_post.append(log_probs_k) log_prob_post = np.array(log_prob_post).T Here, you may choose to normalize the probabilities normalize = False if normalize: logsumexp = np.log(np.sum(np.exp(log_prob_post),axis=-1)) log_probs = log_prob_post - logsumexp[..., None] else: log_probs = log_prob_post categories = np.array([k for k in post_pred_params.keys()]) predictions = categories[np.argmax(log_probs,axis=-1)] N = len(X) shape = [3,2] inds = np.random.choice(N,np.prod(shape)) plt.figure(dpi=200) fig, ax = plt.subplots(*shape) count = 0 for i in np.arange(shape[0]): for j in np.arange(shape[1]): ax[i,j].imshow(X[count].reshape(28,28)) title = "True: {} | Pred: {}".format(y[count],predictions[count]) ax[i,j].set_title(title) count+= 1 If you wanted to extract the LDA features, you could simply use the transformation functions to convert the data from some space to another space. So this is the case when we have a classification problem where the probes are assumed to belong to one of the trained classes. Now, let us look at the case where the probes belong to classes not yet seen. Hypothesis Testing Here, we try to determine whether two samples belong to the same class or not. For that, we can compute the following likelihoods \[\begin{align} P(\boldsymbol{u}^p)P(\boldsymbol{u}^g) &amp;= \text{likelihood of examples belonging to different classes} \\ P(\boldsymbol{u}^p, \boldsymbol{u}^g) &amp;= \int P(\boldsymbol{u}^p | \boldsymbol{v}) P(\boldsymbol{u}^g|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ &amp;= \text{likelihood of examples belonging to the same class} \end{align}\] As a generalized formulation where there are multiple examples, the likelihood ratio is \[\begin{align} R(\{\boldsymbol{u}^{1\cdots m}_p\},\{\boldsymbol{u}^{1\cdots n}_g\}) &amp;= \frac{\text{likelihood(same)}}{\text{likelihood(diff)}} = \frac{P(\boldsymbol{u}^{1\cdots m}_p,\boldsymbol{u}^{1\cdots n}_g)}{P(\boldsymbol{u}^{1\cdots m}_p)P(\boldsymbol{u}^{1\cdots n}_g)} \\ P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;= \int P(\boldsymbol{u}^1 | \boldsymbol{v}) \cdots P(\boldsymbol{u}^n|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ &amp;= \prod_{t=1}^d \frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} \exp \left( - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} - \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} \right) \end{align}\] where \[\bar{u}_t = \frac{1}{n}\sum_{i=1}^n u_t^i\] For the priors $\pi_{\text{same}}$ and $\pi_{\text{diff}}$, the probability that all of the examples are in the same class is \[\left(1 + \dfrac{\pi_{\text{diff}} / \pi_{\text{same}} }{R} \right)^{-1} = \dfrac{R}{R+\pi_{\text{diff}} / \pi_{\text{same}}}\] If $R &gt; \pi_{\text{diff}} / \pi_{\text{same}}$, the two groups of examples belong to the same class; otherwise they do not. The between-class feature variances $\psi_t$ indicate how discriminative the features are. For example, if $\psi=0$, then it is a completely non-discriminative feature. Therefore, we can compute the marginal likelihoods for each of those possibilities: Marginal probability of them being from same class, i.e. $P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n})$ Note that, for the marginal probability that they are from the same class, we will are treating it as a single marginal probability \(P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n}) =P(\underbrace{\boldsymbol{u}_p^1, \cdots, \boldsymbol{u}_p^m}_{\boldsymbol{u}_p^{1\cdots m}},\underbrace{\boldsymbol{u}_g^1, \cdots, \boldsymbol{u}_g^n}_{\boldsymbol{u}_g^{1\cdots n}})\) Marginal probability of them being from different classes, i.e. $P(\boldsymbol{u}_p^{1\cdots m}) \cdot P(\boldsymbol{u}_g^{1\cdots n})$ Based on the post about Bayesian inference on Gaussian distributions, we saw that in order to obtain the marginal likelihood on a dataset $\boldsymbol{X}$ is \[P(\boldsymbol{X}) = P(\boldsymbol{x}^1,\cdots,\boldsymbol{x}^{N}) = \frac{\sigma}{(2\pi \sigma^2)^{N/2} (N\sigma_0^2 + \sigma^2)^{1/2}} \exp \left[ -\frac{\sum_i \boldsymbol{x}^i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{1}{2(N\sigma_0^2 +\sigma^2)} \cdot \left(\frac{\sigma_0^2 N^2 \bar{x}^2}{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x}\mu_0 \right) \right]\] where $N$ represents the number of examples in $\boldsymbol{X}$, $\sigma^2$ represents the within-class variance, $\bar{x}$ is the mean of the $\boldsymbol{X}$ over all of the $N$ samples, and the $\boldsymbol{x}^i$ represents the $i^{th}$ example in $\boldsymbol{X}$. Now, rewriting this for our example, let us first write this for a single class $k$, considering it to be univariate (remember, we have decorrelated all of the dimensions/features, thus each feature behaves as if it was its on univariate feature) \[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) = \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \frac{\mu_{0,k}^2}{2\sigma_{0,k}^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \frac{\sigma_k^2\mu_{0,k}^2}{\sigma_{0,k}^2} + 2n_k\bar{u}_k\mu_{0,k} \right) \right]\] where, $\sigma_{0,k}^2$ represents the prior variance for the $k^{th}$ class, $\mu_{0,k}$ represents the prior mean for the $k^{th}$ class, and $\sigma_k^2$ represents the within-class variance for the $k^{th}$ class. Do remember, however that we have set the prior means $\mu_{0,k}=0$ for all classes. Therefore \[\begin{align} P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) &amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \frac{\color{violet}{\mu_{0,k}}^2}{2\sigma_{0,k}^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \frac{\sigma_k^2\color{violet}{\mu_{0,k}}^2}{\sigma_{0,k}^2} + 2n_k\bar{u}_k\color{violet}{\mu_{0,k}} \right) \right] \\ &amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} - \color{violet}{0} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} + \color{violet}{0} + \color{violet}{0} \right) \right] \\ &amp;= \frac{\sigma_k}{(2\pi \sigma^2_k)^{n_k/2} (n_k\sigma_{0,k}^2 + \sigma_k^2)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2\sigma_k^2} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 +\sigma_k^2)} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\sigma_k^2} \right) \right] \end{align}\] We have also diagonalized our covariance matrices so that the within-class covariance would be the identity matrix. Therefore $\sigma_k^2 = 1$, yielding \[\begin{align} P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) &amp;= \frac{\color{lime}{\sigma_k}}{(2\pi \color{lime}{\sigma^2_k})^{n_k/2} (n_k\sigma_{0,k}^2 + \color{lime}{\sigma_k^2})^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2 \color{lime}{\sigma_k^2}} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 + \color{lime}{\sigma_k^2})} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\color{lime}{\sigma_k^2}} \right) \right] \\ &amp;= \frac{\color{lime}{1}}{(2\pi \cdot \color{lime}{1})^{n_k/2} (n_k\sigma_{0,k}^2 + \color{lime}{1})^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2 \cdot \color{lime}{1}} \right] \exp \left[ \frac{1}{2(n_k\sigma_{0,k}^2 + \color{lime}{1})} \cdot \left(\frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{\color{lime}{1}} \right) \right] \\ &amp;= \frac{1}{(2\pi )^{n_k/2} (n_k\sigma_{0,k}^2 + 1)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i}{2} \right] \exp \left[ \frac{\sigma_{0,k}^2 n_k^2 \bar{u}_k^2}{2(n_k \sigma_{0,k}^2 + 1)} \right] \end{align}\] But, if we are to have multivariate examples, with $\hat{F}$ features, then we rewrite the formulation above to multiply over the feature dimension $f$ \[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k) = \prod_{f=1}^{\hat{F}} \frac{1}{(2\pi )^{n_k/2} (n_k\sigma_{0,k,f}^2 + 1)^{1/2}} \exp \left[ -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i_f}{2} \right] \exp \left[ \frac{\sigma_{0,k,f}^2 n_k^2 \bar{u}_{k,f}^2}{2(n_k \sigma_{0,k}^2 + 1)} \right]\] By taking the natural logarithm of it, we can more easily deal with infinitesimal probabilities, and it helps us make multiplications into additions \[\log[P(\boldsymbol{u}^1_k,\cdots,\boldsymbol{u}^{n_k}_k)] = \sum_{f=1}^{\hat{F}} -\frac{n_k}{2} \log(2\pi) - \frac{1}{2} \log(n_k \sigma^2_{0,k,f} + 1) -\frac{\sum_{i=1}^{n_k} \boldsymbol{u}^i_f}{2} + \frac{\sigma_{0,k,f}^2 n_k^2 \bar{u}_{k,f}^2}{2(n_k\sigma_{0,k}^2 + 1)}\] Since $\sigma_0^2$ represents the prior variance, if we are given a dataset $U_{model}$ with size $n=\vert U_{model}\vert$, we can write the probability of all of those samples occurring as \[\log[P(\boldsymbol{u}^1,\cdots,\boldsymbol{u}^{n})] = \sum_{f=1}^{\hat{F}} \underbrace{-\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n \Sigma^{prior} + I)}_{\log(C)} \underbrace{-\frac{\sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}}{2}}_{E_2} \underbrace{+ \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2(n\Sigma^{prior} + I)}}_{E_1}\] def marginal_logprob(U): n = U.shape[0] S_prior = prior_params["cov"] log_C = -0.5 * np.log(2*np.pi) - 0.5 * np.log(n*S_prior + 1) E1 = 0.5*(n**2 * S_prior * np.mean(U,axis=0)**2)/(n*S_prior + 1) E2 = - 0.5 * np.sum(U**2, axis=0) logP_t = log_C + E1 + E2 logP = np.sum(logP_t, axis=-1) return logP This way, if we were to receive two sets of data \[\begin{align} \boldsymbol{u}_p &amp;\in \mathbb{R}^{m\times \hat{F}} \\ \boldsymbol{u}_g &amp;\in \mathbb{R}^{n\times \hat{F}} \end{align}\] We can set a set by concatenating both of them \(\boldsymbol{u}_{pg} \in \mathbb{R}^{(m+n) \times \hat{F}}\) And then we can pass them through the computation above \[\begin{align} \log[P(\boldsymbol{u}_p^{1\cdots m})] &amp;= \text{log likelihood for probe set} \\ \log[P(\boldsymbol{u}_g^{1\cdots n})] &amp;= \text{log likelihood for gallery set} \\ \log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] &amp;= \text{log likelihood for combined set}\\ \end{align}\] Finally, instead of the ratio, we compute the log of the ratio \[\log(R) = \log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] - [\log(P(\boldsymbol{u}_p^{1\cdots m}) + \log(P(\boldsymbol{u}_g^{1\cdots n})]\] In such case, since we are dealing with log ratios, negative values mean that the model believes the two datapoints are from different categories, where as positive values indicate that the model believes that the two data points are from the same category. logprob_p = marginal_logprob(u_p) logprob_g = marginal_logprob(u_g) logprob_pg = marginal_logprob(np.concatenate([u_p,u_g])) log_Ratio = logprob_pg - (logprob_p + logprob_g) if log_Ratio &gt; 0: print("Belong to the same class") else: print("Belong to different classes")]]></summary></entry><entry><title type="html">Linear Discriminant Analysis</title><link href="http://localhost:4000/lda_python.html" rel="alternate" type="text/html" title="Linear Discriminant Analysis" /><published>2021-01-28T00:00:00-05:00</published><updated>2021-01-28T00:00:00-05:00</updated><id>http://localhost:4000/lda</id><content type="html" xml:base="http://localhost:4000/lda_python.html"><![CDATA[<p>This post was highly inspired by <a href="https://sebastianraschka.com/">Sebastian Raschka</a>’s <a href="https://sebastianraschka.com/Articles/2014_python_lda.html">post</a></p>

<p>The goal of LDA is to find some linear transformation $\mathbf{x} \rightarrow \mathbf{x}^T W$ that maximizes the between-class covariance with respect to the within-class covariance.</p>

\[\uparrow \frac{S_b}{S_w}\]

<p>Here, we can do LDA in a few steps</p>
<ol>
  <li>Compute the mean vectors of the features for each of the different classes</li>
  <li>Compute the covariance matrices (between-class and within-class covariance matrices)</li>
  <li>Compute the eigenvectors and eigenvalues for the covariance matrices</li>
  <li>Sort the eigenvectors by the eigenvalues and choose the top $k$ eigenvectors to obtain the transformation matrix $W$</li>
  <li>Use the transformation matrix $W$ to transform the data</li>
</ol>

<p>So, let us say that we have a dataset $X$, such that there are $N$ examples, and $F$ features
\(X \in \mathbb{R}^{N \times F}\)</p>

<p>And we have its labels $y \in \mathbb{R}^N$</p>

<p>Let us say that there are $C$ total classes, and $N_c$ represents the number of examples in the class $c$, where $c \in [1, C]$, and the dataset $\mathcal{D}_c$ represents all of the examples in the class $c$.</p>

<h2 id="import-libraries">Import Libraries</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
</code></pre></div></div>

<h2 id="setup-dataset">Setup Dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">parsers</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="n">filepath_or_buffer</span><span class="o">=</span><span class="s">'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'</span><span class="p">,</span>
    <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">sep</span><span class="o">=</span><span class="s">','</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">label_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s">"Setosa"</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s">"Versicolor"</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s">"Virginica"</span>
<span class="p">}</span>
<span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'sepal_length'</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span><span class="s">'sepal_width'</span><span class="p">,</span>
                <span class="mi">2</span><span class="p">:</span><span class="s">'petal_length'</span><span class="p">,</span>
                <span class="mi">3</span><span class="p">:</span><span class="s">'petal_width'</span><span class="p">}</span>
<span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">.</span><span class="n">values</span><span class="p">())</span><span class="o">+</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span>
<span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s">"all"</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> 

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]].</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"label"</span><span class="p">].</span><span class="n">values</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">N</span><span class="p">,</span><span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<h2 id="step-1-compute-the-mean-vectors">Step 1: Compute the mean vectors</h2>

<p>Here, we can compute the mean vectors $\mathbf{m}_c$, where $c \in [1,C]$, with</p>

\[\mathbf{m}_c = \frac{1}{|\mathcal{D}_c|} \sum_{\mathbf{x} \in \mathcal{D}_c} \mathbf{x}\]

<p>Note that here, $\mathbf{x} \in \mathbb{R}^F$, as a column vector. Also note that $\mathbf{m}$ represents the overall mean of the whole dataset</p>

\[\mathbf{m} = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \mathbf{x}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_vectors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="n">mu_c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">label</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mean_vectors</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_c</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="step-2-compute-the-covariance-matrices">Step 2: Compute the Covariance Matrices</h2>
<h3 id="class-covariance">Class Covariance</h3>
<p>To compute a class covariance $S_c$, we can have 
\(S_c = \sum_{\mathbf{x} \in \mathcal{D}_c} (\mathbf{x}-\mathbf{m}_c)(\mathbf{x}-\mathbf{m}_c)^T\)</p>

<h3 id="within-class-covariance">Within-Class Covariance</h3>
<p>To compute the within-class covariance, 
\(S_w = \sum_{c=1}^C S_c \in \mathbb{R}^{F \times F}\)</p>

<h3 id="between-class-covariance">Between-Class Covariance</h3>
<p>The between-class covariance can be computed as
\(S_b = \sum_{c=1}^C |\mathcal{D}_c| (\mathbf{m}_c - \mathbf{m}) (\mathbf{m}_c - \mathbf{m})^T \in \mathbb{R}^{F\times F}\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">class_covariance</span><span class="p">(</span><span class="n">_class</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">mean_vectors</span>
    <span class="n">S_c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">))</span>
    <span class="n">m_c</span> <span class="o">=</span> <span class="n">mean_vectors</span><span class="p">[</span><span class="n">_class</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">_class</span><span class="p">]:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">S_c</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">m_c</span><span class="p">).</span><span class="n">dot</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">m_c</span><span class="p">).</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">S_c</span>

<span class="c1"># Within Class Covariance
</span><span class="n">S_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">))</span>
<span class="k">for</span> <span class="n">class_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="n">S_w</span> <span class="o">+=</span> <span class="n">class_covariance</span><span class="p">(</span><span class="n">class_num</span><span class="p">)</span>

<span class="c1"># Between Class Covariance
</span><span class="n">S_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">))</span>
<span class="k">for</span> <span class="n">class_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="n">N_c</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">class_num</span><span class="p">].</span><span class="n">shape</span>
    <span class="n">m_c</span> <span class="o">=</span> <span class="n">mean_vectors</span><span class="p">[</span><span class="n">class_num</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">S_b</span> <span class="o">+=</span> <span class="n">N_c</span> <span class="o">*</span> <span class="p">(</span><span class="n">m_c</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">dot</span><span class="p">((</span><span class="n">m_c</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Within-class Covariance"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">S_w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Between-class Covariance"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">S_b</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Within-class Covariance
[[38.9562 13.683  24.614   5.6556]
 [13.683  17.035   8.12    4.9132]
 [24.614   8.12   27.22    6.2536]
 [ 5.6556  4.9132  6.2536  6.1756]]
Between-class Covariance
[[ 63.21213333 -19.534      165.16466667  71.36306667]
 [-19.534       10.9776     -56.0552     -22.4924    ]
 [165.16466667 -56.0552     436.64373333 186.90813333]
 [ 71.36306667 -22.4924     186.90813333  80.60413333]]

</code></pre></div></div>

<h2 id="step-3-solve-the-generalized-eigenvalue-problem">Step 3: Solve the Generalized Eigenvalue Problem</h2>
<p>So, here, we understand that the <code class="language-plaintext highlighter-rouge">eig(A)</code> function solves for the eigenvalues and eigenvectors of $A$, where $A\mathbf{v} = \lambda \mathbf{v}$. Now, the linear constraints are such that</p>

\[S_b \mathbf{v} = \lambda S_w \mathbf{v}\]

\[\underbrace{S_w^{-1}S_b}_{A} \mathbf{v} = \lambda \mathbf{v}\]

<p>Therefore, we can compute the eigv*’s for it as</p>

\[eig(\underbrace{S_w^{-1}S_b}_{\mathbb{R}^{F\times F}})\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S_w</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">S_b</span><span class="p">)</span>
<span class="n">eigval</span><span class="p">,</span> <span class="n">eigvec</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show the Eigenvalue Solutions
</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigval</span><span class="p">)):</span>
    <span class="n">evec</span> <span class="o">=</span> <span class="n">eigvec</span><span class="p">[:,</span><span class="n">k</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Eigval: "</span><span class="p">,</span><span class="n">eigval</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Eigvec: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">evec</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="n">assert_array_almost_equal</span><span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S_w</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">S_b</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">evec</span><span class="p">),</span>
        <span class="n">eigval</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">evec</span><span class="p">,</span>
        <span class="n">decimal</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"All eigvecs and eigvals match."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Eigval:  32.27195779972981
Eigvec: 
 [[ 0.20490976]
 [ 0.38714331]
 [-0.54648218]
 [-0.71378517]]

Eigval:  0.27756686384004264
Eigvec: 
 [[-0.00898234]
 [-0.58899857]
 [ 0.25428655]
 [-0.76703217]]

Eigval:  -4.1311796919088535e-15
Eigvec: 
 [[-0.83786868]
 [ 0.16963186]
 [ 0.12293803]
 [ 0.50407077]]

Eigval:  1.1953730364935478e-14
Eigvec: 
 [[ 0.20003692]
 [-0.39490682]
 [-0.45668159]
 [ 0.77167076]]

All eigvecs and eigvals match.
</code></pre></div></div>

<h2 id="step-4-select-the-linear-discriminants-for-the-new-latent-subspace">Step 4: Select the linear discriminants for the new latent subspace</h2>
<p>Here, all you have to do is to find the top $k$ eigenvalues and their respective eigenvectors. And then set the transformation. So, if $\mathbf{w}_F$ is the eigenvector associated with the largest eigenvalue $\lambda_F$, and $\mathbf{w}_1$ is the eigenvector associated with the smallest eigenvalue $\lambda_1$ of $S_w^{-1}S_b$, and</p>

\[\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_F\]

<p>Then the transformation matrix $W$ is</p>

\[W = \begin{bmatrix}
| &amp; | &amp; &amp; | \\ 
\mathbf{w}_F &amp; \mathbf{w}_{F-1} &amp; \cdots &amp; \mathbf{w}_{F-k} \\
| &amp; | &amp; &amp; | \\ 
\end{bmatrix} \in \mathbb{R}^{F \times k}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eigenpairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">eigval</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span> <span class="n">eigvec</span><span class="p">[:,</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigval</span><span class="p">))</span>
<span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show the Sorted Eigen Pairs
</span><span class="k">print</span><span class="p">(</span><span class="s">"Unsorted: "</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">eval</span><span class="p">,</span><span class="n">evec</span> <span class="ow">in</span> <span class="n">eigenpairs</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">eval</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span><span class="n">evec</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
<span class="n">eigenpairs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">eigenpairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sorted in Decreasing Order: "</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">eval</span><span class="p">,</span><span class="n">evec</span> <span class="ow">in</span> <span class="n">eigenpairs</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">eval</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span><span class="n">evec</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
<span class="n">tot_eigval</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">eigval</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">e_val</span><span class="p">,</span> <span class="n">e_vec</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">eigenpairs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Eigenvalue {}: {:.2f}%"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">e_val</span><span class="o">/</span><span class="n">tot_eigval</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Unsorted: 
32.27195779972981 	 [ 0.20490976  0.38714331 -0.54648218 -0.71378517]
0.27756686384004264 	 [-0.00898234 -0.58899857  0.25428655 -0.76703217]
4.1311796919088535e-15 	 [-0.83786868  0.16963186  0.12293803  0.50407077]
1.1953730364935478e-14 	 [ 0.20003692 -0.39490682 -0.45668159  0.77167076]

Sorted in Decreasing Order: 
32.27195779972981 	 [ 0.20490976  0.38714331 -0.54648218 -0.71378517]
0.27756686384004264 	 [-0.00898234 -0.58899857  0.25428655 -0.76703217]
1.1953730364935478e-14 	 [ 0.20003692 -0.39490682 -0.45668159  0.77167076]
4.1311796919088535e-15 	 [-0.83786868  0.16963186  0.12293803  0.50407077]

Eigenvalue 0: 99.15%
Eigenvalue 1: 0.85%
Eigenvalue 2: 0.00%
Eigenvalue 3: 0.00%
</code></pre></div></div>

<p>By looking at this, it looks like the most informative eigenpairs are the top 2</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
</code></pre></div></div>

<h2 id="step-5-transform-the-data-to-the-latent-subspace">Step 5: Transform the Data to the Latent Subspace</h2>
<p>Now, the samples in the data space can be placed in the latent subspace by multiplying it by the transformation matrix $W$</p>

\[\underbrace{U}_{\mathbb{R}_{N \times k}} = \underbrace{X}_{\mathbb{R}^{N\times F}}\underbrace{W}_{\mathbb{R}^{F \times k}}\]

<p>This will lead that $W$ will be able to diagonalizes both $S_b$ and $S_w$, making LDA able to decorrelate the data between and within classes.</p>

\[W^T S_b W = \begin{bmatrix}* &amp; &amp; &amp; \\  &amp; * &amp; &amp; \\  &amp; &amp; \ddots &amp; \\  &amp; &amp; &amp; * \end{bmatrix}\]

\[W^T S_w W = \begin{bmatrix}* &amp; &amp; &amp; \\  &amp; * &amp; &amp; \\  &amp; &amp; \ddots &amp; \\  &amp; &amp; &amp; * \end{bmatrix}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">F</span><span class="p">,</span><span class="n">k</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="n">W</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">eigenpairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X_lda</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"X.shape: "</span><span class="p">,</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W.shape: "</span><span class="p">,</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"X_lda.shape: "</span><span class="p">,</span> <span class="n">X_lda</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X.shape:  (150, 4)
W.shape:  (4, 2)
X_lda.shape:  (150, 2)
</code></pre></div></div>

<h2 id="showing-the-results">Showing the Results</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">"o"</span><span class="p">,</span><span class="s">"x"</span><span class="p">,</span><span class="s">"^"</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">"blue"</span><span class="p">,</span><span class="s">"red"</span><span class="p">,</span><span class="s">"cyan"</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="n">X_c</span> <span class="o">=</span> <span class="n">X_lda</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">X_c</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">X_c</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"LD1"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"LD2"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Data Projection to 2 Linear Discriminants"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../assets/images/lda_plda/lda_results.png" alt="" /></p>]]></content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><category term="Machine Learning" /><summary type="html"><![CDATA[This post was highly inspired by Sebastian Raschka’s post The goal of LDA is to find some linear transformation $\mathbf{x} \rightarrow \mathbf{x}^T W$ that maximizes the between-class covariance with respect to the within-class covariance. \[\uparrow \frac{S_b}{S_w}\] Here, we can do LDA in a few steps Compute the mean vectors of the features for each of the different classes Compute the covariance matrices (between-class and within-class covariance matrices) Compute the eigenvectors and eigenvalues for the covariance matrices Sort the eigenvectors by the eigenvalues and choose the top $k$ eigenvectors to obtain the transformation matrix $W$ Use the transformation matrix $W$ to transform the data So, let us say that we have a dataset $X$, such that there are $N$ examples, and $F$ features \(X \in \mathbb{R}^{N \times F}\) And we have its labels $y \in \mathbb{R}^N$ Let us say that there are $C$ total classes, and $N_c$ represents the number of examples in the class $c$, where $c \in [1, C]$, and the dataset $\mathcal{D}_c$ represents all of the examples in the class $c$. Import Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.preprocessing import LabelEncoder Setup Dataset df = pd.io.parsers.read_csv( filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None, sep=',', ) label_dict = { 0: "Setosa", 1: "Versicolor", 2: "Virginica" } feature_dict = {0:'sepal_length', 1:'sepal_width', 2:'petal_length', 3:'petal_width'} df.columns = list(feature_dict.values())+["label"] df.dropna(how="all", inplace=True) X = df.iloc[:,[0,1,2,3]].values y = df["label"].values enc = LabelEncoder() enc = enc.fit(y) y = enc.transform(y) num_classes = len(np.unique(y)) N,F = X.shape Step 1: Compute the mean vectors Here, we can compute the mean vectors $\mathbf{m}_c$, where $c \in [1,C]$, with \[\mathbf{m}_c = \frac{1}{|\mathcal{D}_c|} \sum_{\mathbf{x} \in \mathcal{D}_c} \mathbf{x}\] Note that here, $\mathbf{x} \in \mathbb{R}^F$, as a column vector. Also note that $\mathbf{m}$ represents the overall mean of the whole dataset \[\mathbf{m} = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \mathbf{x}\] mean_vectors = [] for label in range(num_classes): mu_c = np.mean(X[y==label],axis=0) mean_vectors.append(mu_c) m = np.mean(X,axis=0).reshape(F,1) Step 2: Compute the Covariance Matrices Class Covariance To compute a class covariance $S_c$, we can have \(S_c = \sum_{\mathbf{x} \in \mathcal{D}_c} (\mathbf{x}-\mathbf{m}_c)(\mathbf{x}-\mathbf{m}_c)^T\) Within-Class Covariance To compute the within-class covariance, \(S_w = \sum_{c=1}^C S_c \in \mathbb{R}^{F \times F}\) Between-Class Covariance The between-class covariance can be computed as \(S_b = \sum_{c=1}^C |\mathcal{D}_c| (\mathbf{m}_c - \mathbf{m}) (\mathbf{m}_c - \mathbf{m})^T \in \mathbb{R}^{F\times F}\) def class_covariance(_class): global mean_vectors S_c = np.zeros((F,F)) m_c = mean_vectors[_class].reshape(F,1) for x in X[y==_class]: x = x.reshape(F,1) S_c += (x-m_c).dot((x-m_c).T) return S_c # Within Class Covariance S_w = np.zeros((F,F)) for class_num in range(num_classes): S_w += class_covariance(class_num) # Between Class Covariance S_b = np.zeros((F,F)) for class_num in range(num_classes): N_c, _ = X[y==class_num].shape m_c = mean_vectors[class_num].reshape(F,1) S_b += N_c * (m_c - m).dot((m_c - m).T) print("Within-class Covariance") print(S_w) print("Between-class Covariance") print(S_b) Within-class Covariance [[38.9562 13.683 24.614 5.6556] [13.683 17.035 8.12 4.9132] [24.614 8.12 27.22 6.2536] [ 5.6556 4.9132 6.2536 6.1756]] Between-class Covariance [[ 63.21213333 -19.534 165.16466667 71.36306667] [-19.534 10.9776 -56.0552 -22.4924 ] [165.16466667 -56.0552 436.64373333 186.90813333] [ 71.36306667 -22.4924 186.90813333 80.60413333]] Step 3: Solve the Generalized Eigenvalue Problem So, here, we understand that the eig(A) function solves for the eigenvalues and eigenvectors of $A$, where $A\mathbf{v} = \lambda \mathbf{v}$. Now, the linear constraints are such that \[S_b \mathbf{v} = \lambda S_w \mathbf{v}\] \[\underbrace{S_w^{-1}S_b}_{A} \mathbf{v} = \lambda \mathbf{v}\] Therefore, we can compute the eigv*’s for it as \[eig(\underbrace{S_w^{-1}S_b}_{\mathbb{R}^{F\times F}})\] A = np.linalg.inv(S_w).dot(S_b) eigval, eigvec = np.linalg.eig(A) # Show the Eigenvalue Solutions for k in range(len(eigval)): evec = eigvec[:,k].reshape(F,1) print("Eigval: ",eigval[k]) print("Eigvec: \n",evec) print("") np.testing.assert_array_almost_equal( np.linalg.inv(S_w).dot(S_b).dot(evec), eigval[k]*evec, decimal = 6, verbose=True ) print("All eigvecs and eigvals match.") Eigval: 32.27195779972981 Eigvec: [[ 0.20490976] [ 0.38714331] [-0.54648218] [-0.71378517]] Eigval: 0.27756686384004264 Eigvec: [[-0.00898234] [-0.58899857] [ 0.25428655] [-0.76703217]] Eigval: -4.1311796919088535e-15 Eigvec: [[-0.83786868] [ 0.16963186] [ 0.12293803] [ 0.50407077]] Eigval: 1.1953730364935478e-14 Eigvec: [[ 0.20003692] [-0.39490682] [-0.45668159] [ 0.77167076]] All eigvecs and eigvals match. Step 4: Select the linear discriminants for the new latent subspace Here, all you have to do is to find the top $k$ eigenvalues and their respective eigenvectors. And then set the transformation. So, if $\mathbf{w}_F$ is the eigenvector associated with the largest eigenvalue $\lambda_F$, and $\mathbf{w}_1$ is the eigenvector associated with the smallest eigenvalue $\lambda_1$ of $S_w^{-1}S_b$, and \[\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_F\] Then the transformation matrix $W$ is \[W = \begin{bmatrix} | &amp; | &amp; &amp; | \\ \mathbf{w}_F &amp; \mathbf{w}_{F-1} &amp; \cdots &amp; \mathbf{w}_{F-k} \\ | &amp; | &amp; &amp; | \\ \end{bmatrix} \in \mathbb{R}^{F \times k}\] eigenpairs = [ (np.abs(eigval[k]), eigvec[:,k]) for k in range(len(eigval)) ] # Show the Sorted Eigen Pairs print("Unsorted: ") for eval,evec in eigenpairs: print(eval,"\t",evec) print("") eigenpairs = sorted(eigenpairs, key=lambda pair: pair[0],reverse=True) print("Sorted in Decreasing Order: ") for eval,evec in eigenpairs: print(eval,"\t",evec) print("") tot_eigval = np.sum(eigval) for i,(e_val, e_vec) in enumerate(eigenpairs): print("Eigenvalue {}: {:.2f}%".format(i,e_val/tot_eigval*100)) Unsorted: 32.27195779972981 [ 0.20490976 0.38714331 -0.54648218 -0.71378517] 0.27756686384004264 [-0.00898234 -0.58899857 0.25428655 -0.76703217] 4.1311796919088535e-15 [-0.83786868 0.16963186 0.12293803 0.50407077] 1.1953730364935478e-14 [ 0.20003692 -0.39490682 -0.45668159 0.77167076] Sorted in Decreasing Order: 32.27195779972981 [ 0.20490976 0.38714331 -0.54648218 -0.71378517] 0.27756686384004264 [-0.00898234 -0.58899857 0.25428655 -0.76703217] 1.1953730364935478e-14 [ 0.20003692 -0.39490682 -0.45668159 0.77167076] 4.1311796919088535e-15 [-0.83786868 0.16963186 0.12293803 0.50407077] Eigenvalue 0: 99.15% Eigenvalue 1: 0.85% Eigenvalue 2: 0.00% Eigenvalue 3: 0.00% By looking at this, it looks like the most informative eigenpairs are the top 2 k = 2 Step 5: Transform the Data to the Latent Subspace Now, the samples in the data space can be placed in the latent subspace by multiplying it by the transformation matrix $W$ \[\underbrace{U}_{\mathbb{R}_{N \times k}} = \underbrace{X}_{\mathbb{R}^{N\times F}}\underbrace{W}_{\mathbb{R}^{F \times k}}\] This will lead that $W$ will be able to diagonalizes both $S_b$ and $S_w$, making LDA able to decorrelate the data between and within classes. \[W^T S_b W = \begin{bmatrix}* &amp; &amp; &amp; \\ &amp; * &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; * \end{bmatrix}\] \[W^T S_w W = \begin{bmatrix}* &amp; &amp; &amp; \\ &amp; * &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; * \end{bmatrix}\] W = np.zeros((F,k)) for i in range(k): W[:,i] = eigenpairs[i][1] X_lda = X.dot(W) print("X.shape: ",X.shape) print("W.shape: ",W.shape) print("X_lda.shape: ", X_lda.shape) X.shape: (150, 4) W.shape: (4, 2) X_lda.shape: (150, 2) Showing the Results markers = ["o","x","^"] colors = ["blue","red","cyan"] fig,ax = plt.subplots() for i in range(num_classes): X_c = X_lda[y==i] ax.scatter( x = X_c[:,0], y = X_c[:,1], marker=markers[i], color=colors[i], label = label_dict[i] ) ax.set_xlabel("LD1") ax.set_ylabel("LD2") ax.set_title("Data Projection to 2 Linear Discriminants") ax.grid() ax.legend()]]></summary></entry></feed>