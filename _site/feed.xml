<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-02-06T17:26:17-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nick Shu. A Fool in the Making</title><subtitle>Nick's Personal Website
</subtitle><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><entry><title type="html">Bayesian Inference on Gaussian Distributions</title><link href="http://localhost:4000/bayesian_inference_for_gaussian.html" rel="alternate" type="text/html" title="Bayesian Inference on Gaussian Distributions" /><published>2021-02-06T00:00:00-05:00</published><updated>2021-02-06T00:00:00-05:00</updated><id>http://localhost:4000/bayesian-inference-gaussian</id><content type="html" xml:base="http://localhost:4000/bayesian_inference_for_gaussian.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Here we will try to better understand how Bayesian inference works for Gaussian distributions.&lt;/p&gt;

&lt;h2 id=&quot;likelihood&quot;&gt;Likelihood&lt;/h2&gt;

&lt;p&gt;Let’s say that we have a dataset $\boldsymbol{X} = { x_1, \cdots, x_N }$, where $x_i \in \mathbb{R}$ and $x_i \sim \mathcal{N(\mu,\sigma)}$. In other words, we have $N$ examples from an univariate Gaussian distribution which has the mean $\mu$ and the variance $\sigma$ for the distribution. Let us also assume that all of the $N$ examples are sampled independently from each other. Therefore, the likelihood of the dataset is defined as the probability of obtaining the data given $\mu$, as a function of $\mu$. Note that this all makes sense in a diverging connection in a Bayesian network, where the mean node diverges to $N$ nodes, and if $\mu$ is known, then all of the samples are independent.&lt;/p&gt;

\[\bbox[teal,4pt]{p(\boldsymbol{X}|\mu) = \prod_{i=1}^N p(x_i|\mu) = \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right]}\]

&lt;p&gt;Although this is defined as the likelihood, it would be best to determine a single equation that does not have have a factorized notation (i.e. $\prod$). Now, let us define the empirical mean $\bar{x}$ and empirical variance $s^2$.&lt;/p&gt;

\[\begin{align}
\bar{x} &amp;amp;= \frac{1}{N}\sum_{i=1}^N x_i \\ 
s^2 &amp;amp;= \frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2
\end{align}\]

&lt;p&gt;Therefore, we can rewrite&lt;/p&gt;

\[\begin{align}
\sum_{i=1}^N (x_i - \mu)^2 &amp;amp;= \sum_{i=1}^N (x_i - \bar{x} - \mu + \bar{x})^2 \\ 
&amp;amp;= \sum_{i=1}^N [(x_i - \bar{x}) - (\mu - \bar{x})]^2 \\ 
&amp;amp;= \underbrace{\sum_{i=1}^N (x_i - \bar{x})^2}_{ns^2} + \sum_{i=1}^N (\bar{x} - \mu)^2 - 2 \sum_{i=1}^N (x_i - \bar{x})(\mu - \bar{x}) \\ 
&amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( \left(\sum_{i=1}^N x_i \right)-N\bar{x} \right) (\mu - \bar{x})\right] \\ 
&amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( N\bar{x}-N\bar{x} \right) (\mu - \bar{x})\right] \\ 
&amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 - 0 \\ 
&amp;amp;= Ns^2 + \sum_{i=1}^N \underbrace{(\bar{x} - \mu)^2}_{\text{indep of $i$}} \\
&amp;amp;= Ns^2 + N (\bar{x} - \mu)^2
\end{align}\]

&lt;p&gt;Therefore, by putting this into the likelihood&lt;/p&gt;

\[\begin{align}
p(\boldsymbol{X}|\mu) &amp;amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right] \\ 
&amp;amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} [Ns^2 + N (\bar{x} - \mu)^2]\right] \\ 
&amp;amp;\propto (\sigma^2)^{N/2} \exp\left( -\frac{Ns^2}{2\sigma^2} \right) \exp \left( -\frac{N}{2\sigma^2} (\bar{x}-\mu)^2\right)
\end{align}\]

&lt;p&gt;Now, if $\sigma^2$ is constant, that means that all of the data examples have the same variance, and if this is constant, then we can drop the constant factors from the likelihood, thus obtaining that the likelihood of the dataset is&lt;/p&gt;

\[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{N}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\]

&lt;h2 id=&quot;prior&quot;&gt;Prior&lt;/h2&gt;

&lt;p&gt;Now let’s compute the prior. Often, for different distributions, a natural conjugate prior is a prior which leads to the posterior being in the same distribution as the prior.  On the dataset, $\sigma^2$ is the variance of the observation noise, and $\mu$ is the mean of the observations. So, since the likelihood has the form&lt;/p&gt;

\[p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{n}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{n} \right)\]

&lt;p&gt;Then the natural conjugate prior $p(\mu)$ has the form&lt;/p&gt;

\[\bbox[teal,4pt]{p(\mu) \propto \exp\left[ - \frac{(\mu - \mu_0)^2}{2 \sigma_0^2}  \right] \propto \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\]

&lt;p&gt;where the mean of the prior is $\mu_0$, and the variance of the prior is $\sigma_0^2$.&lt;/p&gt;

&lt;h2 id=&quot;posterior&quot;&gt;Posterior&lt;/h2&gt;

&lt;p&gt;Now, we need to compute the posterior $p(\mu \vert \boldsymbol{X})$. The posterior distribution is given by&lt;/p&gt;

\[p(\mu \vert \boldsymbol{X}) \propto p(\boldsymbol{X} \vert \mu) p (\mu)\]

&lt;p&gt;By expanding it, we will have&lt;/p&gt;

\[\begin{align}
p(\mu | \boldsymbol{X}) &amp;amp;\propto p(\boldsymbol{X} | \mu) p (\mu) \\ 
&amp;amp;\propto \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right) \cdot \exp \left( -\frac{1}{2\sigma_0^2} (\mu - \mu_0)^2 \right) \\ 
&amp;amp;= \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i^2 + \mu^2 - 2x_i \mu) + \frac{-1}{2\sigma_0^2} (\mu^2 + \mu_0 - 2 \mu_0 \mu)\right] 
\end{align}\]

&lt;p&gt;We know that the product of two Gaussians yield another Gaussian. So we can write&lt;/p&gt;

\[\begin{align} p(\mu | \boldsymbol{X}) &amp;amp;\propto \exp \left[  \color{cyan}{-\frac{\sum_i x_i^2}{2\sigma^2}}  \color{green}{- \frac{N \mu^2}{2\sigma^2}} \color{magenta}{+\frac{\sum_i x_i \mu}{\sigma^2}} \color{green}{- \frac{\mu^2}{2\sigma_0^2}} \color{cyan}{- \frac{\mu_0}{2\sigma_0^2}} \color{magenta}{+ \frac{\mu_0\mu}{\sigma_0^2}} \right] \\ 
&amp;amp;= \exp \left[ \color{green}{- \frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2} \right)} \color{magenta}{+\mu \left( \frac{\sum_i x_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)} \color{cyan}{- \left( \frac{\sum_i x^2}{2 \sigma^2} + \frac{\mu_0}{2\sigma_0} \right)} \right] \\ 
&amp;amp;\triangleq \exp \left[ -\frac{1}{2\sigma_N^2} (\mu - \mu_N)^2 \right] \\ 
&amp;amp;= \exp \left[ -\frac{1}{2\sigma_N^2} \left( \color{green}{\mu^2} \color{magenta}{-2\mu_N \mu} \color{cyan}{+\mu_N^2} \right)\right]
\end{align}\]

&lt;p&gt;By matching the different coefficients, we can determine the posterior variance $\sigma_N^2$ and the posterior mean $\mu_N$. By matching the $\mu^2$ first, we can determine the $\sigma_N^2$:&lt;/p&gt;

\[\begin{align} 
-\frac{\mu^2}{2\sigma_N^2} &amp;amp;= -\frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2}\right) \\ 
\frac{1}{\sigma_N^2} &amp;amp;= \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = \frac{N\sigma_0^2 + \sigma^2}{\sigma^2\sigma_0^2}\\
\sigma_N^2 &amp;amp;= \left( \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} \right)^{-1} = \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}
\end{align}\]

&lt;p&gt;By matching the $\mu$ coefficients, we can determine the posterior mean $\mu_N$&lt;/p&gt;

\[\begin{align} 
\frac{-2\mu \mu_N}{-2\sigma_N^2} &amp;amp;= \mu \left( \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right) \\ 
\frac{\mu_N}{\sigma_N^2} &amp;amp;= \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ 
&amp;amp;= \frac{N \bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ 
&amp;amp;= \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
\mu_N &amp;amp;= \sigma_N^2 \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
&amp;amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \cdot \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ 
&amp;amp;= \frac{\sigma_0^2 n\bar{x} + \sigma^2 \mu_0}{N\sigma_0^2 + \sigma^2} \\ 
&amp;amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\bar{x} \\ 
&amp;amp;= \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right)
\end{align}\]

&lt;p&gt;So, the two parameters for the posterior Gaussian are:&lt;/p&gt;

\[\bbox[teal,4pt]{\begin{align}
\mu_N &amp;amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \underbrace{\bar{x}}_{\mu_{ML}} = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \\
\sigma_N^2 &amp;amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}
\end{align}}\]

&lt;p&gt;Note that $\bar{x} = \mu_{ML} = N^{-1} \sum_{i=1}^N x_i$ as the maximum likelihood solution for $\mu$. Note that you can represent the variance as the &lt;strong&gt;precision&lt;/strong&gt; of a Gaussian, which should be the inverse of the variance.&lt;/p&gt;

\[\lambda = \frac{1}{\sigma^2} \quad \quad \lambda_0 = \frac{1}{\sigma_0^2} \quad \quad \lambda_N = \frac{1}{\sigma_N^2}\]

&lt;p&gt;We can write the precision of the posterior Gaussian as&lt;/p&gt;

\[\lambda_N = \frac{1}{\sigma_N^2} = \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = N\lambda + \lambda_0\]

&lt;p&gt;And for the mean of the posterior using the precision,&lt;/p&gt;

\[\mu_N = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2} \right) = \frac{\mu_0 \lambda_0 + n\bar{x} \lambda}{\lambda_N}\]

&lt;p&gt;Let us look at how all of these equations behave when you add more data.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For each of the following, when you have $N\rightarrow \infty$, then:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Mean: $\displaystyle \lim_{\color{green}{N}\rightarrow \infty} \mu_N = \lim_{\color{green}{N}\rightarrow \infty} \frac{\sigma^2}{\color{green}{N}\sigma_0^2 + \sigma^2}\mu_0 + \frac{\color{green}{N}\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} \mu_{ML} = \mu_{ML}$&lt;/li&gt;
    &lt;li&gt;Variance: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \sigma_{\color{green}{N}}^2 = \lim_{\color{green}{N\rightarrow \infty}} \frac{\sigma^2\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} = 0$&lt;/li&gt;
    &lt;li&gt;Precision: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \lambda_{\color{green}{N}} = \lim_{\color{green}{N\rightarrow \infty}} = \color{green}{N}\lambda + \lambda_0 = \infty$&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;This all starts to make sense intuitively. As we increase the number of examples, the mean progressively starts to get closer to the mean of the maximum likelihood of the Gaussian data. Meanwhile, the variance steadily starts to shrink to infinitesimally small values, whereas the precision starts to become infinitely high.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;posterior-predictive&quot;&gt;Posterior Predictive&lt;/h2&gt;

&lt;p&gt;Now, we come to compute the posterior predictive. As a recap (Bishop Eq. 2.113), if we have a marginal Gaussian distribution $p(x)$ and a conditional Gaussian distribution $p(y \vert x)$&lt;/p&gt;

\[\begin{align} p(x) &amp;amp;= \texttip{\mathcal{N} (x | \mu, \color{magenta}{\Lambda^{-1}})}{Marginal} \\ p(y|x) &amp;amp;= \mathcal{N} (y | \color{purple}{Ax + b}, \color{yellow}{L^{-1}})\end{align}\]

&lt;p&gt;Then to compute the marginal distribution $p(y)$ is&lt;/p&gt;

\[p(y) = \mathcal{N} (y|\color{purple}{A\mu+b}, \color{yellow}{L^{-1}} + A\color{magenta}{\Lambda^{-1}} A^T)\]

&lt;p&gt;So, the posterior predictive is given by&lt;/p&gt;

\[\begin{align} 
p(x|\boldsymbol{X}) &amp;amp;= \int p(x|\mu) \cdot p(\mu|\boldsymbol{X}) d\mu \\ 
&amp;amp;= \int \mathcal{N(x|\mu, \sigma^2) \cdot \mathcal{N}(\mu | \color{purple}{\mu_N}, \color{yellow}{\sigma_N^2}) d\mu } 
\end{align}\]

\[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\color{purple}{\mu_N}, \color{yellow}{\sigma_N^2} + \color{magenta}{\sigma^2})}\]

&lt;p&gt;[Alternate Proof here]&lt;/p&gt;

&lt;h2 id=&quot;marginal-likelihood&quot;&gt;Marginal Likelihood&lt;/h2&gt;

&lt;p&gt;The marginal likelihood $l$ can be defined as&lt;/p&gt;

\[\bbox[4pt,teal]{\begin{align}
l &amp;amp;= p(\boldsymbol{X}) = \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\
&amp;amp;=\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]
\end{align}}\]

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;In order to compute the marginal likelihood, let us call $m=\mu_0$ and $\tau^2 = \sigma_0^2$ as our hyperparameters for simplicity&lt;/p&gt;

\[\begin{align}
l &amp;amp;= p(\boldsymbol{X}) \\
&amp;amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\
&amp;amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | m, \tau^2) d\mu \\
&amp;amp;= \frac{1}{(\sigma \sqrt{2\pi})^n (\tau \sqrt{2\pi})} \int \exp \left( - \frac{1}{2\sigma^2 \sum_i (x_i - \mu)^2 - \frac{1}{2\tau^2} (\mu-m)^2 }\right) d\mu
\end{align}\]

  &lt;p&gt;Let us now define $S^2 = \frac{1}{\sigma^2}$ and $T^2 = \frac{1}{\tau^2}$
 \(\begin{align}
p(\boldsymbol{X}) &amp;amp;= \frac{1}{(S^{-1}\sqrt{2\pi})^n (T^{-1} \sqrt{2\pi})} \int \exp \left[ -\frac{S^2}{2} \left( \sum_i x_i^2 + n \mu^2 - 2\mu \sum_i x_i\right) - \frac{T^2}{2} \left(\mu^2 +m^2-2m\mu\right)\right] \\
&amp;amp;= \color{purple}{\frac{1}{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}} \int \color{purple}{e^{-\frac{S^2 \sum_i x_i^2}{2}}} e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} \color{purple}{e^{-\frac{T^2}{2}m^2}} e^{T^2m\mu} d\mu \\
&amp;amp;= \underbrace{\color{purple}{\frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}}}_{c} \int e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu} d\mu \\
&amp;amp;= c \int \color{violet}{e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu}} d\mu \\
&amp;amp;= c \int \color{violet}{\exp \left[ -\frac{1}{2} \left( S^2N\mu^2 - 2S^2\mu \sum_i x_i +T^2 \mu^2 -2T^2 m \mu\right) \right]}  d\mu \\
&amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \underbrace{\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} }_{\alpha}\right)\right] d\mu \\
&amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha\right)\right] d\mu \\
&amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha \color{violet}{+ \alpha^2 - \alpha^2} \right) \right] d\mu \\
&amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \color{purple}{+ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 }\right] d\mu \\
&amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} d\mu \\
&amp;amp;= c  \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] d\mu \\
&amp;amp;= c  \exp \left[ \frac{1}{2} \color{red}{\left( S^2N + T^2 \right) \left( \frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} \right)^2} \right] \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu \\ 
&amp;amp;= c  \exp \left[ \frac{1}{2}  \color{red}{\frac{(S^2\sum_i x_i +T^2 m)^2}{S^2 N + T^2}} \right] \underbrace{\color{yellow}{\int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu}}_{\cdots = \frac{1.25331 erf(\frac{0.7071 -T^2m + s^2 N(\mu-\bar{x})+T^2\mu }{\sqrt{NS^2 + T^2}})}{\sqrt{NS^2 + T^2}} = \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} }  \\
&amp;amp;= c  \exp \left[ \frac{1}{2}  \frac{(S^2 \color{brown}{\sum_i x_i} +T^2 m)^2}{S^2 N + T^2} \right] \color{yellow}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ 
&amp;amp;= c  \exp \left[ \frac{1}{2}  \frac{(S^2 \color{brown}{N \bar{x}}+T^2 m)^2}{S^2 N + T^2} \right] \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} \\ 
&amp;amp;= \frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}}  \exp \left[ \color{violet}{ \frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\
&amp;amp;= \frac{1 }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2 \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}}
\end{align}\)&lt;/p&gt;

  &lt;p&gt;Now, let us work on the constant and the exponent portion:&lt;/p&gt;

\[\color{lime}{\frac{\sqrt{2\pi}}{T^{-1}\sqrt{2\pi}\sqrt{S^2N + T^2}} = \frac{1}{\tau\sqrt{\frac{N}{\sigma^2} + \frac{1}{\tau^2}}}\frac{\sigma}{\sigma} = \frac{\sigma}{\sqrt{\sigma^2\tau^2 \left( \frac{n}{\sigma^2}+ \frac{1}{\tau^2} \right)}} = \frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} }\]

\[\color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} = \frac{\left( \frac{1}{\sigma^2} n\bar{x} + \frac{1}{\tau^2} m \right)^2}{2 \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2}\right)} = \frac{\left(\frac{N \bar{x} \tau^2+ \sigma^2 m}{\tau^2 \sigma^2} \right)^2}{2 \left( \frac{\tau^2 N +\sigma^2}{\tau^2 \sigma^2} \right)} = \frac{(N\bar{x}\tau^2 + \sigma^2 m)^2}{2\tau^2 \sigma^2 (\tau^2 N + \sigma^2)}}\]

\[\color{violet}{= \frac{N^2 \bar{x} \tau^4 + \sigma^4 m^2 + 2 \sigma^2 \tau^2 m N \bar{x}}{2 \sigma^2 \tau^2 (\tau^2 N + \sigma^2)} = \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)}}\]

  &lt;p&gt;Therefore, bringing it all together, we obtain&lt;/p&gt;

\[\begin{align}
p(\boldsymbol{X}) &amp;amp;=  \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{(T^{-1}\sqrt{2\pi})\sqrt{S^2N + T^2}}} \\
&amp;amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} }\right] \color{lime}{\frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} } \\
&amp;amp;= \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\tau^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} \right] 
\end{align}\]

  &lt;p&gt;By substituting the $\tau$ and $m$ terms back again, we obtain&lt;/p&gt;

\[\bbox[teal,4pt]{p(\boldsymbol{X}) = \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\sigma_0^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 \mu_0^2}{\sigma_0^2} + 2 \mu_0 N\bar{x}}{2 (\sigma_0^2 N + \sigma)} \right] }\]
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Bringing it all together, if one is to have a dataset $\boldsymbol{X}$ with $N$ examples, which are all univariate, the &lt;strong&gt;likelihood&lt;/strong&gt; of obtaining the dataset given some mean $\mu$, is&lt;/p&gt;

\[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) = \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\]

&lt;p&gt;The &lt;strong&gt;prior distribution&lt;/strong&gt; for the mean $\mu$ has a prior mean $\mu_0$ and variance $\sigma_0^2$ being represented as&lt;/p&gt;

\[\bbox[teal,4pt]{p(\mu) = \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\]

&lt;p&gt;The &lt;strong&gt;posterior distribution&lt;/strong&gt; for the mean given the dataset can be described as&lt;/p&gt;

\[\bbox[teal,4pt]{p(\mu | \boldsymbol{X} ) = \mathcal{N} \left( \mu \bigg| \underbrace{\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \bar{x}}_{\mu_N}, \underbrace{\frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}}_{\sigma^2_N} \right) }\]

&lt;p&gt;And in order to identify the probability of a probe sample $x$ given the dataset $\boldsymbol{X}$ can be described as&lt;/p&gt;

\[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)}\]

&lt;p&gt;Finally, to obtain the marginal likelihood of the dataset, one can compute as&lt;/p&gt;

\[\bbox[4pt,teal]{p(\boldsymbol{X}) =\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]}\]

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kevin P. Murphy &lt;a href=&quot;https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf&quot;&gt;“Conjugate Bayesian Analysis of the Gaussian Distribution”&lt;/a&gt; 3 Oct 2007&lt;/li&gt;
  &lt;li&gt;Christopher Bishop - Pattern Recognition&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Machine Learning" /><category term="Math" /><summary type="html">Introduction Here we will try to better understand how Bayesian inference works for Gaussian distributions. Likelihood Let’s say that we have a dataset $\boldsymbol{X} = { x_1, \cdots, x_N }$, where $x_i \in \mathbb{R}$ and $x_i \sim \mathcal{N(\mu,\sigma)}$. In other words, we have $N$ examples from an univariate Gaussian distribution which has the mean $\mu$ and the variance $\sigma$ for the distribution. Let us also assume that all of the $N$ examples are sampled independently from each other. Therefore, the likelihood of the dataset is defined as the probability of obtaining the data given $\mu$, as a function of $\mu$. Note that this all makes sense in a diverging connection in a Bayesian network, where the mean node diverges to $N$ nodes, and if $\mu$ is known, then all of the samples are independent. \[\bbox[teal,4pt]{p(\boldsymbol{X}|\mu) = \prod_{i=1}^N p(x_i|\mu) = \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right]}\] Although this is defined as the likelihood, it would be best to determine a single equation that does not have have a factorized notation (i.e. $\prod$). Now, let us define the empirical mean $\bar{x}$ and empirical variance $s^2$. \[\begin{align} \bar{x} &amp;amp;= \frac{1}{N}\sum_{i=1}^N x_i \\ s^2 &amp;amp;= \frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2 \end{align}\] Therefore, we can rewrite \[\begin{align} \sum_{i=1}^N (x_i - \mu)^2 &amp;amp;= \sum_{i=1}^N (x_i - \bar{x} - \mu + \bar{x})^2 \\ &amp;amp;= \sum_{i=1}^N [(x_i - \bar{x}) - (\mu - \bar{x})]^2 \\ &amp;amp;= \underbrace{\sum_{i=1}^N (x_i - \bar{x})^2}_{ns^2} + \sum_{i=1}^N (\bar{x} - \mu)^2 - 2 \sum_{i=1}^N (x_i - \bar{x})(\mu - \bar{x}) \\ &amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( \left(\sum_{i=1}^N x_i \right)-N\bar{x} \right) (\mu - \bar{x})\right] \\ &amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 -2 \left[ \left( N\bar{x}-N\bar{x} \right) (\mu - \bar{x})\right] \\ &amp;amp;= Ns^2 + \sum_{i=1}^N (\bar{x} - \mu)^2 - 0 \\ &amp;amp;= Ns^2 + \sum_{i=1}^N \underbrace{(\bar{x} - \mu)^2}_{\text{indep of $i$}} \\ &amp;amp;= Ns^2 + N (\bar{x} - \mu)^2 \end{align}\] Therefore, by putting this into the likelihood \[\begin{align} p(\boldsymbol{X}|\mu) &amp;amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right] \\ &amp;amp;= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left[ -\frac{1}{2\sigma^2} [Ns^2 + N (\bar{x} - \mu)^2]\right] \\ &amp;amp;\propto (\sigma^2)^{N/2} \exp\left( -\frac{Ns^2}{2\sigma^2} \right) \exp \left( -\frac{N}{2\sigma^2} (\bar{x}-\mu)^2\right) \end{align}\] Now, if $\sigma^2$ is constant, that means that all of the data examples have the same variance, and if this is constant, then we can drop the constant factors from the likelihood, thus obtaining that the likelihood of the dataset is \[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{N}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\] Prior Now let’s compute the prior. Often, for different distributions, a natural conjugate prior is a prior which leads to the posterior being in the same distribution as the prior. On the dataset, $\sigma^2$ is the variance of the observation noise, and $\mu$ is the mean of the observations. So, since the likelihood has the form \[p(\boldsymbol{X} | \mu) \propto \exp \left( -\frac{n}{2\sigma^2} (\bar{x} - \mu)^2 \right) \propto \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{n} \right)\] Then the natural conjugate prior $p(\mu)$ has the form \[\bbox[teal,4pt]{p(\mu) \propto \exp\left[ - \frac{(\mu - \mu_0)^2}{2 \sigma_0^2} \right] \propto \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\] where the mean of the prior is $\mu_0$, and the variance of the prior is $\sigma_0^2$. Posterior Now, we need to compute the posterior $p(\mu \vert \boldsymbol{X})$. The posterior distribution is given by \[p(\mu \vert \boldsymbol{X}) \propto p(\boldsymbol{X} \vert \mu) p (\mu)\] By expanding it, we will have \[\begin{align} p(\mu | \boldsymbol{X}) &amp;amp;\propto p(\boldsymbol{X} | \mu) p (\mu) \\ &amp;amp;\propto \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 \right) \cdot \exp \left( -\frac{1}{2\sigma_0^2} (\mu - \mu_0)^2 \right) \\ &amp;amp;= \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i^2 + \mu^2 - 2x_i \mu) + \frac{-1}{2\sigma_0^2} (\mu^2 + \mu_0 - 2 \mu_0 \mu)\right] \end{align}\] We know that the product of two Gaussians yield another Gaussian. So we can write \[\begin{align} p(\mu | \boldsymbol{X}) &amp;amp;\propto \exp \left[ \color{cyan}{-\frac{\sum_i x_i^2}{2\sigma^2}} \color{green}{- \frac{N \mu^2}{2\sigma^2}} \color{magenta}{+\frac{\sum_i x_i \mu}{\sigma^2}} \color{green}{- \frac{\mu^2}{2\sigma_0^2}} \color{cyan}{- \frac{\mu_0}{2\sigma_0^2}} \color{magenta}{+ \frac{\mu_0\mu}{\sigma_0^2}} \right] \\ &amp;amp;= \exp \left[ \color{green}{- \frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2} \right)} \color{magenta}{+\mu \left( \frac{\sum_i x_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)} \color{cyan}{- \left( \frac{\sum_i x^2}{2 \sigma^2} + \frac{\mu_0}{2\sigma_0} \right)} \right] \\ &amp;amp;\triangleq \exp \left[ -\frac{1}{2\sigma_N^2} (\mu - \mu_N)^2 \right] \\ &amp;amp;= \exp \left[ -\frac{1}{2\sigma_N^2} \left( \color{green}{\mu^2} \color{magenta}{-2\mu_N \mu} \color{cyan}{+\mu_N^2} \right)\right] \end{align}\] By matching the different coefficients, we can determine the posterior variance $\sigma_N^2$ and the posterior mean $\mu_N$. By matching the $\mu^2$ first, we can determine the $\sigma_N^2$: \[\begin{align} -\frac{\mu^2}{2\sigma_N^2} &amp;amp;= -\frac{\mu^2}{2} \left( \frac{N}{\sigma} + \frac{1}{\sigma_0^2}\right) \\ \frac{1}{\sigma_N^2} &amp;amp;= \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = \frac{N\sigma_0^2 + \sigma^2}{\sigma^2\sigma_0^2}\\ \sigma_N^2 &amp;amp;= \left( \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} \right)^{-1} = \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \end{align}\] By matching the $\mu$ coefficients, we can determine the posterior mean $\mu_N$ \[\begin{align} \frac{-2\mu \mu_N}{-2\sigma_N^2} &amp;amp;= \mu \left( \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right) \\ \frac{\mu_N}{\sigma_N^2} &amp;amp;= \frac{\sum_{i=1}^N x_i }{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ &amp;amp;= \frac{N \bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \\ &amp;amp;= \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ \mu_N &amp;amp;= \sigma_N^2 \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ &amp;amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \cdot \frac{\sigma_0^2 N\bar{x} + \sigma^2 \mu_0 }{\sigma^2 \sigma_0^2} \\ &amp;amp;= \frac{\sigma_0^2 n\bar{x} + \sigma^2 \mu_0}{N\sigma_0^2 + \sigma^2} \\ &amp;amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\bar{x} \\ &amp;amp;= \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \end{align}\] So, the two parameters for the posterior Gaussian are: \[\bbox[teal,4pt]{\begin{align} \mu_N &amp;amp;= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \underbrace{\bar{x}}_{\mu_{ML}} = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N\bar{x}}{\sigma^2} \right) \\ \sigma_N^2 &amp;amp;= \frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2} \end{align}}\] Note that $\bar{x} = \mu_{ML} = N^{-1} \sum_{i=1}^N x_i$ as the maximum likelihood solution for $\mu$. Note that you can represent the variance as the precision of a Gaussian, which should be the inverse of the variance. \[\lambda = \frac{1}{\sigma^2} \quad \quad \lambda_0 = \frac{1}{\sigma_0^2} \quad \quad \lambda_N = \frac{1}{\sigma_N^2}\] We can write the precision of the posterior Gaussian as \[\lambda_N = \frac{1}{\sigma_N^2} = \frac{N}{\sigma^2} + \frac{1}{\sigma_0^2} = N\lambda + \lambda_0\] And for the mean of the posterior using the precision, \[\mu_N = \sigma_N^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2} \right) = \frac{\mu_0 \lambda_0 + n\bar{x} \lambda}{\lambda_N}\] Let us look at how all of these equations behave when you add more data. For each of the following, when you have $N\rightarrow \infty$, then: Mean: $\displaystyle \lim_{\color{green}{N}\rightarrow \infty} \mu_N = \lim_{\color{green}{N}\rightarrow \infty} \frac{\sigma^2}{\color{green}{N}\sigma_0^2 + \sigma^2}\mu_0 + \frac{\color{green}{N}\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} \mu_{ML} = \mu_{ML}$ Variance: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \sigma_{\color{green}{N}}^2 = \lim_{\color{green}{N\rightarrow \infty}} \frac{\sigma^2\sigma_0^2}{\color{green}{N}\sigma_0^2 + \sigma^2} = 0$ Precision: $\displaystyle \lim_{\color{green}{N\rightarrow \infty}} \lambda_{\color{green}{N}} = \lim_{\color{green}{N\rightarrow \infty}} = \color{green}{N}\lambda + \lambda_0 = \infty$ This all starts to make sense intuitively. As we increase the number of examples, the mean progressively starts to get closer to the mean of the maximum likelihood of the Gaussian data. Meanwhile, the variance steadily starts to shrink to infinitesimally small values, whereas the precision starts to become infinitely high. Posterior Predictive Now, we come to compute the posterior predictive. As a recap (Bishop Eq. 2.113), if we have a marginal Gaussian distribution $p(x)$ and a conditional Gaussian distribution $p(y \vert x)$ \[\begin{align} p(x) &amp;amp;= \texttip{\mathcal{N} (x | \mu, \color{magenta}{\Lambda^{-1}})}{Marginal} \\ p(y|x) &amp;amp;= \mathcal{N} (y | \color{purple}{Ax + b}, \color{yellow}{L^{-1}})\end{align}\] Then to compute the marginal distribution $p(y)$ is \[p(y) = \mathcal{N} (y|\color{purple}{A\mu+b}, \color{yellow}{L^{-1}} + A\color{magenta}{\Lambda^{-1}} A^T)\] So, the posterior predictive is given by \[\begin{align} p(x|\boldsymbol{X}) &amp;amp;= \int p(x|\mu) \cdot p(\mu|\boldsymbol{X}) d\mu \\ &amp;amp;= \int \mathcal{N(x|\mu, \sigma^2) \cdot \mathcal{N}(\mu | \color{purple}{\mu_N}, \color{yellow}{\sigma_N^2}) d\mu } \end{align}\] \[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\color{purple}{\mu_N}, \color{yellow}{\sigma_N^2} + \color{magenta}{\sigma^2})}\] [Alternate Proof here] Marginal Likelihood The marginal likelihood $l$ can be defined as \[\bbox[4pt,teal]{\begin{align} l &amp;amp;= p(\boldsymbol{X}) = \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\ &amp;amp;=\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right] \end{align}}\] Proof In order to compute the marginal likelihood, let us call $m=\mu_0$ and $\tau^2 = \sigma_0^2$ as our hyperparameters for simplicity \[\begin{align} l &amp;amp;= p(\boldsymbol{X}) \\ &amp;amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | \mu_0, \sigma_0^2) d\mu \\ &amp;amp;= \int \left[ \prod_{i=1}^N \mathcal{N} (x_i | \mu, \sigma^2) \right] \mathcal{N} (\mu | m, \tau^2) d\mu \\ &amp;amp;= \frac{1}{(\sigma \sqrt{2\pi})^n (\tau \sqrt{2\pi})} \int \exp \left( - \frac{1}{2\sigma^2 \sum_i (x_i - \mu)^2 - \frac{1}{2\tau^2} (\mu-m)^2 }\right) d\mu \end{align}\] Let us now define $S^2 = \frac{1}{\sigma^2}$ and $T^2 = \frac{1}{\tau^2}$ \(\begin{align} p(\boldsymbol{X}) &amp;amp;= \frac{1}{(S^{-1}\sqrt{2\pi})^n (T^{-1} \sqrt{2\pi})} \int \exp \left[ -\frac{S^2}{2} \left( \sum_i x_i^2 + n \mu^2 - 2\mu \sum_i x_i\right) - \frac{T^2}{2} \left(\mu^2 +m^2-2m\mu\right)\right] \\ &amp;amp;= \color{purple}{\frac{1}{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}} \int \color{purple}{e^{-\frac{S^2 \sum_i x_i^2}{2}}} e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} \color{purple}{e^{-\frac{T^2}{2}m^2}} e^{T^2m\mu} d\mu \\ &amp;amp;= \underbrace{\color{purple}{\frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n(T^{-1}\sqrt{2\pi})}}}_{c} \int e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu} d\mu \\ &amp;amp;= c \int \color{violet}{e^{-\frac{S^2N\mu^2}{2}} e^{S^2\mu \sum_i x_i} e^{-\frac{T^2}{2}\mu^2} e^{T^2m\mu}} d\mu \\ &amp;amp;= c \int \color{violet}{\exp \left[ -\frac{1}{2} \left( S^2N\mu^2 - 2S^2\mu \sum_i x_i +T^2 \mu^2 -2T^2 m \mu\right) \right]} d\mu \\ &amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \underbrace{\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} }_{\alpha}\right)\right] d\mu \\ &amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha\right)\right] d\mu \\ &amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right) \left( \mu^2 - 2\mu \alpha \color{violet}{+ \alpha^2 - \alpha^2} \right) \right] d\mu \\ &amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \color{purple}{+ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 }\right] d\mu \\ &amp;amp;= c \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} d\mu \\ &amp;amp;= c \color{purple}{\exp \left[ \frac{1}{2} \left( S^2N + T^2 \right)\alpha^2 \right]} \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)(\mu-\alpha)^2 \right] d\mu \\ &amp;amp;= c \exp \left[ \frac{1}{2} \color{red}{\left( S^2N + T^2 \right) \left( \frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2} \right)^2} \right] \int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu \\ &amp;amp;= c \exp \left[ \frac{1}{2} \color{red}{\frac{(S^2\sum_i x_i +T^2 m)^2}{S^2 N + T^2}} \right] \underbrace{\color{yellow}{\int \exp \left[ -\frac{1}{2} \left( S^2N + T^2 \right)\left(\mu-\frac{S^2\sum_i x_i +T^2 m}{S^2 N + T^2}\right)^2 \right] d\mu}}_{\cdots = \frac{1.25331 erf(\frac{0.7071 -T^2m + s^2 N(\mu-\bar{x})+T^2\mu }{\sqrt{NS^2 + T^2}})}{\sqrt{NS^2 + T^2}} = \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} } \\ &amp;amp;= c \exp \left[ \frac{1}{2} \frac{(S^2 \color{brown}{\sum_i x_i} +T^2 m)^2}{S^2 N + T^2} \right] \color{yellow}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ &amp;amp;= c \exp \left[ \frac{1}{2} \frac{(S^2 \color{brown}{N \bar{x}}+T^2 m)^2}{S^2 N + T^2} \right] \frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}} \\ &amp;amp;= \frac{e^{-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2} }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[ \color{violet}{ \frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \\ &amp;amp;= \frac{1 }{(S^{-1}\sqrt{2\pi})^n \color{lime}{(T^{-1}\sqrt{2\pi})}} \exp \left[-\frac{S^2 \sum_i x_i^2}{2}-\frac{T^2}{2}m^2 \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{\sqrt{S^2N + T^2}}} \end{align}\) Now, let us work on the constant and the exponent portion: \[\color{lime}{\frac{\sqrt{2\pi}}{T^{-1}\sqrt{2\pi}\sqrt{S^2N + T^2}} = \frac{1}{\tau\sqrt{\frac{N}{\sigma^2} + \frac{1}{\tau^2}}}\frac{\sigma}{\sigma} = \frac{\sigma}{\sqrt{\sigma^2\tau^2 \left( \frac{n}{\sigma^2}+ \frac{1}{\tau^2} \right)}} = \frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} }\] \[\color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} = \frac{\left( \frac{1}{\sigma^2} n\bar{x} + \frac{1}{\tau^2} m \right)^2}{2 \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2}\right)} = \frac{\left(\frac{N \bar{x} \tau^2+ \sigma^2 m}{\tau^2 \sigma^2} \right)^2}{2 \left( \frac{\tau^2 N +\sigma^2}{\tau^2 \sigma^2} \right)} = \frac{(N\bar{x}\tau^2 + \sigma^2 m)^2}{2\tau^2 \sigma^2 (\tau^2 N + \sigma^2)}}\] \[\color{violet}{= \frac{N^2 \bar{x} \tau^4 + \sigma^4 m^2 + 2 \sigma^2 \tau^2 m N \bar{x}}{2 \sigma^2 \tau^2 (\tau^2 N + \sigma^2)} = \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)}}\] Therefore, bringing it all together, we obtain \[\begin{align} p(\boldsymbol{X}) &amp;amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{1}{2} \frac{(S^2 N\bar{x} +T^2 m)^2}{S^2 N + T^2} }\right] \color{lime}{\frac{\sqrt{2\pi}}{(T^{-1}\sqrt{2\pi})\sqrt{S^2N + T^2}}} \\ &amp;amp;= \frac{1 }{(\sigma\sqrt{2\pi})^n} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\tau^2} \right] \exp \left[ \color{violet}{\frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} }\right] \color{lime}{\frac{\sigma}{\sqrt{\tau^2N + \sigma^2}} } \\ &amp;amp;= \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\tau^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{m^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 m^2}{\tau^2} + 2mN\bar{x}}{2 (\tau^2 N + \sigma)} \right] \end{align}\] By substituting the $\tau$ and $m$ terms back again, we obtain \[\bbox[teal,4pt]{p(\boldsymbol{X}) = \frac{\sigma}{(\sigma\sqrt{2\pi})^n \sqrt{\sigma_0^2N + \sigma^2}} \exp \left[-\frac{\sum_i x_i^2}{2\sigma^2}-\frac{\mu_0^2}{2\sigma_0^2} \right] \exp \left[ \frac{\frac{N^2 \bar{x}^2 \tau^2}{\sigma^2} + \frac{\sigma^2 \mu_0^2}{\sigma_0^2} + 2 \mu_0 N\bar{x}}{2 (\sigma_0^2 N + \sigma)} \right] }\] Conclusion Bringing it all together, if one is to have a dataset $\boldsymbol{X}$ with $N$ examples, which are all univariate, the likelihood of obtaining the dataset given some mean $\mu$, is \[\bbox[teal,4pt]{p(\boldsymbol{X} | \mu) = \mathcal{N} \left(\bar{x} \bigg| \mu , \frac{\sigma^2}{N} \right) }\] The prior distribution for the mean $\mu$ has a prior mean $\mu_0$ and variance $\sigma_0^2$ being represented as \[\bbox[teal,4pt]{p(\mu) = \mathcal{N}(\mu | \mu_0, \sigma_0^2) }\] The posterior distribution for the mean given the dataset can be described as \[\bbox[teal,4pt]{p(\mu | \boldsymbol{X} ) = \mathcal{N} \left( \mu \bigg| \underbrace{\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \bar{x}}_{\mu_N}, \underbrace{\frac{\sigma^2\sigma_0^2}{N\sigma_0^2 + \sigma^2}}_{\sigma^2_N} \right) }\] And in order to identify the probability of a probe sample $x$ given the dataset $\boldsymbol{X}$ can be described as \[\bbox[teal,4pt]{p(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)}\] Finally, to obtain the marginal likelihood of the dataset, one can compute as \[\bbox[4pt,teal]{p(\boldsymbol{X}) =\frac{\sigma}{(\sqrt{2\pi}\sigma)^n \sqrt{N\sigma_0^2 + \sigma^2}} \exp \left( -\frac{\sum_i x_i}{2\sigma^2} - \frac{\mu_0^2}{2\sigma_0^2} \right) \exp \left[\frac{1}{2(N\sigma_0^2 + \sigma^2)} \cdot \left( \frac{\sigma_0^2 N^2 \bar{x}^2 }{\sigma^2} + \frac{\sigma^2\mu_0^2}{\sigma_0^2} + 2N\bar{x} \mu_0 \right) \right]}\] Sources Kevin P. Murphy “Conjugate Bayesian Analysis of the Gaussian Distribution” 3 Oct 2007 Christopher Bishop - Pattern Recognition</summary></entry><entry><title type="html">Probabilistic Linear Discriminant Analysis</title><link href="http://localhost:4000/plda_python.html" rel="alternate" type="text/html" title="Probabilistic Linear Discriminant Analysis" /><published>2021-01-30T00:00:00-05:00</published><updated>2021-01-30T00:00:00-05:00</updated><id>http://localhost:4000/plda</id><content type="html" xml:base="http://localhost:4000/plda_python.html">&lt;p&gt;I need to note that a lot of this post was inspired by RaviSoji’s &lt;a href=&quot;https://github.com/RaviSoji/plda&quot;&gt;PLDA implementation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let us say that we have a training dataset $X \in \mathbb{R}^{N\times F}$ and $y \in \mathbb{R}^N$, where $N$ is the number of examples in the whole dataset, $F$ is the number of features in the dataset, $K$ specifies the number of classes that there are, and $\boldsymbol{m} \in \mathbb{R}^F$ represents the mean vector of the entire dataset $X$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/home/data/MNIST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#.reshape(-1,28,28)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a classification problem we will create several subspaces:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{D}$ represents the space where the data lives in&lt;/li&gt;
  &lt;li&gt;$\mathcal{X}$ represents the subspace found via PCA where the data in $\mathcal{D}$ gets transformed to the subspace $\mathcal{X}$. Here, you may consider $\mathcal{X}$ to be the space of the preprocessed state&lt;/li&gt;
  &lt;li&gt;$\mathcal{U}$ represents the subspace found via PLDA, where the data will be ultimately transformed to, which is described in detail by Ioffe 2006&lt;/li&gt;
  &lt;li&gt;$\mathcal{U}_{model}$ represents the subspace within $\mathcal{U}$ which contains only the dimensions that are relevant to the problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, in general, the data will first flow in the following fashion
\(\mathcal{D} \leftrightarrow \mathcal{X} \leftrightarrow \mathcal{U} \leftrightarrow \mathcal{U}_{model}\)&lt;/p&gt;

&lt;p&gt;First, for a classification problem, we perform a PCA to reduce the dimensionality, and get rid of features that may not be very important. In other words, we will bring the data from $\mathcal{D}$ to $\mathcal{X}$. For this one needs to determine the number of components that one would want to take into account for in its subspace, and set that as the matrix rank. This can be predefined or not. If it is not defined, we compute the covariance matrices for each of the classes (i.e. a between-class covariance matrix and a within-class covariance matrix per class $k$). In other words&lt;/p&gt;

&lt;p&gt;For each class&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Compute the mean vectors $m_k \in \mathbb{R}^F$&lt;/li&gt;
  &lt;li&gt;Compute the covariance matrix $\sigma_k \in \mathbb{R}^{F\times F}$&lt;/li&gt;
  &lt;li&gt;Compute the between-class covariance matrix per class $S_{b,k} \in \mathbb{R}^{F\times F}$
 \(S_{b,k} = \frac{n_k}{N}\underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times 1}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{1\times F}}\)&lt;/li&gt;
  &lt;li&gt;Compute the within-class covariance matrix per class $S_{w,k} \in \mathbb{R}^{F\times F}$
 \(S_{w,k} = \frac{n_k-1}{N} \odot \sigma_{k}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then compute the within-class and between-class covariance matrices, $S_w \in \mathbb{R}^{F\times F}$ and $S_b \in \mathbb{R}^{F\times F}$ respectively. If one was to set \(\boldsymbol{m}_{ks} \in \mathbb{R}^{K\times F}\) as the matrix representing all of the mean vectors, $\boldsymbol{\sigma_{ks} \in \mathbb{R}^{K\times F \times F}}$ the tensor representing all of the class covariances, and $n_{ks}\in \mathbb{R}^K$ a vector representing all of the number of examples in each class, it is possible to vectorize it all as&lt;/p&gt;

\[\begin{align}
    S_b &amp;amp;= \underbrace{\frac{n_{ks}}{N} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times K}}}_{\mathbb{R}^{F\times K}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{K\times F}} \\
    S_w &amp;amp;= \sum_{k} S_{w,k} \\ 
    &amp;amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \odot \underbrace{\frac{n_k-1}{N}}_{\mathbb{R}^K} \\
    &amp;amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \cdot \underbrace{\frac{n_k-1}{N}\text{[:, None, None]}}_{\mathbb{R}^{K\times 1 \times 1}}
\end{align}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_principal_components&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# This might overfit
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_principal_components&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;matrix_rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_principal_components&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],[],[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Get only the data associated with class k
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Compute the mean, number of samples, and class covariance
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;m_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sigma_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Append them all
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;matrix_rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matrix_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matrix_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, there are going to be several transformations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{D} \rightarrow \mathcal{X}$&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Here, there are two case scenarios. If PCA was defined in order to reduce the dimensions, then the data in $\mathcal{D}$ will be transformed via PCA. Otherwise, you can return the data itself&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathcal{X} \rightarrow \mathcal{D}$&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this case, it is very similar to the converse. If PCA was defined, in order to bring it back to the original data space $\mathcal{D}$, you need to inverse transform the data. Otherwise, just return the data itself&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_D_to_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_X_to_D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inverse_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, at this point, we convert the training data from $\mathcal{D}$ to the space $\mathcal{X}$, having the data be represented as $X_{pca}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_from_D_to_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Shape of X_pca =&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Shape of X_pca = (60000, 712)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the PLDA, we use a Gaussian mixture model, where $\boldsymbol{x}$ retpresents a sample in the mixture, and $\mathcal{y}$ represents the center of a mixture component. In general, the class-conditional distributions is represented by&lt;/p&gt;

\[P(\boldsymbol{x} | \boldsymbol{y}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{y}, \Phi_w)\]

&lt;p&gt;where all of the class-conditional distributions share one common covariance 
[I DON’T KNOW WHY THEY ALL SHARE THE SAME COVARIANCE]&lt;/p&gt;

&lt;p&gt;If we recall, the &lt;strong&gt;LDA formulation&lt;/strong&gt; is the result if we were to set $\mu_k$ values to be constrainted to be in a lower dimension, and perform the likelihood maximization with respect to $\mu_k$, $\pi_k$, and $\Phi_w$, where the priord of the class variable $\boldsymbol{y}$ is set to put a probability mass on each of the points&lt;/p&gt;

\[P_{LDA}(\boldsymbol{y}) = \sum_{k=1}^{K} \pi_k \delta(\boldsymbol{y}-\mu_k)\]

&lt;p&gt;But that won’t be the case in the &lt;strong&gt;PLDA formulation&lt;/strong&gt;. Instead, PLDA sets it so taht the prior is not to be within a discrete set of values, but instead, sampled from a Gaussian prior.&lt;/p&gt;

\[P_{PLDA}(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{m}, \Phi_b)\]

&lt;p&gt;Note that this normal distribution $P_{PLDA}(y)$ uses the mean of the full dataset. This formulation makes it such that $\Phi_w$ is positive definite, and $\Phi_b$ is positive semi-definite. Theoretically, it is possible to find a transformation $V$ which can simultaneously diagonalize $\Phi_b$ and $\Phi_w$&lt;/p&gt;

\[\begin{align}
    V^T \Phi_b V &amp;amp;= \Psi \\
    V^T \Phi_w V &amp;amp;= I
\end{align}\]

&lt;p&gt;We can define $A = V^{-T} = \text{inv}(V^T)$, resulting in&lt;/p&gt;

\[\begin{align}
    \Phi_w &amp;amp;= AA^T \\
    \Phi_b &amp;amp;= A\Psi A^T
\end{align}\]

&lt;p&gt;Thus, the &lt;strong&gt;PLDA model&lt;/strong&gt; is defined as:&lt;/p&gt;

\[\bbox[teal, 4pt]{\begin{align}
\boldsymbol{x} &amp;amp;= \boldsymbol{m} + A \boldsymbol{u} \quad \text{where} \\
&amp;amp; \boldsymbol{u} \sim \mathcal{N}(\cdot | \boldsymbol{v}, I) \\
&amp;amp; \boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)
\end{align}}\]

&lt;p&gt;Here, $\boldsymbol{u}$ represents the sample data representation of $\boldsymbol{x}$, but projected in the lated projected space $\mathcal{U}$, and $\boldsymbol{v}$ represents the sample label in the lated projected space. These transformations can be computed via&lt;/p&gt;

\[\boldsymbol{x} = \boldsymbol{m} + A \boldsymbol{u} \quad \leftrightarrow \quad \boldsymbol{u} = V^T (\boldsymbol{x} - \boldsymbol{m})\]

\[\boldsymbol{y} = \boldsymbol{m} + A \boldsymbol{v} \quad \leftrightarrow \quad \boldsymbol{v} = V^T (\boldsymbol{y} - \boldsymbol{m})\]

&lt;p&gt;And from this point on, we determine the optimal $\boldsymbol{m}$, $A$, and $\Psi$ are.&lt;/p&gt;

&lt;p&gt;S. Ioffe: “&lt;em&gt;In the training data, the grouping of examples into clusters is given, and we learn the model parameters by maximizing the likelihood. If, instead, the model parameters are fixed, likelihood maximization with respect to the class assignment labels solves a clustering problem&lt;/em&gt;”&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_X_to_U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_U_to_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But, at this point, we don’t know what the parameters $A$ or $\Psi$ are, so we can’t use these functions yet.&lt;/p&gt;

&lt;h3 id=&quot;learning-the-model-parameters-boldsymbolm-psi-a&quot;&gt;Learning the Model Parameters ($\boldsymbol{m}, \Psi, A$)&lt;/h3&gt;
&lt;p&gt;The loading matrix $A$ is essentially finding the variances $\Phi_b$ and $\Phi_w$, and all of the parameters can be defined using a maximum likelihood framework. Let us say that $D_k$ represents the dataset which contains only samples from the $k^{th}$ class, and $\boldsymbol{x}_k^i$ represents the $i^{th}$ sample from $D_k$, and it belongs to the $k^{th}$ class. Given $N$ training examples separated into $K$ classes, and assuming that they are all independently drawm from their respective class, the log likelihood is&lt;/p&gt;

\[l(\boldsymbol{x}^{1 \cdots N}) = \sum_{k=1}^K \ln P(\boldsymbol{x}^i : i \in D_k) = \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k)\]

&lt;p&gt;where the joint probability distribution of a set of $n$ patterns (assuming all these $n$ patterns belong to the same class $k$) is:&lt;/p&gt;

\[\begin{align}
P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) &amp;amp;= \int \color{red}{P(\boldsymbol{x}^1_k | \boldsymbol{y})} \cdots \color{cyan}{P(\boldsymbol{x}^n_k | \boldsymbol{y})} \color{magenta}{P(\boldsymbol{y})} d\boldsymbol{y} \\
&amp;amp;= \int \color{red}{\mathcal{N}(\boldsymbol{x}^1_k | \boldsymbol{y}, \Phi_w)} \cdots \color{cyan}{\mathcal{N}(\boldsymbol{x}^n_k | \boldsymbol{y}, \Phi_w)} \color{magenta}{\mathcal{N}(\boldsymbol{y} | 0, \Phi_b)} d\boldsymbol{y}
\end{align}\]

&lt;p&gt;By computing the integral, we obtain&lt;/p&gt;

\[\color{red}{MAGIC}\]

\[\ln P(\boldsymbol{x}^1_k,\cdots, \boldsymbol{x}^n_k) = C - \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} (\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T\right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} ( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)\right)\right]\]

&lt;p&gt;where \(\bar{\boldsymbol{x}}_k = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}^i_k\), and $C$ is a constant that can be ignored&lt;/p&gt;

&lt;p&gt;At this point, as a “hack”, it sets the number of examples for each class to be $n$. In other words, every class ends up having exactly $n$ examples to learn from. Now, if one were to maximize the equation $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\boldsymbol{m}$, one would obtain $\boldsymbol{m}^* = \frac{1}{N} \sum_i \boldsymbol{x}^i$. If one substitutes it back, one would get&lt;/p&gt;

\[\begin{align}
l(\boldsymbol{x}^{1\cdots N})
&amp;amp;= \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) \\
&amp;amp;= -\sum_{k=1}^K \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} \color{cyan}{(\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T}  \right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} \color{red}{( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)} \right)  \right] \\
&amp;amp;= \cdots \\
&amp;amp;= \color{red}{\text{MAGIC}} \\
&amp;amp;= \cdots \\
&amp;amp;= - \frac{c}{2} \left[ \ln |\Phi_b + \frac{\Phi_w}{n} | + \text{tr} \left( (\Phi_b + \frac{\Phi_w}{n})^{-1} \color{cyan}{S_b} \right) + (n-1) \ln |\Phi_w | + n \text{tr} (\Phi_w^{-1} \color{red}{S_w}) \right]
\end{align}\]

&lt;p&gt;Now, we need to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Phi_b$ and $\Phi_w$ subject to $\Phi_w$ being p.d. and $\Phi_b$ being p.s.d. &lt;strong&gt;Without&lt;/strong&gt; these constraints, we would obtain&lt;/p&gt;

\[\Phi_w = \frac{n}{n-1} S_w \quad \text{and} \quad \Phi_b = S_b - \frac{1}{n-1} S_w\]

&lt;p&gt;Therefore, if $S_w$ and $S_b$ are diagonal, then the covariances $\Phi_w$ and $\Phi_b$ will also be diagonal, and the diagonalization property holds as long as the contraints above are satisfied.&lt;/p&gt;

&lt;p&gt;As we have previously stated, we know that&lt;/p&gt;

\[\Phi_b = A \Psi A^T\]

&lt;p&gt;If you fix $\Psi$ and maximize $l(\boldsymbol{x}^{1\cdots N})$ via unconstrained optimization with respect to $A^{-1}$ will make $A^{-1}S_b A^{-T}$ and $A^{-1}S_w A^{-T}$, making the $A^{-T}$ to be the solution of the generalized eigenvector problem involving $S_b$ and $S_w$, where $S_b \boldsymbol{v} = \lambda S_w \boldsymbol{v}$. Then, the projection of the data to the latent space with the LDA projection. [REVIEW]&lt;/p&gt;

&lt;p&gt;Then, if you were to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Psi$ subject to $\Psi \geq 0$ and $\text{rank}(\Psi) \leq \hat{F}$, then we’ll get the method to optimize the model [REVIEW]&lt;/p&gt;

&lt;p&gt;Ioffe 2006: “&lt;em&gt;Our method was derived for the case where each class in the training data is represented by the same number $n$ of examples. This may not be true in practice, in which case, we can resample the data to make the number of examples the same, use EM (as shown in section 5), or use approximations. We took the latter approach, using the closed-form solution in Fig. 2, where $n$ was taken to be the average number of examples per class&lt;/em&gt;”&lt;/p&gt;

&lt;h2 id=&quot;algorithm-plda-optimization&quot;&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;: PLDA Optimization&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Input&lt;/em&gt;:&lt;/strong&gt; Training $N$ examples from $K$ classes, with $n = N/K$ per class&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Output&lt;/em&gt;:&lt;/strong&gt; Parameters $\boldsymbol{m}, A, \Psi$, maximizing the likelihood of the PLDA model&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Compute the covariance matrices $S_b$, and $S_w$&lt;/li&gt;
    &lt;li&gt;Compute the transformation matrix $W$ such that $S_b \boldsymbol{w} = \lambda S_w \boldsymbol{w}$ (i.e. $eig(S_w^{-1}S_b)$)&lt;/li&gt;
    &lt;li&gt;Compute the covariance matrices in the latent space
      &lt;ul&gt;
        &lt;li&gt;$\Lambda_b = W^T S_b W$&lt;/li&gt;
        &lt;li&gt;$\Lambda_w = W^T S_w W$&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Determine the following parameters
      &lt;ul&gt;
        &lt;li&gt;$\boldsymbol{m} = \frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$&lt;/li&gt;
        &lt;li&gt;$A = W^{-T} \left( \frac{n}{n-1} \Lambda_w \right)^{1/2}$&lt;/li&gt;
        &lt;li&gt;$\Psi = \max \left( 0, \frac{n-1}{n} (\Lambda_b / \Lambda_w) - \frac{1}{n} \right)$&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Reduce the dimensionality to $\hat{F}$ by keeping the largest elements of $\Psi$, while setting the rest to zero.&lt;/li&gt;
    &lt;li&gt;In the latent space $\boldsymbol{u} = A^{-1}(\boldsymbol{x}-\boldsymbol{m})$, only the features for non-zero entries are needed for recognition&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Scipy’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eigh(A,B)&lt;/code&gt; function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg&lt;/code&gt; solves the generalized eigenvalue problem for a complex Hermittian or a real symmetric matrix, so that $A\boldsymbol{v} = \lambda B \boldsymbol{v}$. Otherwise, if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is ommited, then it is assumed that $B=I$&lt;br /&gt;
Also note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scipy.linalg.eigh != np.linalg.eigh&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_Sb_Sw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],[],[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Get only the data associated with class k
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Compute the mean, number of samples, and class covariance
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;m_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sigma_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Append them all
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma_ks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_Sb_Sw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Compute W
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigvals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvecs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvecs&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Compute Lambdas
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lambda_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Lambda_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Compute A
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lambda_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Compute Psi
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag_Lambda_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lambda_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diag_Lambda_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lambda_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag_Lambda_b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag_Lambda_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this point you can transform the data that is in the PCA subspace $\mathcal{X}$ to the $\mathcal{U}$ subspace, having the data be represented as $U$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_from_X_to_U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, again, not every dimension will be relevant in the $\mathcal{U}$ subspace, and that is why we reduce the $\mathcal{U}$ to \(\mathcal{U}_{model}\) , which only contains the relevant dimensions of $\mathcal{U}$. Therefore, in order to go back and forth in between the two subspaces, simply drop them the irrelevant dimensions or add the relevant dimensions to a zero matrix. This new data will be represented as $U_{model}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_U_to_Umodel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_model&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform_from_Umodel_to_U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Compute the relevant dimensions of Psi
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argwhere&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;U_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_from_U_to_Umodel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Great! Now we we have the data in the $\mathcal{U}_{model}$ space. Now, all that there is left is to understand how to perform inference, and in order to do that, one needs to find the prior parameters for $\boldsymbol{v}$, the posterior parameters, and the posterior predictive parameters.  Let’s discuss now how to find the probability parameters.&lt;/p&gt;

&lt;h2 id=&quot;inference-on-the-latent-space&quot;&gt;Inference on the Latent Space&lt;/h2&gt;

&lt;p&gt;First, if you need some review on Bayesian inference for Gaussian distributions, you may check the other &lt;a href=&quot;https://nicolasshu.com/bayesian_inference_for_gaussian.html&quot;&gt;post&lt;/a&gt; to understand priors, posteriors, posterior predictives, and marginal probability. For this problem, note that the different dimensions have been decorrelated (i.e. the covariances have been decorrelated), thus the different dimensions could be treated as univariate problems.&lt;/p&gt;

&lt;p&gt;We need to determing the prior parameters of $\boldsymbol{v}$, which leads to the probability distribution $P(\boldsymbol{v})$, the posterior parameters, which leads to \(P(\boldsymbol{v} \vert \boldsymbol{u})\) , and the posterior predictive parameters, for \(P(\boldsymbol{u}^p \vert \boldsymbol{u}^g_{1\cdots n})\)&lt;/p&gt;

&lt;p&gt;The easiest to determine right off the bat are the prior parameters. For the prior parameters, as one may recall in the model formulation&lt;/p&gt;

\[\boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)\]

&lt;p&gt;which, in turn, are simple to compute. Here we’ll call $\mu_{prior}$ the prior mean, and $\Sigma_{prior}$ the prior covariance, making&lt;/p&gt;

\[\bbox[teal, 4pt]{\begin{align}
\mu^{prior} &amp;amp;= \boldsymbol{0} \\
\Sigma^{prior} &amp;amp;= \Psi_{\forall d \in D}
\end{align}}\]

&lt;p&gt;where $D$ represents all of the relevant dimensions, which are all that the variances are not zero. Then from this point, we’ll use the notation setting $\hat{\Psi} = \Psi_{\forall d \in D}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prior_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;mean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Psi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relevant_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, the more involved ones are the posteriors parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One advantage that PLDA has is that it allows one to make inferences about classes not present during training.&lt;/strong&gt; Let us consider the following case of classification first. We are given a set of data to learn from, which Ioffe refers to as a “gallery”. This set ${ \boldsymbol{x}^1, \cdots,\boldsymbol{x}^k, \cdots, \boldsymbol{x}^K  }$ contains $K$ examples, with one example from each of the $K$ classes. We are also given a probe example $\boldsymbol{x}^p$, and assume that it belongs to one of the $K$ classes. If we are to determine to which class it belongs, maximizing the likelihood will do the job. This can be more easily accomplised in the lated space by performing the trnasformation $\boldsymbol{u} = A^{-1}(\boldsymbol{x} - \boldsymbol{m})$, since it will decorrelate the data. For this example, $\boldsymbol{x}^p$ will be transformed to $\boldsymbol{u}^p$&lt;/p&gt;

&lt;h3 id=&quot;single-training-example-per-class&quot;&gt;Single Training Example per Class&lt;/h3&gt;
&lt;p&gt;Let us consider an example $\boldsymbol{u}^g$ from the training set (i.e. gallery), where, again, it belongs to some class between $1 \cdots K$ The probability that the probe $\boldsymbol{u}^p$ belongs to the same class as $\boldsymbol{u}^g$ is defined by the probability $P(\boldsymbol{u}^p | \boldsymbol{u}^g)$.&lt;/p&gt;

&lt;p&gt;So, the posterior probability will provide a way to perform inference on the class variable $\boldsymbol{v}$ (i.e. the transformed version of $\boldsymbol{y}$). As we may remember, the parameters for a posterior Gaussian are&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

&lt;p&gt;Now, since we only have a single sample $\bar{x} = \boldsymbol{u}$. Additionally, we know that the mean for the prior $\mu^{prior}$ is all zeros (since the data has been centralized), then&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

&lt;p&gt;Additionally, since we would be looking at a single class at this point and the covariances have been diagonalized, the within-class covariance is an identity matrix, making the $(\sigma^{prior})^2 = 1$. Therefore the parameters turn into&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;amp;= \frac{1(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1}
\end{align}\]

&lt;p&gt;And, once again, since we are dealing with a single sample, $N=1$&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1}
\end{align}\]

&lt;p&gt;The posterior can then be defined as&lt;/p&gt;

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}) = \mathcal{N}\left(\boldsymbol{v} \bigg| \frac{\hat{\Psi}}{\hat{\Psi} + I}\boldsymbol{u}, \frac{\hat{\Psi}}{\hat{\Psi} + I}\right)}\]

&lt;p&gt;Now, if we see how this flows, the class variable $\boldsymbol{v}$ will be used to determine $\boldsymbol{u}$ examples, which are then used to determine the data $\boldsymbol{x}$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;graphviz&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Digraph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Digraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;v&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;u1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;u2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'v1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;v2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/lda_plda/v_u_x.png&quot; alt=&quot;drawing&quot; align=&quot;middle&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From probabilistic graphical models, if we observe $\boldsymbol{v}$ (i.e. $\boldsymbol{v}$ is given), then $\boldsymbol{u}^p$ and $\boldsymbol{u}^g$ are conditionally independent. We also know that the posterior predictive probability is&lt;/p&gt;

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu^{post}, (\sigma^{post})^2 + \sigma^2)\]

&lt;p&gt;And since the within class variance has been diagonalized to an identity matrix ($\sigma^2 = 1$), we’ll obtain&lt;/p&gt;

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}^g) = \mathcal{N}\left(\boldsymbol{u}^p \bigg| \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \boldsymbol{u}^g, I + \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \right)}\]

&lt;p&gt;In order to classify $\boldsymbol{u}^p$, then we compute $P(\boldsymbol{u}^p \vert \boldsymbol{u}^g) \forall g \in {1,\cdots,M }$, and pick the maximum.&lt;/p&gt;

&lt;p&gt;Ioffe 2006: “&lt;em&gt;With PLDA, we were able to combine the knowledge about the general structure of the data, obtained during training, and the examples of new classes, yielding a principled way to perform classification&lt;/em&gt;”&lt;/p&gt;

&lt;h3 id=&quot;multiple-training-examples-per-class&quot;&gt;Multiple Training Examples Per Class&lt;/h3&gt;
&lt;p&gt;We can improve the recognition performance by using more examples. Let us say that we have $n_k$ examples from class $k$, making&lt;/p&gt;

\[n_k = |U_{model,k}|\]

&lt;p&gt;These examples are all independent examples $\boldsymbol{u}_{1\cdots n}^g$. Just as before, we know that, here, we are looking at a single class $k$. We have previously diagonalized all of the covariance matrices, making all of the dimensions (i.e. features) decorrelated, thus they can be worked on individually as if they were univariate features. This also means that the within-class covariance is an identity matrix, making $\sigma^2 = 1$. As we may remember, the parameters for a posterior Gaussian are&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

&lt;p&gt;Again, since our model estimates that the prior mean $\mu^{prior}$ is all zeros, then&lt;/p&gt;

\[\begin{align}
\mu^{post} &amp;amp;=  \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

&lt;p&gt;Since we’re looking at the class $k$, then $N=n_k$ and $\bar{x} = \bar{\boldsymbol{u}}_k$. Thus the posterior for multiple samples is&lt;/p&gt;

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}_k^{1\cdots n_k} ) = \mathcal{N}\left(\boldsymbol{v} \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi}+I}\bar{\boldsymbol{u}}_k, \dfrac{\hat{\Psi}}{n_k \hat{\Psi}+I} \right) }\]

&lt;p&gt;Therefore, in order to compute the posterior parameters, where $\mu_k^{post}$ and $\Sigma_k^{post}$ are the mean posterior and the covariance posterior for each class $k$, we have&lt;/p&gt;

\[\bbox[teal, 4pt]{\begin{align}
\Sigma^{post}_k &amp;amp;= \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = \Sigma^{prior} \odot \frac{1}{1 + n_k \cdot \Sigma^{prior}} \\
\mu_k^{post} &amp;amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \underbrace{\left( \sum_{\boldsymbol{u} \in U_{model,k}} \boldsymbol{u} \right)}_{n_k \bar{\boldsymbol{u}}_k} \cdot \Sigma^{post}_k 
\end{align}}\]

&lt;p&gt;where we can recall that $\Sigma^{prior} = \hat{\Psi}$, and $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u_model_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_model_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_model_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As per page 535 by Ioffe 2006, if one sets multiple examples of a class to a single model, assuming we have $n_k$ independent examples \(\{ \boldsymbol{u}_k^i \}_{i=1}^{n_k}\), then the probability of obtaining a sample $\boldsymbol{u}^p$, given the set above, can be obtained from the posterior predictive&lt;/p&gt;

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)\]

&lt;p&gt;Once again, since the within-class covariance has been diagonalized to an identity matrix, then $\sigma^2 = 1$&lt;/p&gt;

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}_k^{1\cdots n_k}) = P(\boldsymbol{u}^p | \boldsymbol{u}_k^1, \cdots, \boldsymbol{u}_k^{n_k}) = \mathcal{N} \left( \boldsymbol{u}^p \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k, I + \dfrac{\hat{\Psi}}{n_k \hat{\Psi} + I} \right) }\]

&lt;p&gt;where $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$&lt;/p&gt;

&lt;p&gt;This means that, you may compute the predictive parameters as copying the posterior parameters ($\mu_k^{postpred}$, $\Sigma_k^{postpred}$), and adding an identity matrix to the covariance&lt;/p&gt;

\[\bbox[teal, 4pt]{\begin{align}
\mu_k^{postpred} &amp;amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \mu_k^{post} \\
\Sigma_k^{postpred} &amp;amp;= I + \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = I + \Sigma_k^{post}
\end{align}}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;post_pred_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post_pred_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Great! This is all that we need to precompute before performing any inference! At this point, you have determined all of the parameters which describe you data. As a review, in order to prepare this algorithm, we have received a training dataset $X$ and its corresponding labels $y$. Then, we have decided on a certain number of components to be used for a PCA. This PCA will bring the data from the space $\mathcal{D}$ to the space $\mathcal{X}$, obtaining the data $X_{pca}$. Then we found the parameters $\boldsymbol{m}$, $A$, and $\hat{\Psi}$, which optimize the PLDA formulation. These would then allow us to bring the data from the $\mathcal{X}$ subspace to the $\mathcal{U}$ latent space, obtaining the data $U$. Then, we reduced the dimensions of $U$ by discarding any dimensions which had a zero variance in $\hat{\Psi}$, yielding the data $U_{model}$. Finally, using these same parameters, we obtain the prior parameters ($\mu^{prior}$ and $\Sigma^{prior}$), the posterior parameters ($\mu^{post}_k$ and $\Sigma^{post}_k$), and the posterior predictive parameters ($\mu^{postpred}_k$ and $\Sigma^{postpred}_k$)&lt;/p&gt;

&lt;p&gt;Now, we’re ready to do the inference on the latent space&lt;/p&gt;

&lt;p&gt;We have previously established that we have the classes (i.e. categories) $1,\cdots, K$. What we will do is to iterate through each of the possible classes, and compute its posterior probabilities by using the parameters computed for the posterior predictive probabilities and creating a Gaussian distribution. Finally, we can obtain the probability on that Gaussian at each of those locations. We can also use the log probability for each of the samples on each of the classes&lt;/p&gt;

\[\bbox[teal, 4pt]{\begin{align}
P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) &amp;amp;= \mathcal{N}(\boldsymbol{u}_{model} | \mu_k^{postpred}, \Sigma_k^{postpred}) \\ 
\boldsymbol{y}^* &amp;amp;= \arg\max_{k} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k)
\end{align}}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post_pred_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_probs_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logpdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_probs_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, you may choose to normalize the probabilities&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logsumexp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logsumexp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_prob_post&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;categories&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post_pred_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;True: {} | Pred: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you wanted to extract the LDA features, you could simply use the transformation functions to convert the data from some space to another space.&lt;/p&gt;

&lt;p&gt;So this is the case when we have a classification problem where the probes are assumed to belong to one of the trained classes. Now, let us look at the case where the probes belong to classes not yet seen.&lt;/p&gt;

&lt;h2 id=&quot;hypothesis-testing&quot;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;Here, we try to determine whether two samples belong to the same class or not. For that, we can compute the following likelihoods&lt;/p&gt;

\[\begin{align}
P(\boldsymbol{u}^p)P(\boldsymbol{u}^g) &amp;amp;= \text{likelihood of examples belonging to different classes} \\
P(\boldsymbol{u}^p, \boldsymbol{u}^g) &amp;amp;= \int P(\boldsymbol{u}^p | \boldsymbol{v}) P(\boldsymbol{u}^g|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ 
&amp;amp;= \text{likelihood of examples belonging to the same class}
\end{align}\]

&lt;p&gt;As a generalized formulation where there are multiple examples, the likelihood ratio is&lt;/p&gt;

\[\begin{align}
R(\{\boldsymbol{u}^{1\cdots m}_p\},\{\boldsymbol{u}^{1\cdots n}_g\}) &amp;amp;= \frac{\text{likelihood(same)}}{\text{likelihood(diff)}} = \frac{P(\boldsymbol{u}^{1\cdots m}_p,\boldsymbol{u}^{1\cdots n}_g)}{P(\boldsymbol{u}^{1\cdots m}_p)P(\boldsymbol{u}^{1\cdots n}_g)} \\
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;amp;= \int P(\boldsymbol{u}^1 | \boldsymbol{v}) \cdots P(\boldsymbol{u}^n|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\
&amp;amp;= \prod_{t=1}^d \frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} \exp \left( - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} - \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} \right)
\end{align}\]

&lt;p&gt;where&lt;/p&gt;

\[\bar{u}_t = \frac{1}{n}\sum_{i=1}^n u_t^i\]

&lt;p&gt;For the priors $\pi_{\text{same}}$ and $\pi_{\text{diff}}$, the probability that all of the examples are in the same class is&lt;/p&gt;

\[\left(1 + \dfrac{\pi_{\text{diff}} / \pi_{\text{same}} }{R} \right)^{-1} = \dfrac{R}{R+\pi_{\text{diff}} / \pi_{\text{same}}}\]

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;
If $R &amp;gt; \pi_{\text{diff}} / \pi_{\text{same}}$, the two groups of examples belong to the same class; otherwise they do not.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The between-class feature variances $\psi_t$ indicate how discriminative the features are. For example, if $\psi=0$, then it is a completely non-discriminative feature.&lt;/p&gt;

&lt;p&gt;Therefore, we can compute the marginal likelihoods for each of those possibilities:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Marginal probability of them being from same class, i.e. $P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n})$
    &lt;blockquote&gt;
      &lt;p&gt;Note that, for the marginal probability that they are from the same class, we will are treating it as a single marginal probability
\(P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n}) =P(\underbrace{\boldsymbol{u}_p^1, \cdots, \boldsymbol{u}_p^m}_{\boldsymbol{u}_p^{1\cdots m}},\underbrace{\boldsymbol{u}_g^1, \cdots, \boldsymbol{u}_g^n}_{\boldsymbol{u}_g^{1\cdots n}})\)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Marginal probability of them being from different classes, i.e. $P(\boldsymbol{u}_p^{1\cdots m}) \cdot P(\boldsymbol{u}_g^{1\cdots n})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By taking the logarithmic, we can can more easily deal with infinitesimal probabilities and helps us make multiplications into additions. Therefore, let us consider&lt;/p&gt;

\[\begin{align}
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;amp;= \prod_{t=1}^d \underbrace{\frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} }_{C} \exp \left( \underbrace{- \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})}}_{E_1} \underbrace{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }_{E_2} \right) \\
&amp;amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
\log (P(\boldsymbol{u}^{1\cdots n})) &amp;amp;= \sum_{t=1}^d \color{red}{\log (C)} +\color{cyan}{ \log(e^{E_1})} + \color{magenta}{\log(e^{E_2})} \\
&amp;amp;= \sum_{t=1}^d \color{red}{\log (C)} + \color{cyan}{E_1} + \color{magenta}{E_2} \\
&amp;amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(\psi_t + \frac{1}{n} \right)} \color{cyan}{ - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} } \\
&amp;amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(n\psi_t + 1 \right) + \frac{1}{2} \log(n)} \color{cyan}{ - \frac{n \bar{u}_t^2}{2(n\psi_t + 1)} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }
\end{align}\]

&lt;blockquote&gt;
  &lt;p&gt;It is ideal to work in the logarithmic space. With some magic, we get&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
\[\color{red}{\text{Can't get my math to align with this}}\]
&lt;/blockquote&gt;

&lt;blockquote&gt;
\[\begin{align}
P(\boldsymbol{u}^{1\cdots n}) &amp;amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
\log (C) &amp;amp;= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n\Sigma^{prior} + I) \\
\log (e^{E_1}) &amp;amp;= E_1 = \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2 (n\Sigma^{prior} + I)} \\
\log (e^{E_2}) &amp;amp;= E_2 = -\frac{1}{2} \sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}^2 \\
\log(P(\boldsymbol{u}^{1\cdots n})) &amp;amp;= \sum_{t=1}^d \log(C) + E_1 + E_2 \\ 
\end{align}\]
&lt;/blockquote&gt;

&lt;p&gt;Now, note that because the data in $U_{model}$ has been normalized $mean(U_{model}) = \boldsymbol{0}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;marginal_logprob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_prior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cov&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_prior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;E1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_prior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_prior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;E2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logP_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E2&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;logP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logP_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logP&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This way, if we were to receive two sets of data&lt;/p&gt;

\[\begin{align}
\boldsymbol{u}_p &amp;amp;\in \mathbb{R}^{m\times \hat{F}} \\
\boldsymbol{u}_g &amp;amp;\in \mathbb{R}^{n\times \hat{F}}
\end{align}\]

&lt;p&gt;We can set a set by concatenating both of them
\(\boldsymbol{u}_{pg} \in \mathbb{R}^{(m+n) \times \hat{F}}\)&lt;/p&gt;

&lt;p&gt;And then we can pass them through the computation above&lt;/p&gt;

\[\begin{align}
log(P(\boldsymbol{u}_p^{1\cdots m}) &amp;amp;= \text{log likelihood for probe set} \\ 
log(P(\boldsymbol{u}_g^{1\cdots n}) &amp;amp;= \text{log likelihood for gallery set} \\ 
log(P(\boldsymbol{u}_{pg}^{1\cdots m+n}) &amp;amp;= \text{log likelihood for combined set}\\ 
\end{align}\]

&lt;p&gt;Finally, instead of the ratio, we compute the log of the ratio&lt;/p&gt;

\[\log(R) = log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] - [log(P(\boldsymbol{u}_p^{1\cdots m}) + log(P(\boldsymbol{u}_g^{1\cdots n})]\]

&lt;p&gt;In such case, since we are dealing with log ratios, negative values mean that the model believes the two datapoints are from different categories, where as positive values indicate that the model believes that the two data points are from the same category.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;logprob_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marginal_logprob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logprob_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marginal_logprob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logprob_pg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marginal_logprob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;log_Ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logprob_pg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logprob_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logprob_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_Ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Belong to the same class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Belong to different classes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><category term="Machine Learning" /><summary type="html">I need to note that a lot of this post was inspired by RaviSoji’s PLDA implementation Let us say that we have a training dataset $X \in \mathbb{R}^{N\times F}$ and $y \in \mathbb{R}^N$, where $N$ is the number of examples in the whole dataset, $F$ is the number of features in the dataset, $K$ specifies the number of classes that there are, and $\boldsymbol{m} \in \mathbb{R}^F$ represents the mean vector of the entire dataset $X$. from mnist import MNIST from sklearn.decomposition import PCA import scipy ds = MNIST(&quot;/home/data/MNIST&quot;) ds.gz = True images, labels = ds.load_training() X = np.array(images)#.reshape(-1,28,28) y = np.array(labels) K = len(np.unique(y)) F = len(X[0]) m = np.mean(X,axis=0) For a classification problem we will create several subspaces: $\mathcal{D}$ represents the space where the data lives in $\mathcal{X}$ represents the subspace found via PCA where the data in $\mathcal{D}$ gets transformed to the subspace $\mathcal{X}$. Here, you may consider $\mathcal{X}$ to be the space of the preprocessed state $\mathcal{U}$ represents the subspace found via PLDA, where the data will be ultimately transformed to, which is described in detail by Ioffe 2006 $\mathcal{U}_{model}$ represents the subspace within $\mathcal{U}$ which contains only the dimensions that are relevant to the problem So, in general, the data will first flow in the following fashion \(\mathcal{D} \leftrightarrow \mathcal{X} \leftrightarrow \mathcal{U} \leftrightarrow \mathcal{U}_{model}\) First, for a classification problem, we perform a PCA to reduce the dimensionality, and get rid of features that may not be very important. In other words, we will bring the data from $\mathcal{D}$ to $\mathcal{X}$. For this one needs to determine the number of components that one would want to take into account for in its subspace, and set that as the matrix rank. This can be predefined or not. If it is not defined, we compute the covariance matrices for each of the classes (i.e. a between-class covariance matrix and a within-class covariance matrix per class $k$). In other words For each class Compute the mean vectors $m_k \in \mathbb{R}^F$ Compute the covariance matrix $\sigma_k \in \mathbb{R}^{F\times F}$ Compute the between-class covariance matrix per class $S_{b,k} \in \mathbb{R}^{F\times F}$ \(S_{b,k} = \frac{n_k}{N}\underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times 1}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{1\times F}}\) Compute the within-class covariance matrix per class $S_{w,k} \in \mathbb{R}^{F\times F}$ \(S_{w,k} = \frac{n_k-1}{N} \odot \sigma_{k}\) Then compute the within-class and between-class covariance matrices, $S_w \in \mathbb{R}^{F\times F}$ and $S_b \in \mathbb{R}^{F\times F}$ respectively. If one was to set \(\boldsymbol{m}_{ks} \in \mathbb{R}^{K\times F}\) as the matrix representing all of the mean vectors, $\boldsymbol{\sigma_{ks} \in \mathbb{R}^{K\times F \times F}}$ the tensor representing all of the class covariances, and $n_{ks}\in \mathbb{R}^K$ a vector representing all of the number of examples in each class, it is possible to vectorize it all as \[\begin{align} S_b &amp;amp;= \underbrace{\frac{n_{ks}}{N} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times K}}}_{\mathbb{R}^{F\times K}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{K\times F}} \\ S_w &amp;amp;= \sum_{k} S_{w,k} \\ &amp;amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \odot \underbrace{\frac{n_k-1}{N}}_{\mathbb{R}^K} \\ &amp;amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \cdot \underbrace{\frac{n_k-1}{N}\text{[:, None, None]}}_{\mathbb{R}^{K\times 1 \times 1}} \end{align}\] n_principal_components = None # This might overfit if n_principal_components: matrix_rank = n_principal_components else: m_ks, sigma_ks, n_ks = [],[],[] for k in range(K): # Get only the data associated with class k X_k = X[y==k] # Compute the mean, number of samples, and class covariance m_k = np.mean(X_k,axis=0) n_k = len(X_k) sigma_k = np.cov(X_k.T) # Append them all m_ks.append(m_k) n_ks.append(n_k) sigma_ks.append(sigma_k) m_ks = np.array(m_ks) n_ks = np.array(n_ks) sigma_ks = np.array(sigma_ks) assert m_ks.shape == (K,F) assert n_ks.shape == (K,) assert sigma_ks.shape == (K,F,F) S_b = ((m_ks - m).T * n_ks/N) @ (m_ks - m) S_w = np.sum(sigma_ks * ((n_ks-1)/N).reshape(-1,1,1), axis=0) matrix_rank = np.linalg.matrix_rank(S_w) if F != matrix_rank: pca = PCA(n_components = matrix_rank) pca.fit(X) Now, there are going to be several transformations: $\mathcal{D} \rightarrow \mathcal{X}$ Here, there are two case scenarios. If PCA was defined in order to reduce the dimensions, then the data in $\mathcal{D}$ will be transformed via PCA. Otherwise, you can return the data itself $\mathcal{X} \rightarrow \mathcal{D}$ In this case, it is very similar to the converse. If PCA was defined, in order to bring it back to the original data space $\mathcal{D}$, you need to inverse transform the data. Otherwise, just return the data itself def transform_from_D_to_X(x): global pca if pca: return pca.transform(x) else: return x def transform_from_X_to_D(x): global pca if pca: return pca.inverse_transform(x) else: return x So, at this point, we convert the training data from $\mathcal{D}$ to the space $\mathcal{X}$, having the data be represented as $X_{pca}$ X_pca = transform_from_D_to_X(X) print(&quot;Shape of X_pca =&quot;,X_pca.shape) # Shape of X_pca = (60000, 712) In the PLDA, we use a Gaussian mixture model, where $\boldsymbol{x}$ retpresents a sample in the mixture, and $\mathcal{y}$ represents the center of a mixture component. In general, the class-conditional distributions is represented by \[P(\boldsymbol{x} | \boldsymbol{y}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{y}, \Phi_w)\] where all of the class-conditional distributions share one common covariance [I DON’T KNOW WHY THEY ALL SHARE THE SAME COVARIANCE] If we recall, the LDA formulation is the result if we were to set $\mu_k$ values to be constrainted to be in a lower dimension, and perform the likelihood maximization with respect to $\mu_k$, $\pi_k$, and $\Phi_w$, where the priord of the class variable $\boldsymbol{y}$ is set to put a probability mass on each of the points \[P_{LDA}(\boldsymbol{y}) = \sum_{k=1}^{K} \pi_k \delta(\boldsymbol{y}-\mu_k)\] But that won’t be the case in the PLDA formulation. Instead, PLDA sets it so taht the prior is not to be within a discrete set of values, but instead, sampled from a Gaussian prior. \[P_{PLDA}(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{m}, \Phi_b)\] Note that this normal distribution $P_{PLDA}(y)$ uses the mean of the full dataset. This formulation makes it such that $\Phi_w$ is positive definite, and $\Phi_b$ is positive semi-definite. Theoretically, it is possible to find a transformation $V$ which can simultaneously diagonalize $\Phi_b$ and $\Phi_w$ \[\begin{align} V^T \Phi_b V &amp;amp;= \Psi \\ V^T \Phi_w V &amp;amp;= I \end{align}\] We can define $A = V^{-T} = \text{inv}(V^T)$, resulting in \[\begin{align} \Phi_w &amp;amp;= AA^T \\ \Phi_b &amp;amp;= A\Psi A^T \end{align}\] Thus, the PLDA model is defined as: \[\bbox[teal, 4pt]{\begin{align} \boldsymbol{x} &amp;amp;= \boldsymbol{m} + A \boldsymbol{u} \quad \text{where} \\ &amp;amp; \boldsymbol{u} \sim \mathcal{N}(\cdot | \boldsymbol{v}, I) \\ &amp;amp; \boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi) \end{align}}\] Here, $\boldsymbol{u}$ represents the sample data representation of $\boldsymbol{x}$, but projected in the lated projected space $\mathcal{U}$, and $\boldsymbol{v}$ represents the sample label in the lated projected space. These transformations can be computed via \[\boldsymbol{x} = \boldsymbol{m} + A \boldsymbol{u} \quad \leftrightarrow \quad \boldsymbol{u} = V^T (\boldsymbol{x} - \boldsymbol{m})\] \[\boldsymbol{y} = \boldsymbol{m} + A \boldsymbol{v} \quad \leftrightarrow \quad \boldsymbol{v} = V^T (\boldsymbol{y} - \boldsymbol{m})\] And from this point on, we determine the optimal $\boldsymbol{m}$, $A$, and $\Psi$ are. S. Ioffe: “In the training data, the grouping of examples into clusters is given, and we learn the model parameters by maximizing the likelihood. If, instead, the model parameters are fixed, likelihood maximization with respect to the class assignment labels solves a clustering problem” def transform_from_X_to_U(x): global m, A return (x-m) @ np.linalg.inv(A) def transform_from_U_to_X(x): global m, A return m + (x @ A.T) But, at this point, we don’t know what the parameters $A$ or $\Psi$ are, so we can’t use these functions yet. Learning the Model Parameters ($\boldsymbol{m}, \Psi, A$) The loading matrix $A$ is essentially finding the variances $\Phi_b$ and $\Phi_w$, and all of the parameters can be defined using a maximum likelihood framework. Let us say that $D_k$ represents the dataset which contains only samples from the $k^{th}$ class, and $\boldsymbol{x}_k^i$ represents the $i^{th}$ sample from $D_k$, and it belongs to the $k^{th}$ class. Given $N$ training examples separated into $K$ classes, and assuming that they are all independently drawm from their respective class, the log likelihood is \[l(\boldsymbol{x}^{1 \cdots N}) = \sum_{k=1}^K \ln P(\boldsymbol{x}^i : i \in D_k) = \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k)\] where the joint probability distribution of a set of $n$ patterns (assuming all these $n$ patterns belong to the same class $k$) is: \[\begin{align} P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) &amp;amp;= \int \color{red}{P(\boldsymbol{x}^1_k | \boldsymbol{y})} \cdots \color{cyan}{P(\boldsymbol{x}^n_k | \boldsymbol{y})} \color{magenta}{P(\boldsymbol{y})} d\boldsymbol{y} \\ &amp;amp;= \int \color{red}{\mathcal{N}(\boldsymbol{x}^1_k | \boldsymbol{y}, \Phi_w)} \cdots \color{cyan}{\mathcal{N}(\boldsymbol{x}^n_k | \boldsymbol{y}, \Phi_w)} \color{magenta}{\mathcal{N}(\boldsymbol{y} | 0, \Phi_b)} d\boldsymbol{y} \end{align}\] By computing the integral, we obtain \[\color{red}{MAGIC}\] \[\ln P(\boldsymbol{x}^1_k,\cdots, \boldsymbol{x}^n_k) = C - \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} (\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T\right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} ( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)\right)\right]\] where \(\bar{\boldsymbol{x}}_k = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}^i_k\), and $C$ is a constant that can be ignored At this point, as a “hack”, it sets the number of examples for each class to be $n$. In other words, every class ends up having exactly $n$ examples to learn from. Now, if one were to maximize the equation $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\boldsymbol{m}$, one would obtain $\boldsymbol{m}^* = \frac{1}{N} \sum_i \boldsymbol{x}^i$. If one substitutes it back, one would get \[\begin{align} l(\boldsymbol{x}^{1\cdots N}) &amp;amp;= \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) \\ &amp;amp;= -\sum_{k=1}^K \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} \color{cyan}{(\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T} \right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} \color{red}{( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)} \right) \right] \\ &amp;amp;= \cdots \\ &amp;amp;= \color{red}{\text{MAGIC}} \\ &amp;amp;= \cdots \\ &amp;amp;= - \frac{c}{2} \left[ \ln |\Phi_b + \frac{\Phi_w}{n} | + \text{tr} \left( (\Phi_b + \frac{\Phi_w}{n})^{-1} \color{cyan}{S_b} \right) + (n-1) \ln |\Phi_w | + n \text{tr} (\Phi_w^{-1} \color{red}{S_w}) \right] \end{align}\] Now, we need to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Phi_b$ and $\Phi_w$ subject to $\Phi_w$ being p.d. and $\Phi_b$ being p.s.d. Without these constraints, we would obtain \[\Phi_w = \frac{n}{n-1} S_w \quad \text{and} \quad \Phi_b = S_b - \frac{1}{n-1} S_w\] Therefore, if $S_w$ and $S_b$ are diagonal, then the covariances $\Phi_w$ and $\Phi_b$ will also be diagonal, and the diagonalization property holds as long as the contraints above are satisfied. As we have previously stated, we know that \[\Phi_b = A \Psi A^T\] If you fix $\Psi$ and maximize $l(\boldsymbol{x}^{1\cdots N})$ via unconstrained optimization with respect to $A^{-1}$ will make $A^{-1}S_b A^{-T}$ and $A^{-1}S_w A^{-T}$, making the $A^{-T}$ to be the solution of the generalized eigenvector problem involving $S_b$ and $S_w$, where $S_b \boldsymbol{v} = \lambda S_w \boldsymbol{v}$. Then, the projection of the data to the latent space with the LDA projection. [REVIEW] Then, if you were to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Psi$ subject to $\Psi \geq 0$ and $\text{rank}(\Psi) \leq \hat{F}$, then we’ll get the method to optimize the model [REVIEW] Ioffe 2006: “Our method was derived for the case where each class in the training data is represented by the same number $n$ of examples. This may not be true in practice, in which case, we can resample the data to make the number of examples the same, use EM (as shown in section 5), or use approximations. We took the latter approach, using the closed-form solution in Fig. 2, where $n$ was taken to be the average number of examples per class” Algorithm: PLDA Optimization Input: Training $N$ examples from $K$ classes, with $n = N/K$ per class Output: Parameters $\boldsymbol{m}, A, \Psi$, maximizing the likelihood of the PLDA model Compute the covariance matrices $S_b$, and $S_w$ Compute the transformation matrix $W$ such that $S_b \boldsymbol{w} = \lambda S_w \boldsymbol{w}$ (i.e. $eig(S_w^{-1}S_b)$) Compute the covariance matrices in the latent space $\Lambda_b = W^T S_b W$ $\Lambda_w = W^T S_w W$ Determine the following parameters $\boldsymbol{m} = \frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$ $A = W^{-T} \left( \frac{n}{n-1} \Lambda_w \right)^{1/2}$ $\Psi = \max \left( 0, \frac{n-1}{n} (\Lambda_b / \Lambda_w) - \frac{1}{n} \right)$ Reduce the dimensionality to $\hat{F}$ by keeping the largest elements of $\Psi$, while setting the rest to zero. In the latent space $\boldsymbol{u} = A^{-1}(\boldsymbol{x}-\boldsymbol{m})$, only the features for non-zero entries are needed for recognition Note: Scipy’s eigh(A,B) function in linalg solves the generalized eigenvalue problem for a complex Hermittian or a real symmetric matrix, so that $A\boldsymbol{v} = \lambda B \boldsymbol{v}$. Otherwise, if B is ommited, then it is assumed that $B=I$ Also note that scipy.linalg.eigh != np.linalg.eigh def compute_Sb_Sw(X,y): m = np.mean(X,axis=0) _,F = X.shape m_ks, sigma_ks, n_ks = [],[],[] for k in range(K): # Get only the data associated with class k X_k = X[y==k] # Compute the mean, number of samples, and class covariance m_k = np.mean(X_k,axis=0) n_k = len(X_k) sigma_k = np.cov(X_k.T) # Append them all m_ks.append(m_k) n_ks.append(n_k) sigma_ks.append(sigma_k) m_ks = np.array(m_ks) n_ks = np.array(n_ks) sigma_ks = np.array(sigma_ks) assert m_ks.shape == (K,F) assert n_ks.shape == (K,) assert sigma_ks.shape == (K,F,F) S_b = ((m_ks - m).T * n_ks/N) @ (m_ks - m) S_w = np.sum(sigma_ks * ((n_ks-1)/N).reshape(-1,1,1), axis=0) return S_b, S_w assert X.shape[0] == y.shape[0] m = X_pca.mean(axis=0) n = N/K S_b, S_w = compute_Sb_Sw(X_pca,y) # Compute W eigvals, eigvecs = scipy.linalg.eigh(S_b, S_w) W = eigvecs # Compute Lambdas Lambda_b = W.T @ S_b @ W Lambda_w = W.T @ S_w @ W # Compute A A = np.linalg.inv(W.T) * (n / (n-1) * np.diag(Lambda_w))**0.5 print(A.shape) # Compute Psi diag_Lambda_w = Lambda_w.diagonal() diag_Lambda_b = Lambda_b.diagonal() Psi = (n - 1)*(diag_Lambda_b/diag_Lambda_w) - (1/n) Psi[ Psi &amp;lt;= 0 ] = 0 Psi = np.diag(Psi) From this point you can transform the data that is in the PCA subspace $\mathcal{X}$ to the $\mathcal{U}$ subspace, having the data be represented as $U$ u = transform_from_X_to_U(X_pca) Now, again, not every dimension will be relevant in the $\mathcal{U}$ subspace, and that is why we reduce the $\mathcal{U}$ to \(\mathcal{U}_{model}\) , which only contains the relevant dimensions of $\mathcal{U}$. Therefore, in order to go back and forth in between the two subspaces, simply drop them the irrelevant dimensions or add the relevant dimensions to a zero matrix. This new data will be represented as $U_{model}$ def transform_from_U_to_Umodel(x,dims): u_model = u[...,dims] return u_model def transform_from_Umodel_to_U(x,dims,u_dim): shape = (*x.shape[:-1], u_dim) u = np.zeros(shape) u[..., dims] = x return u # Compute the relevant dimensions of Psi relevant_dims = np.squeeze(np.argwhere(Psi.diagonal() != 0))[0] if relevant_dims.ndim == 0: relevant_dims = relevant_dims.reshape(1,) U_model = transform_from_U_to_Umodel(X_pca,relevant_dims) Great! Now we we have the data in the $\mathcal{U}_{model}$ space. Now, all that there is left is to understand how to perform inference, and in order to do that, one needs to find the prior parameters for $\boldsymbol{v}$, the posterior parameters, and the posterior predictive parameters. Let’s discuss now how to find the probability parameters. Inference on the Latent Space First, if you need some review on Bayesian inference for Gaussian distributions, you may check the other post to understand priors, posteriors, posterior predictives, and marginal probability. For this problem, note that the different dimensions have been decorrelated (i.e. the covariances have been decorrelated), thus the different dimensions could be treated as univariate problems. We need to determing the prior parameters of $\boldsymbol{v}$, which leads to the probability distribution $P(\boldsymbol{v})$, the posterior parameters, which leads to \(P(\boldsymbol{v} \vert \boldsymbol{u})\) , and the posterior predictive parameters, for \(P(\boldsymbol{u}^p \vert \boldsymbol{u}^g_{1\cdots n})\) The easiest to determine right off the bat are the prior parameters. For the prior parameters, as one may recall in the model formulation \[\boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)\] which, in turn, are simple to compute. Here we’ll call $\mu_{prior}$ the prior mean, and $\Sigma_{prior}$ the prior covariance, making \[\bbox[teal, 4pt]{\begin{align} \mu^{prior} &amp;amp;= \boldsymbol{0} \\ \Sigma^{prior} &amp;amp;= \Psi_{\forall d \in D} \end{align}}\] where $D$ represents all of the relevant dimensions, which are all that the variances are not zero. Then from this point, we’ll use the notation setting $\hat{\Psi} = \Psi_{\forall d \in D}$ prior_params = { &quot;mean&quot;: np.zeros(relevant_dims), &quot;cov&quot;: np.diag(Psi)[relevant_dims] } Now, the more involved ones are the posteriors parameters. One advantage that PLDA has is that it allows one to make inferences about classes not present during training. Let us consider the following case of classification first. We are given a set of data to learn from, which Ioffe refers to as a “gallery”. This set ${ \boldsymbol{x}^1, \cdots,\boldsymbol{x}^k, \cdots, \boldsymbol{x}^K }$ contains $K$ examples, with one example from each of the $K$ classes. We are also given a probe example $\boldsymbol{x}^p$, and assume that it belongs to one of the $K$ classes. If we are to determine to which class it belongs, maximizing the likelihood will do the job. This can be more easily accomplised in the lated space by performing the trnasformation $\boldsymbol{u} = A^{-1}(\boldsymbol{x} - \boldsymbol{m})$, since it will decorrelate the data. For this example, $\boldsymbol{x}^p$ will be transformed to $\boldsymbol{u}^p$ Single Training Example per Class Let us consider an example $\boldsymbol{u}^g$ from the training set (i.e. gallery), where, again, it belongs to some class between $1 \cdots K$ The probability that the probe $\boldsymbol{u}^p$ belongs to the same class as $\boldsymbol{u}^g$ is defined by the probability $P(\boldsymbol{u}^p | \boldsymbol{u}^g)$. So, the posterior probability will provide a way to perform inference on the class variable $\boldsymbol{v}$ (i.e. the transformed version of $\boldsymbol{y}$). As we may remember, the parameters for a posterior Gaussian are \[\begin{align} \mu^{post} &amp;amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Now, since we only have a single sample $\bar{x} = \boldsymbol{u}$. Additionally, we know that the mean for the prior $\mu^{prior}$ is all zeros (since the data has been centralized), then \[\begin{align} \mu^{post} &amp;amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Additionally, since we would be looking at a single class at this point and the covariances have been diagonalized, the within-class covariance is an identity matrix, making the $(\sigma^{prior})^2 = 1$. Therefore the parameters turn into \[\begin{align} \mu^{post} &amp;amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;amp;= \frac{1(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \end{align}\] And, once again, since we are dealing with a single sample, $N=1$ \[\begin{align} \mu^{post} &amp;amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \boldsymbol{u} \\ (\sigma^{post})^2 &amp;amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \end{align}\] The posterior can then be defined as \[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}) = \mathcal{N}\left(\boldsymbol{v} \bigg| \frac{\hat{\Psi}}{\hat{\Psi} + I}\boldsymbol{u}, \frac{\hat{\Psi}}{\hat{\Psi} + I}\right)}\] Now, if we see how this flows, the class variable $\boldsymbol{v}$ will be used to determine $\boldsymbol{u}$ examples, which are then used to determine the data $\boldsymbol{x}$. from graphviz import Digraph dot = Digraph() dot.node('v',&quot;v&quot;) dot.node('1',&quot;u1&quot;) dot.node('2',&quot;u2&quot;) dot.node(&quot;a&quot;,&quot;x1&quot;) dot.node(&quot;b&quot;,&quot;x2&quot;) dot.edges(['v1',&quot;v2&quot;,&quot;1a&quot;,&quot;2b&quot;]) dot From probabilistic graphical models, if we observe $\boldsymbol{v}$ (i.e. $\boldsymbol{v}$ is given), then $\boldsymbol{u}^p$ and $\boldsymbol{u}^g$ are conditionally independent. We also know that the posterior predictive probability is \[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu^{post}, (\sigma^{post})^2 + \sigma^2)\] And since the within class variance has been diagonalized to an identity matrix ($\sigma^2 = 1$), we’ll obtain \[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}^g) = \mathcal{N}\left(\boldsymbol{u}^p \bigg| \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \boldsymbol{u}^g, I + \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \right)}\] In order to classify $\boldsymbol{u}^p$, then we compute $P(\boldsymbol{u}^p \vert \boldsymbol{u}^g) \forall g \in {1,\cdots,M }$, and pick the maximum. Ioffe 2006: “With PLDA, we were able to combine the knowledge about the general structure of the data, obtained during training, and the examples of new classes, yielding a principled way to perform classification” Multiple Training Examples Per Class We can improve the recognition performance by using more examples. Let us say that we have $n_k$ examples from class $k$, making \[n_k = |U_{model,k}|\] These examples are all independent examples $\boldsymbol{u}_{1\cdots n}^g$. Just as before, we know that, here, we are looking at a single class $k$. We have previously diagonalized all of the covariance matrices, making all of the dimensions (i.e. features) decorrelated, thus they can be worked on individually as if they were univariate features. This also means that the within-class covariance is an identity matrix, making $\sigma^2 = 1$. As we may remember, the parameters for a posterior Gaussian are \[\begin{align} \mu^{post} &amp;amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Again, since our model estimates that the prior mean $\mu^{prior}$ is all zeros, then \[\begin{align} \mu^{post} &amp;amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\ (\sigma^{post})^2 &amp;amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \end{align}\] Since we’re looking at the class $k$, then $N=n_k$ and $\bar{x} = \bar{\boldsymbol{u}}_k$. Thus the posterior for multiple samples is \[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}_k^{1\cdots n_k} ) = \mathcal{N}\left(\boldsymbol{v} \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi}+I}\bar{\boldsymbol{u}}_k, \dfrac{\hat{\Psi}}{n_k \hat{\Psi}+I} \right) }\] Therefore, in order to compute the posterior parameters, where $\mu_k^{post}$ and $\Sigma_k^{post}$ are the mean posterior and the covariance posterior for each class $k$, we have \[\bbox[teal, 4pt]{\begin{align} \Sigma^{post}_k &amp;amp;= \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = \Sigma^{prior} \odot \frac{1}{1 + n_k \cdot \Sigma^{prior}} \\ \mu_k^{post} &amp;amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \underbrace{\left( \sum_{\boldsymbol{u} \in U_{model,k}} \boldsymbol{u} \right)}_{n_k \bar{\boldsymbol{u}}_k} \cdot \Sigma^{post}_k \end{align}}\] where we can recall that $\Sigma^{prior} = \hat{\Psi}$, and $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$ posterior_params = {} for k in np.unique(y): u_model_k = u_model[y==k] n_k = len(u_model_k) cov = prior_params[&quot;cov&quot;] / (1 + n_k * prior_params[&quot;cov&quot;]) mean = np.sum(u_model_k, axis=0) * cov posterior_params[k] = {&quot;mean&quot;: mean, &quot;cov&quot;:cov} As per page 535 by Ioffe 2006, if one sets multiple examples of a class to a single model, assuming we have $n_k$ independent examples \(\{ \boldsymbol{u}_k^i \}_{i=1}^{n_k}\), then the probability of obtaining a sample $\boldsymbol{u}^p$, given the set above, can be obtained from the posterior predictive \[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)\] Once again, since the within-class covariance has been diagonalized to an identity matrix, then $\sigma^2 = 1$ \[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}_k^{1\cdots n_k}) = P(\boldsymbol{u}^p | \boldsymbol{u}_k^1, \cdots, \boldsymbol{u}_k^{n_k}) = \mathcal{N} \left( \boldsymbol{u}^p \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k, I + \dfrac{\hat{\Psi}}{n_k \hat{\Psi} + I} \right) }\] where $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$ This means that, you may compute the predictive parameters as copying the posterior parameters ($\mu_k^{postpred}$, $\Sigma_k^{postpred}$), and adding an identity matrix to the covariance \[\bbox[teal, 4pt]{\begin{align} \mu_k^{postpred} &amp;amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \mu_k^{post} \\ \Sigma_k^{postpred} &amp;amp;= I + \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = I + \Sigma_k^{post} \end{align}}\] post_pred_params = posterior_params.copy() for k,params in post_pred_params.items(): params[&quot;cov&quot;] += 1 Great! This is all that we need to precompute before performing any inference! At this point, you have determined all of the parameters which describe you data. As a review, in order to prepare this algorithm, we have received a training dataset $X$ and its corresponding labels $y$. Then, we have decided on a certain number of components to be used for a PCA. This PCA will bring the data from the space $\mathcal{D}$ to the space $\mathcal{X}$, obtaining the data $X_{pca}$. Then we found the parameters $\boldsymbol{m}$, $A$, and $\hat{\Psi}$, which optimize the PLDA formulation. These would then allow us to bring the data from the $\mathcal{X}$ subspace to the $\mathcal{U}$ latent space, obtaining the data $U$. Then, we reduced the dimensions of $U$ by discarding any dimensions which had a zero variance in $\hat{\Psi}$, yielding the data $U_{model}$. Finally, using these same parameters, we obtain the prior parameters ($\mu^{prior}$ and $\Sigma^{prior}$), the posterior parameters ($\mu^{post}_k$ and $\Sigma^{post}_k$), and the posterior predictive parameters ($\mu^{postpred}_k$ and $\Sigma^{postpred}_k$) Now, we’re ready to do the inference on the latent space We have previously established that we have the classes (i.e. categories) $1,\cdots, K$. What we will do is to iterate through each of the possible classes, and compute its posterior probabilities by using the parameters computed for the posterior predictive probabilities and creating a Gaussian distribution. Finally, we can obtain the probability on that Gaussian at each of those locations. We can also use the log probability for each of the samples on each of the classes \[\bbox[teal, 4pt]{\begin{align} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) &amp;amp;= \mathcal{N}(\boldsymbol{u}_{model} | \mu_k^{postpred}, \Sigma_k^{postpred}) \\ \boldsymbol{y}^* &amp;amp;= \arg\max_{k} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) \end{align}}\] from scipy.stats import multivariate_normal as gaussian log_prob_post = [] for k, param in post_pred_params.items(): mean,cov = param[&quot;mean&quot;], param[&quot;cov&quot;] log_probs_k = gaussian(mean,np.diag(cov)).logpdf(U_model) log_prob_post.append(log_probs_k) log_prob_post = np.array(log_prob_post).T Here, you may choose to normalize the probabilities normalize = False if normalize: logsumexp = np.log(np.sum(np.exp(log_prob_post),axis=-1)) log_probs = log_prob_post - logsumexp[..., None] else: log_probs = log_prob_post categories = np.array([k for k in post_pred_params.keys()]) predictions = categories[np.argmax(log_probs,axis=-1)] N = len(X) shape = [3,2] inds = np.random.choice(N,np.prod(shape)) plt.figure(dpi=200) fig, ax = plt.subplots(*shape) count = 0 for i in np.arange(shape[0]): for j in np.arange(shape[1]): ax[i,j].imshow(X[count].reshape(28,28)) title = &quot;True: {} | Pred: {}&quot;.format(y[count],predictions[count]) ax[i,j].set_title(title) count+= 1 If you wanted to extract the LDA features, you could simply use the transformation functions to convert the data from some space to another space. So this is the case when we have a classification problem where the probes are assumed to belong to one of the trained classes. Now, let us look at the case where the probes belong to classes not yet seen. Hypothesis Testing Here, we try to determine whether two samples belong to the same class or not. For that, we can compute the following likelihoods \[\begin{align} P(\boldsymbol{u}^p)P(\boldsymbol{u}^g) &amp;amp;= \text{likelihood of examples belonging to different classes} \\ P(\boldsymbol{u}^p, \boldsymbol{u}^g) &amp;amp;= \int P(\boldsymbol{u}^p | \boldsymbol{v}) P(\boldsymbol{u}^g|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ &amp;amp;= \text{likelihood of examples belonging to the same class} \end{align}\] As a generalized formulation where there are multiple examples, the likelihood ratio is \[\begin{align} R(\{\boldsymbol{u}^{1\cdots m}_p\},\{\boldsymbol{u}^{1\cdots n}_g\}) &amp;amp;= \frac{\text{likelihood(same)}}{\text{likelihood(diff)}} = \frac{P(\boldsymbol{u}^{1\cdots m}_p,\boldsymbol{u}^{1\cdots n}_g)}{P(\boldsymbol{u}^{1\cdots m}_p)P(\boldsymbol{u}^{1\cdots n}_g)} \\ P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;amp;= \int P(\boldsymbol{u}^1 | \boldsymbol{v}) \cdots P(\boldsymbol{u}^n|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ &amp;amp;= \prod_{t=1}^d \frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} \exp \left( - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} - \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} \right) \end{align}\] where \[\bar{u}_t = \frac{1}{n}\sum_{i=1}^n u_t^i\] For the priors $\pi_{\text{same}}$ and $\pi_{\text{diff}}$, the probability that all of the examples are in the same class is \[\left(1 + \dfrac{\pi_{\text{diff}} / \pi_{\text{same}} }{R} \right)^{-1} = \dfrac{R}{R+\pi_{\text{diff}} / \pi_{\text{same}}}\] If $R &amp;gt; \pi_{\text{diff}} / \pi_{\text{same}}$, the two groups of examples belong to the same class; otherwise they do not. The between-class feature variances $\psi_t$ indicate how discriminative the features are. For example, if $\psi=0$, then it is a completely non-discriminative feature. Therefore, we can compute the marginal likelihoods for each of those possibilities: Marginal probability of them being from same class, i.e. $P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n})$ Note that, for the marginal probability that they are from the same class, we will are treating it as a single marginal probability \(P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n}) =P(\underbrace{\boldsymbol{u}_p^1, \cdots, \boldsymbol{u}_p^m}_{\boldsymbol{u}_p^{1\cdots m}},\underbrace{\boldsymbol{u}_g^1, \cdots, \boldsymbol{u}_g^n}_{\boldsymbol{u}_g^{1\cdots n}})\) Marginal probability of them being from different classes, i.e. $P(\boldsymbol{u}_p^{1\cdots m}) \cdot P(\boldsymbol{u}_g^{1\cdots n})$ By taking the logarithmic, we can can more easily deal with infinitesimal probabilities and helps us make multiplications into additions. Therefore, let us consider \[\begin{align} P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;amp;= \prod_{t=1}^d \underbrace{\frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} }_{C} \exp \left( \underbrace{- \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})}}_{E_1} \underbrace{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }_{E_2} \right) \\ &amp;amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ \log (P(\boldsymbol{u}^{1\cdots n})) &amp;amp;= \sum_{t=1}^d \color{red}{\log (C)} +\color{cyan}{ \log(e^{E_1})} + \color{magenta}{\log(e^{E_2})} \\ &amp;amp;= \sum_{t=1}^d \color{red}{\log (C)} + \color{cyan}{E_1} + \color{magenta}{E_2} \\ &amp;amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(\psi_t + \frac{1}{n} \right)} \color{cyan}{ - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} } \\ &amp;amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(n\psi_t + 1 \right) + \frac{1}{2} \log(n)} \color{cyan}{ - \frac{n \bar{u}_t^2}{2(n\psi_t + 1)} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} } \end{align}\] It is ideal to work in the logarithmic space. With some magic, we get \[\color{red}{\text{Can't get my math to align with this}}\] \[\begin{align} P(\boldsymbol{u}^{1\cdots n}) &amp;amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ \log (C) &amp;amp;= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n\Sigma^{prior} + I) \\ \log (e^{E_1}) &amp;amp;= E_1 = \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2 (n\Sigma^{prior} + I)} \\ \log (e^{E_2}) &amp;amp;= E_2 = -\frac{1}{2} \sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}^2 \\ \log(P(\boldsymbol{u}^{1\cdots n})) &amp;amp;= \sum_{t=1}^d \log(C) + E_1 + E_2 \\ \end{align}\] Now, note that because the data in $U_{model}$ has been normalized $mean(U_{model}) = \boldsymbol{0}$ def marginal_logprob(U): n = U.shape[0] S_prior = prior_params[&quot;cov&quot;] log_C = -0.5 * np.log(2*np.pi) - 0.5 * np.log(n*S_prior + 1) E1 = 0.5*(n**2 * S_prior * np.mean(U,axis=0)**2)/(n*S_prior + 1) E2 = - 0.5 * np.sum(U**2, axis=0) logP_t = log_C + E1 + E2 logP = np.sum(logP_t, axis=-1) return logP This way, if we were to receive two sets of data \[\begin{align} \boldsymbol{u}_p &amp;amp;\in \mathbb{R}^{m\times \hat{F}} \\ \boldsymbol{u}_g &amp;amp;\in \mathbb{R}^{n\times \hat{F}} \end{align}\] We can set a set by concatenating both of them \(\boldsymbol{u}_{pg} \in \mathbb{R}^{(m+n) \times \hat{F}}\) And then we can pass them through the computation above \[\begin{align} log(P(\boldsymbol{u}_p^{1\cdots m}) &amp;amp;= \text{log likelihood for probe set} \\ log(P(\boldsymbol{u}_g^{1\cdots n}) &amp;amp;= \text{log likelihood for gallery set} \\ log(P(\boldsymbol{u}_{pg}^{1\cdots m+n}) &amp;amp;= \text{log likelihood for combined set}\\ \end{align}\] Finally, instead of the ratio, we compute the log of the ratio \[\log(R) = log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] - [log(P(\boldsymbol{u}_p^{1\cdots m}) + log(P(\boldsymbol{u}_g^{1\cdots n})]\] In such case, since we are dealing with log ratios, negative values mean that the model believes the two datapoints are from different categories, where as positive values indicate that the model believes that the two data points are from the same category. logprob_p = marginal_logprob(u_p) logprob_g = marginal_logprob(u_g) logprob_pg = marginal_logprob(np.concatenate([u_p,u_g])) log_Ratio = logprob_pg - (logprob_p + logprob_g) if log_Ratio &amp;gt; 0: print(&quot;Belong to the same class&quot;) else: print(&quot;Belong to different classes&quot;)</summary></entry><entry><title type="html">Linear Discriminant Analysis</title><link href="http://localhost:4000/lda_python.html" rel="alternate" type="text/html" title="Linear Discriminant Analysis" /><published>2021-01-28T00:00:00-05:00</published><updated>2021-01-28T00:00:00-05:00</updated><id>http://localhost:4000/lda</id><content type="html" xml:base="http://localhost:4000/lda_python.html">&lt;p&gt;This post was highly inspired by &lt;a href=&quot;https://sebastianraschka.com/&quot;&gt;Sebastian Raschka&lt;/a&gt;’s &lt;a href=&quot;https://sebastianraschka.com/Articles/2014_python_lda.html&quot;&gt;post&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The goal of LDA is to find some linear transformation $\mathbf{x} \rightarrow \mathbf{x}^T W$ that maximizes the between-class covariance with respect to the within-class covariance.&lt;/p&gt;

\[\uparrow \frac{S_b}{S_w}\]

&lt;p&gt;Here, we can do LDA in a few steps&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Compute the mean vectors of the features for each of the different classes&lt;/li&gt;
  &lt;li&gt;Compute the covariance matrices (between-class and within-class covariance matrices)&lt;/li&gt;
  &lt;li&gt;Compute the eigenvectors and eigenvalues for the covariance matrices&lt;/li&gt;
  &lt;li&gt;Sort the eigenvectors by the eigenvalues and choose the top $k$ eigenvectors to obtain the transformation matrix $W$&lt;/li&gt;
  &lt;li&gt;Use the transformation matrix $W$ to transform the data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, let us say that we have a dataset $X$, such that there are $N$ examples, and $F$ features
\(X \in \mathbb{R}^{N \times F}\)&lt;/p&gt;

&lt;p&gt;And we have its labels $y \in \mathbb{R}^N$&lt;/p&gt;

&lt;p&gt;Let us say that there are $C$ total classes, and $N_c$ represents the number of examples in the class $c$, where $c \in [1, C]$, and the dataset $\mathcal{D}_c$ represents all of the examples in the class $c$.&lt;/p&gt;

&lt;h2 id=&quot;import-libraries&quot;&gt;Import Libraries&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelEncoder&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;setup-dataset&quot;&gt;Setup Dataset&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parsers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filepath_or_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Setosa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Versicolor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Virginica&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sepal_length'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sepal_width'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'petal_length'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'petal_width'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;how&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;all&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-1-compute-the-mean-vectors&quot;&gt;Step 1: Compute the mean vectors&lt;/h2&gt;

&lt;p&gt;Here, we can compute the mean vectors $\mathbf{m}_c$, where $c \in [1,C]$, with&lt;/p&gt;

\[\mathbf{m}_c = \frac{1}{|\mathcal{D}_c|} \sum_{\mathbf{x} \in \mathcal{D}_c} \mathbf{x}\]

&lt;p&gt;Note that here, $\mathbf{x} \in \mathbb{R}^F$, as a column vector. Also note that $\mathbf{m}$ represents the overall mean of the whole dataset&lt;/p&gt;

\[\mathbf{m} = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \mathbf{x}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mean_vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mean_vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-2-compute-the-covariance-matrices&quot;&gt;Step 2: Compute the Covariance Matrices&lt;/h2&gt;
&lt;h3 id=&quot;class-covariance&quot;&gt;Class Covariance&lt;/h3&gt;
&lt;p&gt;To compute a class covariance $S_c$, we can have 
\(S_c = \sum_{\mathbf{x} \in \mathcal{D}_c} (\mathbf{x}-\mathbf{m}_c)(\mathbf{x}-\mathbf{m}_c)^T\)&lt;/p&gt;

&lt;h3 id=&quot;within-class-covariance&quot;&gt;Within-Class Covariance&lt;/h3&gt;
&lt;p&gt;To compute the within-class covariance, 
\(S_w = \sum_{c=1}^C S_c \in \mathbb{R}^{F \times F}\)&lt;/p&gt;

&lt;h3 id=&quot;between-class-covariance&quot;&gt;Between-Class Covariance&lt;/h3&gt;
&lt;p&gt;The between-class covariance can be computed as
\(S_b = \sum_{c=1}^C |\mathcal{D}_c| (\mathbf{m}_c - \mathbf{m}) (\mathbf{m}_c - \mathbf{m})^T \in \mathbb{R}^{F\times F}\)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;class_covariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_vectors&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;S_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S_c&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Within Class Covariance
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_covariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Between Class Covariance
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Within-class Covariance&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Between-class Covariance&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Within-class Covariance
[[38.9562 13.683  24.614   5.6556]
 [13.683  17.035   8.12    4.9132]
 [24.614   8.12   27.22    6.2536]
 [ 5.6556  4.9132  6.2536  6.1756]]
Between-class Covariance
[[ 63.21213333 -19.534      165.16466667  71.36306667]
 [-19.534       10.9776     -56.0552     -22.4924    ]
 [165.16466667 -56.0552     436.64373333 186.90813333]
 [ 71.36306667 -22.4924     186.90813333  80.60413333]]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-3-solve-the-generalized-eigenvalue-problem&quot;&gt;Step 3: Solve the Generalized Eigenvalue Problem&lt;/h2&gt;
&lt;p&gt;So, here, we understand that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eig(A)&lt;/code&gt; function solves for the eigenvalues and eigenvectors of $A$, where $A\mathbf{v} = \lambda \mathbf{v}$. Now, the linear constraints are such that&lt;/p&gt;

\[S_b \mathbf{v} = \lambda S_w \mathbf{v}\]

\[\underbrace{S_w^{-1}S_b}_{A} \mathbf{v} = \lambda \mathbf{v}\]

&lt;p&gt;Therefore, we can compute the eigv*’s for it as&lt;/p&gt;

\[eig(\underbrace{S_w^{-1}S_b}_{\mathbb{R}^{F\times F}})\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Show the Eigenvalue Solutions
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Eigval: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Eigvec: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assert_array_almost_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;decimal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;All eigvecs and eigvals match.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Eigval:  32.27195779972981
Eigvec: 
 [[ 0.20490976]
 [ 0.38714331]
 [-0.54648218]
 [-0.71378517]]

Eigval:  0.27756686384004264
Eigvec: 
 [[-0.00898234]
 [-0.58899857]
 [ 0.25428655]
 [-0.76703217]]

Eigval:  -4.1311796919088535e-15
Eigvec: 
 [[-0.83786868]
 [ 0.16963186]
 [ 0.12293803]
 [ 0.50407077]]

Eigval:  1.1953730364935478e-14
Eigvec: 
 [[ 0.20003692]
 [-0.39490682]
 [-0.45668159]
 [ 0.77167076]]

All eigvecs and eigvals match.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-4-select-the-linear-discriminants-for-the-new-latent-subspace&quot;&gt;Step 4: Select the linear discriminants for the new latent subspace&lt;/h2&gt;
&lt;p&gt;Here, all you have to do is to find the top $k$ eigenvalues and their respective eigenvectors. And then set the transformation. So, if $\mathbf{w}_F$ is the eigenvector associated with the largest eigenvalue $\lambda_F$, and $\mathbf{w}_1$ is the eigenvector associated with the smallest eigenvalue $\lambda_1$ of $S_w^{-1}S_b$, and&lt;/p&gt;

\[\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_F\]

&lt;p&gt;Then the transformation matrix $W$ is&lt;/p&gt;

\[W = \begin{bmatrix}
| &amp;amp; | &amp;amp; &amp;amp; | \\ 
\mathbf{w}_F &amp;amp; \mathbf{w}_{F-1} &amp;amp; \cdots &amp;amp; \mathbf{w}_{F-k} \\
| &amp;amp; | &amp;amp; &amp;amp; | \\ 
\end{bmatrix} \in \mathbb{R}^{F \times k}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Show the Sorted Eigen Pairs
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Unsorted: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sorted in Decreasing Order: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tot_eigval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Eigenvalue {}: {:.2f}%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tot_eigval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Unsorted: 
32.27195779972981 	 [ 0.20490976  0.38714331 -0.54648218 -0.71378517]
0.27756686384004264 	 [-0.00898234 -0.58899857  0.25428655 -0.76703217]
4.1311796919088535e-15 	 [-0.83786868  0.16963186  0.12293803  0.50407077]
1.1953730364935478e-14 	 [ 0.20003692 -0.39490682 -0.45668159  0.77167076]

Sorted in Decreasing Order: 
32.27195779972981 	 [ 0.20490976  0.38714331 -0.54648218 -0.71378517]
0.27756686384004264 	 [-0.00898234 -0.58899857  0.25428655 -0.76703217]
1.1953730364935478e-14 	 [ 0.20003692 -0.39490682 -0.45668159  0.77167076]
4.1311796919088535e-15 	 [-0.83786868  0.16963186  0.12293803  0.50407077]

Eigenvalue 0: 99.15%
Eigenvalue 1: 0.85%
Eigenvalue 2: 0.00%
Eigenvalue 3: 0.00%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By looking at this, it looks like the most informative eigenpairs are the top 2&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-5-transform-the-data-to-the-latent-subspace&quot;&gt;Step 5: Transform the Data to the Latent Subspace&lt;/h2&gt;
&lt;p&gt;Now, the samples in the data space can be placed in the latent subspace by multiplying it by the transformation matrix $W$&lt;/p&gt;

\[\underbrace{U}_{\mathbb{R}_{N \times k}} = \underbrace{X}_{\mathbb{R}^{N\times F}}\underbrace{W}_{\mathbb{R}^{F \times k}}\]

&lt;p&gt;This will lead that $W$ will be able to diagonalizes both $S_b$ and $S_w$, making LDA able to decorrelate the data between and within classes.&lt;/p&gt;

\[W^T S_b W = \begin{bmatrix}* &amp;amp; &amp;amp; &amp;amp; \\  &amp;amp; * &amp;amp; &amp;amp; \\  &amp;amp; &amp;amp; \ddots &amp;amp; \\  &amp;amp; &amp;amp; &amp;amp; * \end{bmatrix}\]

\[W^T S_w W = \begin{bmatrix}* &amp;amp; &amp;amp; &amp;amp; \\  &amp;amp; * &amp;amp; &amp;amp; \\  &amp;amp; &amp;amp; \ddots &amp;amp; \\  &amp;amp; &amp;amp; &amp;amp; * \end{bmatrix}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenpairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X.shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W.shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X_lda.shape: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X.shape:  (150, 4)
W.shape:  (4, 2)
X_lda.shape:  (150, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;showing-the-results&quot;&gt;Showing the Results&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;markers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;o&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;^&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cyan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;markers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LD1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LD2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Data Projection to 2 Linear Discriminants&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/lda_plda/lda_results.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><category term="Machine Learning" /><summary type="html">This post was highly inspired by Sebastian Raschka’s post The goal of LDA is to find some linear transformation $\mathbf{x} \rightarrow \mathbf{x}^T W$ that maximizes the between-class covariance with respect to the within-class covariance. \[\uparrow \frac{S_b}{S_w}\] Here, we can do LDA in a few steps Compute the mean vectors of the features for each of the different classes Compute the covariance matrices (between-class and within-class covariance matrices) Compute the eigenvectors and eigenvalues for the covariance matrices Sort the eigenvectors by the eigenvalues and choose the top $k$ eigenvectors to obtain the transformation matrix $W$ Use the transformation matrix $W$ to transform the data So, let us say that we have a dataset $X$, such that there are $N$ examples, and $F$ features \(X \in \mathbb{R}^{N \times F}\) And we have its labels $y \in \mathbb{R}^N$ Let us say that there are $C$ total classes, and $N_c$ represents the number of examples in the class $c$, where $c \in [1, C]$, and the dataset $\mathcal{D}_c$ represents all of the examples in the class $c$. Import Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.preprocessing import LabelEncoder Setup Dataset df = pd.io.parsers.read_csv( filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None, sep=',', ) label_dict = { 0: &quot;Setosa&quot;, 1: &quot;Versicolor&quot;, 2: &quot;Virginica&quot; } feature_dict = {0:'sepal_length', 1:'sepal_width', 2:'petal_length', 3:'petal_width'} df.columns = list(feature_dict.values())+[&quot;label&quot;] df.dropna(how=&quot;all&quot;, inplace=True) X = df.iloc[:,[0,1,2,3]].values y = df[&quot;label&quot;].values enc = LabelEncoder() enc = enc.fit(y) y = enc.transform(y) num_classes = len(np.unique(y)) N,F = X.shape Step 1: Compute the mean vectors Here, we can compute the mean vectors $\mathbf{m}_c$, where $c \in [1,C]$, with \[\mathbf{m}_c = \frac{1}{|\mathcal{D}_c|} \sum_{\mathbf{x} \in \mathcal{D}_c} \mathbf{x}\] Note that here, $\mathbf{x} \in \mathbb{R}^F$, as a column vector. Also note that $\mathbf{m}$ represents the overall mean of the whole dataset \[\mathbf{m} = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \mathbf{x}\] mean_vectors = [] for label in range(num_classes): mu_c = np.mean(X[y==label],axis=0) mean_vectors.append(mu_c) m = np.mean(X,axis=0).reshape(F,1) Step 2: Compute the Covariance Matrices Class Covariance To compute a class covariance $S_c$, we can have \(S_c = \sum_{\mathbf{x} \in \mathcal{D}_c} (\mathbf{x}-\mathbf{m}_c)(\mathbf{x}-\mathbf{m}_c)^T\) Within-Class Covariance To compute the within-class covariance, \(S_w = \sum_{c=1}^C S_c \in \mathbb{R}^{F \times F}\) Between-Class Covariance The between-class covariance can be computed as \(S_b = \sum_{c=1}^C |\mathcal{D}_c| (\mathbf{m}_c - \mathbf{m}) (\mathbf{m}_c - \mathbf{m})^T \in \mathbb{R}^{F\times F}\) def class_covariance(_class): global mean_vectors S_c = np.zeros((F,F)) m_c = mean_vectors[_class].reshape(F,1) for x in X[y==_class]: x = x.reshape(F,1) S_c += (x-m_c).dot((x-m_c).T) return S_c # Within Class Covariance S_w = np.zeros((F,F)) for class_num in range(num_classes): S_w += class_covariance(class_num) # Between Class Covariance S_b = np.zeros((F,F)) for class_num in range(num_classes): N_c, _ = X[y==class_num].shape m_c = mean_vectors[class_num].reshape(F,1) S_b += N_c * (m_c - m).dot((m_c - m).T) print(&quot;Within-class Covariance&quot;) print(S_w) print(&quot;Between-class Covariance&quot;) print(S_b) Within-class Covariance [[38.9562 13.683 24.614 5.6556] [13.683 17.035 8.12 4.9132] [24.614 8.12 27.22 6.2536] [ 5.6556 4.9132 6.2536 6.1756]] Between-class Covariance [[ 63.21213333 -19.534 165.16466667 71.36306667] [-19.534 10.9776 -56.0552 -22.4924 ] [165.16466667 -56.0552 436.64373333 186.90813333] [ 71.36306667 -22.4924 186.90813333 80.60413333]] Step 3: Solve the Generalized Eigenvalue Problem So, here, we understand that the eig(A) function solves for the eigenvalues and eigenvectors of $A$, where $A\mathbf{v} = \lambda \mathbf{v}$. Now, the linear constraints are such that \[S_b \mathbf{v} = \lambda S_w \mathbf{v}\] \[\underbrace{S_w^{-1}S_b}_{A} \mathbf{v} = \lambda \mathbf{v}\] Therefore, we can compute the eigv*’s for it as \[eig(\underbrace{S_w^{-1}S_b}_{\mathbb{R}^{F\times F}})\] A = np.linalg.inv(S_w).dot(S_b) eigval, eigvec = np.linalg.eig(A) # Show the Eigenvalue Solutions for k in range(len(eigval)): evec = eigvec[:,k].reshape(F,1) print(&quot;Eigval: &quot;,eigval[k]) print(&quot;Eigvec: \n&quot;,evec) print(&quot;&quot;) np.testing.assert_array_almost_equal( np.linalg.inv(S_w).dot(S_b).dot(evec), eigval[k]*evec, decimal = 6, verbose=True ) print(&quot;All eigvecs and eigvals match.&quot;) Eigval: 32.27195779972981 Eigvec: [[ 0.20490976] [ 0.38714331] [-0.54648218] [-0.71378517]] Eigval: 0.27756686384004264 Eigvec: [[-0.00898234] [-0.58899857] [ 0.25428655] [-0.76703217]] Eigval: -4.1311796919088535e-15 Eigvec: [[-0.83786868] [ 0.16963186] [ 0.12293803] [ 0.50407077]] Eigval: 1.1953730364935478e-14 Eigvec: [[ 0.20003692] [-0.39490682] [-0.45668159] [ 0.77167076]] All eigvecs and eigvals match. Step 4: Select the linear discriminants for the new latent subspace Here, all you have to do is to find the top $k$ eigenvalues and their respective eigenvectors. And then set the transformation. So, if $\mathbf{w}_F$ is the eigenvector associated with the largest eigenvalue $\lambda_F$, and $\mathbf{w}_1$ is the eigenvector associated with the smallest eigenvalue $\lambda_1$ of $S_w^{-1}S_b$, and \[\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_F\] Then the transformation matrix $W$ is \[W = \begin{bmatrix} | &amp;amp; | &amp;amp; &amp;amp; | \\ \mathbf{w}_F &amp;amp; \mathbf{w}_{F-1} &amp;amp; \cdots &amp;amp; \mathbf{w}_{F-k} \\ | &amp;amp; | &amp;amp; &amp;amp; | \\ \end{bmatrix} \in \mathbb{R}^{F \times k}\] eigenpairs = [ (np.abs(eigval[k]), eigvec[:,k]) for k in range(len(eigval)) ] # Show the Sorted Eigen Pairs print(&quot;Unsorted: &quot;) for eval,evec in eigenpairs: print(eval,&quot;\t&quot;,evec) print(&quot;&quot;) eigenpairs = sorted(eigenpairs, key=lambda pair: pair[0],reverse=True) print(&quot;Sorted in Decreasing Order: &quot;) for eval,evec in eigenpairs: print(eval,&quot;\t&quot;,evec) print(&quot;&quot;) tot_eigval = np.sum(eigval) for i,(e_val, e_vec) in enumerate(eigenpairs): print(&quot;Eigenvalue {}: {:.2f}%&quot;.format(i,e_val/tot_eigval*100)) Unsorted: 32.27195779972981 [ 0.20490976 0.38714331 -0.54648218 -0.71378517] 0.27756686384004264 [-0.00898234 -0.58899857 0.25428655 -0.76703217] 4.1311796919088535e-15 [-0.83786868 0.16963186 0.12293803 0.50407077] 1.1953730364935478e-14 [ 0.20003692 -0.39490682 -0.45668159 0.77167076] Sorted in Decreasing Order: 32.27195779972981 [ 0.20490976 0.38714331 -0.54648218 -0.71378517] 0.27756686384004264 [-0.00898234 -0.58899857 0.25428655 -0.76703217] 1.1953730364935478e-14 [ 0.20003692 -0.39490682 -0.45668159 0.77167076] 4.1311796919088535e-15 [-0.83786868 0.16963186 0.12293803 0.50407077] Eigenvalue 0: 99.15% Eigenvalue 1: 0.85% Eigenvalue 2: 0.00% Eigenvalue 3: 0.00% By looking at this, it looks like the most informative eigenpairs are the top 2 k = 2 Step 5: Transform the Data to the Latent Subspace Now, the samples in the data space can be placed in the latent subspace by multiplying it by the transformation matrix $W$ \[\underbrace{U}_{\mathbb{R}_{N \times k}} = \underbrace{X}_{\mathbb{R}^{N\times F}}\underbrace{W}_{\mathbb{R}^{F \times k}}\] This will lead that $W$ will be able to diagonalizes both $S_b$ and $S_w$, making LDA able to decorrelate the data between and within classes. \[W^T S_b W = \begin{bmatrix}* &amp;amp; &amp;amp; &amp;amp; \\ &amp;amp; * &amp;amp; &amp;amp; \\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; * \end{bmatrix}\] \[W^T S_w W = \begin{bmatrix}* &amp;amp; &amp;amp; &amp;amp; \\ &amp;amp; * &amp;amp; &amp;amp; \\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; * \end{bmatrix}\] W = np.zeros((F,k)) for i in range(k): W[:,i] = eigenpairs[i][1] X_lda = X.dot(W) print(&quot;X.shape: &quot;,X.shape) print(&quot;W.shape: &quot;,W.shape) print(&quot;X_lda.shape: &quot;, X_lda.shape) X.shape: (150, 4) W.shape: (4, 2) X_lda.shape: (150, 2) Showing the Results markers = [&quot;o&quot;,&quot;x&quot;,&quot;^&quot;] colors = [&quot;blue&quot;,&quot;red&quot;,&quot;cyan&quot;] fig,ax = plt.subplots() for i in range(num_classes): X_c = X_lda[y==i] ax.scatter( x = X_c[:,0], y = X_c[:,1], marker=markers[i], color=colors[i], label = label_dict[i] ) ax.set_xlabel(&quot;LD1&quot;) ax.set_ylabel(&quot;LD2&quot;) ax.set_title(&quot;Data Projection to 2 Linear Discriminants&quot;) ax.grid() ax.legend()</summary></entry><entry><title type="html">Appending to Dataset in HDF5 (h5py)</title><link href="http://localhost:4000/wger_on_single_board_computer.html" rel="alternate" type="text/html" title="Appending to Dataset in HDF5 (h5py)" /><published>2021-01-14T00:00:00-05:00</published><updated>2021-01-14T00:00:00-05:00</updated><id>http://localhost:4000/wger</id><content type="html" xml:base="http://localhost:4000/wger_on_single_board_computer.html">&lt;p&gt;So hard. Okay. Several things. This is if you wish to run it not via Docker. As of today (2021-01-14), their docker images were not working very well for single board computers or for servers. So it had to go via the local.&lt;/p&gt;

&lt;h2 id=&quot;install-wger&quot;&gt;Install wger&lt;/h2&gt;

&lt;p&gt;Go to &lt;a href=&quot;https://github.com/wger-project/wger&quot;&gt;wger’s website&lt;/a&gt;, and follow their instructions for installing wger. As of today (2021-01-14), their instructions for running it as a Django app are the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;python3-dev nodejs npm git
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; yarn sass
python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; venv wger
&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;wger/bin/activate
git clone https://github.com/wger-project/wger.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;wger
pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; requirements.txt
python3 setup.py develop
wger create-settings
wger bootstrap
wger load-online-fixtures
python3 manage.py runserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then login as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin&lt;/code&gt; and password is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adminadmin&lt;/code&gt;. This will be launched on its port 8000. But because they are set on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8000&lt;/code&gt;, you can’t reach it from another computer. So let’s expose some ports…&lt;/p&gt;

&lt;h2 id=&quot;open-the-ports&quot;&gt;Open the Ports&lt;/h2&gt;

&lt;p&gt;Follow this guy’s explanation on (https://www.journaldev.com/34113/opening-a-port-on-linux)&lt;/p&gt;

&lt;p&gt;Now, we’re going to forward the 8000 port to 8001, and expose 8001. Basically, you need to open the firewall port via&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;ufw allow 8001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: You might need to use if you use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;firewalld&lt;/code&gt; systems (e.g. CentOS)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;firewall-cmd --add-port=8001/tcp --permanent
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On your single board comptuer, you can install a GUI to check the firewall (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ufw&lt;/code&gt;) with&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;gufw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;forward-ports&quot;&gt;Forward Ports&lt;/h2&gt;

&lt;p&gt;Omg. I need to make another post about this some other time. This is such a useful tool. The &lt;a href=&quot;https://github.com/icflorescu/iisexpress-proxy&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iisexpress-proxy&lt;/code&gt;&lt;/a&gt; allows you to forward ports from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XXXX&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY&lt;/code&gt; wish such ease. Install it with:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; iisexpress-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then simply run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iisexpress-proxy 8000 to 8001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And now your port http://localhost.8000 can be seen at http://192.168.1.X:8001&lt;/p&gt;

&lt;h2 id=&quot;starting-the-servers-wger&quot;&gt;Starting the Server’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wger&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I haven’t thought out much about how to launch it yet. I haven’t gotten &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -d -m &amp;lt;command&amp;gt;&lt;/code&gt; to work properly. So as of now, do the following:&lt;/p&gt;

&lt;h3 id=&quot;1-ssh-into-the-board&quot;&gt;1) SSH into the Board&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh username@192.168.1.X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-run-the-app&quot;&gt;2) Run the App&lt;/h3&gt;
&lt;p&gt;Create a screen and run the app&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;screen &lt;span class=&quot;nt&quot;&gt;-S&lt;/span&gt; wger
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Activate the virtual environment&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/envs/wger/bin/activate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the app and detach&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python manage.py runserver
&lt;span class=&quot;c&quot;&gt;# Ctrl + A + D&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-forward-the-port&quot;&gt;3) Forward the Port&lt;/h3&gt;

&lt;p&gt;Create a screen to forward the port&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;screen &lt;span class=&quot;nt&quot;&gt;-S&lt;/span&gt; portforward
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Forward the port and detach&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iisexpress-proxy 8000 to 8001
&lt;span class=&quot;c&quot;&gt;# Ctrl + A + D&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can access it at the board’s IP and port 8001: http://192.168.1.X:8001&lt;/p&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><summary type="html">So hard. Okay. Several things. This is if you wish to run it not via Docker. As of today (2021-01-14), their docker images were not working very well for single board computers or for servers. So it had to go via the local. Install wger Go to wger’s website, and follow their instructions for installing wger. As of today (2021-01-14), their instructions for running it as a Django app are the following: sudo apt-get install python3-dev nodejs npm git sudo npm install -g yarn sass python3 -m venv wger source wger/bin/activate git clone https://github.com/wger-project/wger.git cd wger pip install -r requirements.txt python3 setup.py develop wger create-settings wger bootstrap wger load-online-fixtures python3 manage.py runserver Then login as admin and password is adminadmin. This will be launched on its port 8000. But because they are set on http://localhost:8000, you can’t reach it from another computer. So let’s expose some ports… Open the Ports Follow this guy’s explanation on (https://www.journaldev.com/34113/opening-a-port-on-linux) Now, we’re going to forward the 8000 port to 8001, and expose 8001. Basically, you need to open the firewall port via sudo ufw allow 8001 Note: You might need to use if you use firewalld systems (e.g. CentOS) firewall-cmd --add-port=8001/tcp --permanent On your single board comptuer, you can install a GUI to check the firewall (ufw) with sudo apt install gufw Forward Ports Omg. I need to make another post about this some other time. This is such a useful tool. The iisexpress-proxy allows you to forward ports from XXXX to YYYY wish such ease. Install it with: npm install -g iisexpress-proxy Then simply run iisexpress-proxy 8000 to 8001 And now your port http://localhost.8000 can be seen at http://192.168.1.X:8001 Starting the Server’s wger I haven’t thought out much about how to launch it yet. I haven’t gotten screen -d -m &amp;lt;command&amp;gt; to work properly. So as of now, do the following: 1) SSH into the Board ssh username@192.168.1.X 2) Run the App Create a screen and run the app screen -S wger Activate the virtual environment source ~/envs/wger/bin/activate Run the app and detach python manage.py runserver # Ctrl + A + D 3) Forward the Port Create a screen to forward the port screen -S portforward Forward the port and detach iisexpress-proxy 8000 to 8001 # Ctrl + A + D Now you can access it at the board’s IP and port 8001: http://192.168.1.X:8001</summary></entry><entry><title type="html">Appending to Dataset in HDF5 (h5py)</title><link href="http://localhost:4000/appending_to_dataset_h5py.html" rel="alternate" type="text/html" title="Appending to Dataset in HDF5 (h5py)" /><published>2020-11-24T00:00:00-05:00</published><updated>2020-11-24T00:00:00-05:00</updated><id>http://localhost:4000/h5py-appending-to-dataset</id><content type="html" xml:base="http://localhost:4000/appending_to_dataset_h5py.html">&lt;h2 id=&quot;simple-example&quot;&gt;Simple Example&lt;/h2&gt;

&lt;p&gt;This is a reminder of how to add data to a dataset in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h5py&lt;/code&gt;. Let’s first import some packages, and declare a path for a file&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;h5py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.h5&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Optional 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can create&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Create the file
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create an empty dataset with features shaped (40,200)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;my_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# None means that this dimension can be extended
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;f16&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# (0,40,200)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'h5py._hl.dataset.Dataset'&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we set the shape as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(0,?,?)&lt;/code&gt; to mean that it is an empty dataset. We make the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxshape&lt;/code&gt; to have the shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(None,?,?)&lt;/code&gt;, which means that axis 0 can be expanded to whatever we wish. Next, we can start to expand our dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;new_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_data&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (2, 40, 200)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# (4, 40, 200)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# (6, 40, 200)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And this is how you iteratively append to a dataset. In order to check for the data in the file&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (6, 40, 200)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'numpy.ndarray'&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;full-code&quot;&gt;Full Code&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;h5py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.h5&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Optional 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the file
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create an empty dataset with features shaped (40,200)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;my_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# None means that this dimension can be extended
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;f16&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# (0,40,200)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'h5py._hl.dataset.Dataset'&amp;gt;
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_data&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# (2, 40, 200)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# (4, 40, 200)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# (6, 40, 200)
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (6, 40, 200)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'numpy.ndarray'&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;example-for-a-dataset-for-ml&quot;&gt;Example for a Dataset for ML&lt;/h2&gt;

&lt;p&gt;First import the packages, and establish a destination path&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;h5py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.h5&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Optional
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next open up a HDF5 file in appending mode&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we will add groups, where we will keep labels and data. Let’s say we have two different types of features: A and B. We will keep them in separate groups: “features_A” and “features_B”. Here we will create two groups, but we will only work with one.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;group_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_B&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then we will create the datasets. Per group, one dataset will be for features, and another will be for labels. It is important to say that, if you wish to set data in datasets as strings (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str&lt;/code&gt;), you need to create a special type of data type. We’ll use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h5py.special_dtype&lt;/code&gt; to do so [&lt;a href=&quot;https://stackoverflow.com/a/43935389/4962905&quot;&gt;Source&lt;/a&gt;]&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gender&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s set up a few attributes to describe this dataset&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;win_dur&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.025&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hop_dur&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then we can start to populate the dataset and the labels&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;m&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can check the results:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A/features&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A/gender&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;h5py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.h5&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Optional 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_B&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gender&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxshape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16000&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;win_dur&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.025&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hop_dur&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;m&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;feats_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;labels_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A/features&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A/gender&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# {'fs': 16000, 'hop_dur': 0.01, 'win_dur': 0.025}
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# (3, 40, 200)
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'numpy.ndarray'&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ['f', 'm', 'm']
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'numpy.ndarray'&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'str'&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Python" /><summary type="html">Simple Example This is a reminder of how to add data to a dataset in h5py. Let’s first import some packages, and declare a path for a file import h5py import numpy as np path = &quot;test.h5&quot; # Optional import os os.remove(path) We can create # Create the file hdf = h5py.File(path, &quot;a&quot;) # Create an empty dataset with features shaped (40,200) dset = hdf.create_dataset( name = &quot;my_data&quot;, shape = (0,40,200), maxshape = (None, 40, 200), # None means that this dimension can be extended dtype = &quot;f16&quot; ) print(dset.shape) # (0,40,200) print(type(dset)) # &amp;lt;class 'h5py._hl.dataset.Dataset'&amp;gt; Here we set the shape as (0,?,?) to mean that it is an empty dataset. We make the maxshape to have the shape (None,?,?), which means that axis 0 can be expanded to whatever we wish. Next, we can start to expand our dataset: for k in range(3): N = 2 dset.resize(dset.shape[0]+N, axis=0) new_data = np.random.random([N,40,200]) dset[-N:] = new_data print(dset.shape) # (2, 40, 200) # (4, 40, 200) # (6, 40, 200) hdf.close() And this is how you iteratively append to a dataset. In order to check for the data in the file with h5py.File(path,&quot;r&quot;) as hdf: data = hdf[&quot;my_data&quot;][:] print(data.shape) # (6, 40, 200) print(type(data)) # &amp;lt;class 'numpy.ndarray'&amp;gt; Full Code import h5py import numpy as np path = &quot;test.h5&quot; # Optional import os os.remove(path) # Create the file with h5py.File(path, &quot;a&quot;) as hdf: # Create an empty dataset with features shaped (40,200) dset = hdf.create_dataset( name = &quot;my_data&quot;, shape = (0,40,200), maxshape = (None, 40, 200), # None means that this dimension can be extended dtype = &quot;f16&quot; ) print(dset.shape) # (0,40,200) print(type(dset)) # &amp;lt;class 'h5py._hl.dataset.Dataset'&amp;gt; for k in range(3): N = 2 dset.resize(dset.shape[0]+N, axis=0) new_data = np.random.random([N,40,200]) dset[-N:] = new_data print(dset.shape) # (2, 40, 200) # (4, 40, 200) # (6, 40, 200) with h5py.File(path,&quot;r&quot;) as hdf: data = hdf[&quot;my_data&quot;][:] print(data.shape) # (6, 40, 200) print(type(data)) # &amp;lt;class 'numpy.ndarray'&amp;gt; Example for a Dataset for ML First import the packages, and establish a destination path import h5py import numpy as np import random path = &quot;test.h5&quot; # Optional os.remove(path) Next open up a HDF5 file in appending mode hdf = h5py.File(path, &quot;a&quot;) Now we will add groups, where we will keep labels and data. Let’s say we have two different types of features: A and B. We will keep them in separate groups: “features_A” and “features_B”. Here we will create two groups, but we will only work with one. group_A = hdf.create_group(&quot;features_A&quot;) group_B = hdf.create_group(&quot;features_B&quot;) And then we will create the datasets. Per group, one dataset will be for features, and another will be for labels. It is important to say that, if you wish to set data in datasets as strings (str), you need to create a special type of data type. We’ll use h5py.special_dtype to do so [Source] dt = h5py.special_dtype(vlen=str) feats_A = group_A.create_dataset(&quot;features&quot;,(0,40,200), maxshape=(None,40,200)) labels_A = group_A.create_dataset(&quot;gender&quot;, (0,), maxshape=(None,), dtype=dt) Now, let’s set up a few attributes to describe this dataset group_A.attrs[&quot;fs&quot;] = 16000 group_A.attrs[&quot;win_dur&quot;] = 0.025 group_A.attrs[&quot;hop_dur&quot;] = 0.01 And then we can start to populate the dataset and the labels gender = [&quot;m&quot;, &quot;f&quot;] for k in range(3): feats_A.resize(feats_A.shape[0]+1, axis=0) labels_A.resize(labels_A.shape[0]+1, axis=0) feats_A[-1] = np.random.random([1,40,200]) labels_A[-1]= random.sample(gender,1)[0] hdf.close() Now we can check the results: with h5py.File(path,&quot;r&quot;) as hdf: feats = hdf[&quot;features_A/features&quot;][:] labels= hdf[&quot;features_A/gender&quot;][:] g1 = hdf[&quot;features_A&quot;] params = dict(g1.attrs) print(params) print(feats) print(type(feats)) print(labels) print(type(labels)) import h5py import numpy as np import random path = &quot;test.h5&quot; # Optional import os os.remove(path) with h5py.File(path, &quot;a&quot;) as hdf: group_A = hdf.create_group(&quot;features_A&quot;) group_B = hdf.create_group(&quot;features_B&quot;) dt = h5py.special_dtype(vlen=str) feats_A = group_A.create_dataset(&quot;features&quot;,(0,40,200), maxshape=(None,40,200)) labels_A = group_A.create_dataset(&quot;gender&quot;, (0,), maxshape=(None,), dtype=dt) group_A.attrs[&quot;fs&quot;] = 16000 group_A.attrs[&quot;win_dur&quot;] = 0.025 group_A.attrs[&quot;hop_dur&quot;] = 0.01 gender = [&quot;m&quot;, &quot;f&quot;] for k in range(3): feats_A.resize(feats_A.shape[0]+1, axis=0) labels_A.resize(labels_A.shape[0]+1, axis=0) feats_A[-1] = np.random.random([1,40,200]) labels_A[-1]= random.sample(gender,1)[0] with h5py.File(path,&quot;r&quot;) as hdf: feats = hdf[&quot;features_A/features&quot;][:] labels= hdf[&quot;features_A/gender&quot;][:] g1 = hdf[&quot;features_A&quot;] params = dict(g1.attrs) print(params) # {'fs': 16000, 'hop_dur': 0.01, 'win_dur': 0.025} print(feats) # (3, 40, 200) print(type(feats)) # &amp;lt;class 'numpy.ndarray'&amp;gt; print(labels) # ['f', 'm', 'm'] print(type(labels)) # &amp;lt;class 'numpy.ndarray'&amp;gt; print(type(labels[0])) # &amp;lt;class 'str'&amp;gt;</summary></entry><entry><title type="html">Slurm Commands</title><link href="http://localhost:4000/slurm_commands.html" rel="alternate" type="text/html" title="Slurm Commands" /><published>2020-11-21T00:00:00-05:00</published><updated>2020-11-21T00:00:00-05:00</updated><id>http://localhost:4000/slurm-commands</id><content type="html" xml:base="http://localhost:4000/slurm_commands.html">&lt;p&gt;Here is a quick reminders on &lt;a href=&quot;https://slurm.schedmd.com/&quot;&gt;Slurm&lt;/a&gt; and how to use it&lt;/p&gt;

&lt;h2 id=&quot;one-line-commands&quot;&gt;One Line Commands&lt;/h2&gt;

&lt;h3 id=&quot;view-the-running-jobs&quot;&gt;View the Running Jobs&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ squeue
   JOBID  PARTITION             NAME     USER   ST       TIME     NODES    NODELIST(REASON)
   26529      batch       train_BERT      joe    R    1:24:52         1              cnode1
   26530      batch     train_ResNet    maria    R    2:12:56         1              cnode2
   26531      batch      update_plex    maria    R    2:14:20         1              cnode3
   26532      batch    download_data    janet    R    2:16:17         1              cnode7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;view-nodes-and-partitions&quot;&gt;View Nodes and Partitions&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sinfo
PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST
batch*          up 8-00:00:00      9    mix cnode[1-9]
batch*          up 8-00:00:00      3   idle cnode[10-12]
gpu             up 8-00:00:00      9    mix cnode[1-9]
gpu             up 8-00:00:00      3   idle cnode[10-12]
rtx             up 8-00:00:00      4   idle rnode[1-4]
reservations    up   infinite      1  down* nvidia-dgx1
reservations    up   infinite     10    mix beauty,cnode[1-9]
reservations    up   infinite      7   idle cnode[10-12],rnode[1-4]
overflow        up   infinite      1  down* nvidia-dgx1
overflow        up   infinite     10    mix beauty,cnode[1-9]
overflow        up   infinite      7   idle cnode[10-12],rnode[1-4]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;submitting-a-job&quot;&gt;Submitting a Job&lt;/h2&gt;

&lt;p&gt;You can submit either a job or open an interactive bash, and then run your job through there&lt;/p&gt;

&lt;h3 id=&quot;running-a-job-with-sbatch&quot;&gt;Running a Job with sbatch&lt;/h3&gt;

&lt;p&gt;First create a shell script that will be used&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# run.sh&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --job-name  {job_name}         # E.g. train_model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --nodelist  {node_name}        # E.g. cnode1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --partition {partition_name}   # E.g. gpu&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --gpus      {number_of_gpus}   # E.g. 1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --time      {max_time}         # E.g. 08-00&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --ntasks    {number_of_cpus}   # E.g. 16&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --mem       {ram_memory}       # E.g. 32G&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Enable using a python &lt;/span&gt;
scl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;rh-python36 bash
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /home/user/envs/myenv/bin/activate

&lt;span class=&quot;c&quot;&gt;# Copy data into your /tmp folder (Read below)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; /home/user/data/my_dataset /tmp/user/my_dataset

&lt;span class=&quot;c&quot;&gt;# Copy your script&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;my_experiment.py /tmp/user

&lt;span class=&quot;c&quot;&gt;# Change into that directory&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /tmp/user

&lt;span class=&quot;c&quot;&gt;# Run your script&lt;/span&gt;
python my_experiment.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sometimes, job is deployed at a different node than that of where you originally are. For example, if you are on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnode0&lt;/code&gt;, sometimes the job gets deployed at another node (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnode1&lt;/code&gt;). This can be seen in your terminal, which should say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user@cnode0&lt;/code&gt;. With that, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt; directory in different nodes are not the same. In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnode0&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt; directory is not the same as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnode1&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt;. Therefore, if you are dealing with a dataset, it is not wise to have the server to get data from another computer every time. So, it’s helpful to copy that dataset into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt; directory&lt;/p&gt;

&lt;p&gt;Finally, you can run the job with&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sbatch run.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is the same script as the one above, but using the shortened flags&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# run.sh&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -J       {job_name}         # E.g. train_model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -w       {node_name}        # E.g. cnode1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -p       {partition_name}   # E.g. gpu&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -G       {number_of_gpus}   # E.g. 1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -t       {max_time}         # E.g. 08-00&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -n       {number_of_cpus}   # E.g. 16&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --mem    {ram_memory}       # E.g. 32G&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Enable using a python &lt;/span&gt;
scl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;rh-python36 bash
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /home/user/envs/myenv/bin/activate

&lt;span class=&quot;c&quot;&gt;# Copy data into your /tmp folder (Read below)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; /home/user/data/my_dataset /tmp/user/my_dataset

&lt;span class=&quot;c&quot;&gt;# Copy your script&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;my_experiment.py /tmp/user

&lt;span class=&quot;c&quot;&gt;# Change into that directory&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /tmp/user

&lt;span class=&quot;c&quot;&gt;# Run your script&lt;/span&gt;
python my_experiment.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;run-a-job-with-srun&quot;&gt;Run a Job with srun&lt;/h3&gt;

&lt;p&gt;This is the way you can run a job interactively. Basically, a lot of the above is done again, but just interactively.&lt;/p&gt;

&lt;p&gt;First you need to get into the interactive bash. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--pty&lt;/code&gt; flag in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;srun&lt;/code&gt; allows you to execute a task in a pseudo terminal mode&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@cnode0:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;srun &lt;span class=&quot;nt&quot;&gt;-J&lt;/span&gt; jobname &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; gpu &lt;span class=&quot;nt&quot;&gt;-G&lt;/span&gt; 1 &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; cnode7 &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 10 &lt;span class=&quot;nt&quot;&gt;--mem&lt;/span&gt; 16G &lt;span class=&quot;nt&quot;&gt;--pty&lt;/span&gt; bash
&lt;span class=&quot;c&quot;&gt;# or srun -J jobname -p batch -G 1 -w cnode7 -n 10 --mem 16G --pty bash&lt;/span&gt;
user@cnode7:~&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can run your jobs as you normally would. E.g.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@cnode7:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; /home/user/data/my_dataset /tmp/user
user@cnode7:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /home/user/envs/myenv/bin/activate
user@cnode7:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;my_experiment.py /tmp/user
user@cnode7:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /tmp/user
user@cnode7:~&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;python my_experiment.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://slurm.schedmd.com/quickstart.html&quot;&gt;Slurm’s Quick Start User Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example written by Jim Kinney and Robert Tweedy is&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This is an example SBATCH script &quot;slurm_example_script.sh&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For all available options, see the 'sbatch' manpage.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Note that all SBATCH commands must start with a #SBATCH directive;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to comment out one of these you must add another # at the beginning of the line.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# All #SBATCH directives must be declared before any other commands appear in the script.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Once you understand how to use this file, you can remove these comments to make it&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# easier to read/edit/work with/etc. :-)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (Recommended)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Name the project in the batch job queue&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -J ExampleName&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (Optional)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### If you'd like to give a bit more information about your job, you can&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### use the command below.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --comment='A comment/brief descriptive name of your job goes here.'&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (REQUIRED)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Select the queue (also called &quot;partition&quot;) to use. The available partitions for your&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### use are visible using the 'sinfo' command.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### You must specify 'gpu' or another partition to have access to the system GPUs.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -p batch&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (REQUIRED for GPU, otherwise do not specify)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### If you select a GPU queue, you must also use the command below to select the number of GPUs&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### to use. Note that you're limited to 1 GPU per job as a maximum on the basic GPU queue.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### If you need to use more than 1, contact bmi-it@emory.edu to schedule a multi-gpu test for&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### access to the multi-gpu queue.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### If you need a specific type of GPU, you can prefix the number with the GPU's type like&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### so: &quot;SBATCH -G turing:1&quot;. The available types of GPUs as of 04/16/2020 are:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### turing (12 total)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### titan (only 1; requesting this GPU may result in a delay in your job starting)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### pascal (4 total; using this GPU requires that your code handle being pre-empted/stopped at any&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###        time, as there are certain users with priority access to these GPUs).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### volta (8 total) - You must use the 'dgx-only' partition to select these GPUs.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### rtx (4 total) - NVidia Quadro RTX 6000. You must use the 'rtx' or 'overflow' partitions to select these GPUs.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;##SBATCH -G 1&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (REQUIRED) if you don't want your job to end after 8 hours!&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### If you know your job needs to run for a long time or will finish up relatively&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### quickly then set the command below to specify how long your job should take to run.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### This may allow it to start running sooner if the cluster is under heavy load.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Your job will be held to the value you specify, which means that it will be ended&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### if it should go over the limit. If you're unsure of how long your job will take to run, it's&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### better to err on the longer side as jobs can always finish earlier, but they can't extend their&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### requested time limit to run longer.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### The format can be &quot;minutes&quot;, &quot;hours:minutes:seconds&quot;, &quot;days-hours&quot;, or &quot;days-hours:minutes:seconds&quot;.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### By default, jobs will run for 8 hours if this isn't specified.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -t 8:0:0&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### (optional) Output and error file definitions. To have all output in a file named&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### &quot;slurm-&amp;lt;jobID&amp;gt;.out&quot; just remove the two SBATCH commands below. Specifying the -e parameter&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### will split the stdout and stderr output into different files.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### The %A is replaced with the job's ID.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -o file-%A.out&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -e file-%A.err&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### You can specify the number of nodes, number of cpus/threads, and amount of memory per node&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### you need for your job. We recommend specifying only memory unless you know you need a&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### specific number of nodes/threads, as you will be automatically allocated a reasonable&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### amount of threads based on the memory amount requested.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (REQUIRED)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Request 4 GB of RAM - You should always specify some value for this option, otherwise&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                       your job's available memory will be limited to a default value&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                       which may not be high enough for your code to run successfully.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                       This value is for the amount of RAM per computational node.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --mem 4G&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (optional)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Request 4 cpus/threads - Specify a value for this function if you know your code uses&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          multiple CPU threads when running and need to override the&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          automatic allocation of threads based on your memory request&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          above. Note that this value is for the TOTAL number of threads&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          available to the job, NOT threads per computational node! Also note&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          that Matlab is limited to using up to 15 threads per node due to&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                          licensing restrictions imposed by the Matlab software.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;##SBATCH -n 4&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (optional)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Request 2 cpus/threads per task - This differs from the &quot;-n&quot; parameter above in that it&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                                   specifies how many threads should be allocated per job&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                                   task. By default, this is 1 thread per task. Set this&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                                   parameter if you need to dedicate multiple threads to a&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                                   single program/application rather than running multiple&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                                   separate applications which require a single thread each.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;##SBATCH -c 2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (very optional; leave as '1' unless you know what you're doing)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Request 1 node - Only specify a value other than 1 for this option when you know that&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                  your code will run across multiple systems concurrently. Otherwise&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###                  you're just wasting resources that could be used by others.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH -N 1&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### (optional)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### This is to send you emails of job status&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### See the manpage for sbatch for all the available options.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --mail-user=nicolas.s.shu@gmail.com&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --mail-type=ALL&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Your actual commands to start your code go below this area. If you need to run anything in&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### the SCL python environments that's more complex than a simple Python script (as in, if you&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### have to do some other setup in the shell environment first for your code), then you should&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### write a wrapper script that does all the necessary steps and then run it like in the below&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### example:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### scl enable rh-python36 '/home/mynetid/my_wrapper_script.sh'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Otherwise, you're probably not running everything you think you are in the SCL environment.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;hostname
echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Hello world!'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; test.txt
scl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;rh-python36 &lt;span class=&quot;s1&quot;&gt;'python shit.py'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; shit.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Instructions" /><category term="Server" /><summary type="html">Here is a quick reminders on Slurm and how to use it One Line Commands View the Running Jobs $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 26529 batch train_BERT joe R 1:24:52 1 cnode1 26530 batch train_ResNet maria R 2:12:56 1 cnode2 26531 batch update_plex maria R 2:14:20 1 cnode3 26532 batch download_data janet R 2:16:17 1 cnode7 View Nodes and Partitions $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST batch* up 8-00:00:00 9 mix cnode[1-9] batch* up 8-00:00:00 3 idle cnode[10-12] gpu up 8-00:00:00 9 mix cnode[1-9] gpu up 8-00:00:00 3 idle cnode[10-12] rtx up 8-00:00:00 4 idle rnode[1-4] reservations up infinite 1 down* nvidia-dgx1 reservations up infinite 10 mix beauty,cnode[1-9] reservations up infinite 7 idle cnode[10-12],rnode[1-4] overflow up infinite 1 down* nvidia-dgx1 overflow up infinite 10 mix beauty,cnode[1-9] overflow up infinite 7 idle cnode[10-12],rnode[1-4] Submitting a Job You can submit either a job or open an interactive bash, and then run your job through there Running a Job with sbatch First create a shell script that will be used # run.sh #!/bin/bash #SBATCH --job-name {job_name} # E.g. train_model #SBATCH --nodelist {node_name} # E.g. cnode1 #SBATCH --partition {partition_name} # E.g. gpu #SBATCH --gpus {number_of_gpus} # E.g. 1 #SBATCH --time {max_time} # E.g. 08-00 #SBATCH --ntasks {number_of_cpus} # E.g. 16 #SBATCH --mem {ram_memory} # E.g. 32G # Enable using a python scl enable rh-python36 bash source /home/user/envs/myenv/bin/activate # Copy data into your /tmp folder (Read below) cp /home/user/data/my_dataset /tmp/user/my_dataset # Copy your script cp my_experiment.py /tmp/user # Change into that directory cd /tmp/user # Run your script python my_experiment.py Sometimes, job is deployed at a different node than that of where you originally are. For example, if you are on cnode0, sometimes the job gets deployed at another node (e.g. cnode1). This can be seen in your terminal, which should say user@cnode0. With that, the /tmp directory in different nodes are not the same. In other words, cnode0’s /tmp directory is not the same as cnode1’s /tmp. Therefore, if you are dealing with a dataset, it is not wise to have the server to get data from another computer every time. So, it’s helpful to copy that dataset into the /tmp directory Finally, you can run the job with sbatch run.sh Below is the same script as the one above, but using the shortened flags # run.sh #!/bin/bash #SBATCH -J {job_name} # E.g. train_model #SBATCH -w {node_name} # E.g. cnode1 #SBATCH -p {partition_name} # E.g. gpu #SBATCH -G {number_of_gpus} # E.g. 1 #SBATCH -t {max_time} # E.g. 08-00 #SBATCH -n {number_of_cpus} # E.g. 16 #SBATCH --mem {ram_memory} # E.g. 32G # Enable using a python scl enable rh-python36 bash source /home/user/envs/myenv/bin/activate # Copy data into your /tmp folder (Read below) cp /home/user/data/my_dataset /tmp/user/my_dataset # Copy your script cp my_experiment.py /tmp/user # Change into that directory cd /tmp/user # Run your script python my_experiment.py Run a Job with srun This is the way you can run a job interactively. Basically, a lot of the above is done again, but just interactively. First you need to get into the interactive bash. The --pty flag in srun allows you to execute a task in a pseudo terminal mode user@cnode0:~$ srun -J jobname -p gpu -G 1 -w cnode7 -n 10 --mem 16G --pty bash # or srun -J jobname -p batch -G 1 -w cnode7 -n 10 --mem 16G --pty bash user@cnode7:~$ Then you can run your jobs as you normally would. E.g. user@cnode7:~$ cp /home/user/data/my_dataset /tmp/user user@cnode7:~$ source /home/user/envs/myenv/bin/activate user@cnode7:~$ cp my_experiment.py /tmp/user user@cnode7:~$ cd /tmp/user user@cnode7:~$ python my_experiment.py Additional Resources Slurm’s Quick Start User Guide An example written by Jim Kinney and Robert Tweedy is #!/bin/bash # This is an example SBATCH script &quot;slurm_example_script.sh&quot; # For all available options, see the 'sbatch' manpage. # # Note that all SBATCH commands must start with a #SBATCH directive; # to comment out one of these you must add another # at the beginning of the line. # All #SBATCH directives must be declared before any other commands appear in the script. # # Once you understand how to use this file, you can remove these comments to make it # easier to read/edit/work with/etc. :-) ### (Recommended) ### Name the project in the batch job queue #SBATCH -J ExampleName ### (Optional) ### If you'd like to give a bit more information about your job, you can ### use the command below. #SBATCH --comment='A comment/brief descriptive name of your job goes here.' ### (REQUIRED) ### Select the queue (also called &quot;partition&quot;) to use. The available partitions for your ### use are visible using the 'sinfo' command. ### You must specify 'gpu' or another partition to have access to the system GPUs. #SBATCH -p batch ### (REQUIRED for GPU, otherwise do not specify) ### If you select a GPU queue, you must also use the command below to select the number of GPUs ### to use. Note that you're limited to 1 GPU per job as a maximum on the basic GPU queue. ### If you need to use more than 1, contact bmi-it@emory.edu to schedule a multi-gpu test for ### access to the multi-gpu queue. ### ### If you need a specific type of GPU, you can prefix the number with the GPU's type like ### so: &quot;SBATCH -G turing:1&quot;. The available types of GPUs as of 04/16/2020 are: ### turing (12 total) ### titan (only 1; requesting this GPU may result in a delay in your job starting) ### pascal (4 total; using this GPU requires that your code handle being pre-empted/stopped at any ### time, as there are certain users with priority access to these GPUs). ### volta (8 total) - You must use the 'dgx-only' partition to select these GPUs. ### rtx (4 total) - NVidia Quadro RTX 6000. You must use the 'rtx' or 'overflow' partitions to select these GPUs. ##SBATCH -G 1 ### (REQUIRED) if you don't want your job to end after 8 hours! ### If you know your job needs to run for a long time or will finish up relatively ### quickly then set the command below to specify how long your job should take to run. ### This may allow it to start running sooner if the cluster is under heavy load. ### Your job will be held to the value you specify, which means that it will be ended ### if it should go over the limit. If you're unsure of how long your job will take to run, it's ### better to err on the longer side as jobs can always finish earlier, but they can't extend their ### requested time limit to run longer. ### ### The format can be &quot;minutes&quot;, &quot;hours:minutes:seconds&quot;, &quot;days-hours&quot;, or &quot;days-hours:minutes:seconds&quot;. ### By default, jobs will run for 8 hours if this isn't specified. #SBATCH -t 8:0:0 ### (optional) Output and error file definitions. To have all output in a file named ### &quot;slurm-&amp;lt;jobID&amp;gt;.out&quot; just remove the two SBATCH commands below. Specifying the -e parameter ### will split the stdout and stderr output into different files. ### The %A is replaced with the job's ID. #SBATCH -o file-%A.out #SBATCH -e file-%A.err ### You can specify the number of nodes, number of cpus/threads, and amount of memory per node ### you need for your job. We recommend specifying only memory unless you know you need a ### specific number of nodes/threads, as you will be automatically allocated a reasonable ### amount of threads based on the memory amount requested. ### (REQUIRED) ### Request 4 GB of RAM - You should always specify some value for this option, otherwise ### your job's available memory will be limited to a default value ### which may not be high enough for your code to run successfully. ### This value is for the amount of RAM per computational node. #SBATCH --mem 4G ### (optional) ### Request 4 cpus/threads - Specify a value for this function if you know your code uses ### multiple CPU threads when running and need to override the ### automatic allocation of threads based on your memory request ### above. Note that this value is for the TOTAL number of threads ### available to the job, NOT threads per computational node! Also note ### that Matlab is limited to using up to 15 threads per node due to ### licensing restrictions imposed by the Matlab software. ##SBATCH -n 4 ### (optional) ### Request 2 cpus/threads per task - This differs from the &quot;-n&quot; parameter above in that it ### specifies how many threads should be allocated per job ### task. By default, this is 1 thread per task. Set this ### parameter if you need to dedicate multiple threads to a ### single program/application rather than running multiple ### separate applications which require a single thread each. ##SBATCH -c 2 ### (very optional; leave as '1' unless you know what you're doing) ### Request 1 node - Only specify a value other than 1 for this option when you know that ### your code will run across multiple systems concurrently. Otherwise ### you're just wasting resources that could be used by others. #SBATCH -N 1 ### (optional) ### This is to send you emails of job status ### See the manpage for sbatch for all the available options. #SBATCH --mail-user=nicolas.s.shu@gmail.com #SBATCH --mail-type=ALL ### Your actual commands to start your code go below this area. If you need to run anything in ### the SCL python environments that's more complex than a simple Python script (as in, if you ### have to do some other setup in the shell environment first for your code), then you should ### write a wrapper script that does all the necessary steps and then run it like in the below ### example: ### ### scl enable rh-python36 '/home/mynetid/my_wrapper_script.sh' ### ### Otherwise, you're probably not running everything you think you are in the SCL environment. hostname echo 'Hello world!' &amp;gt; test.txt scl enable rh-python36 'python shit.py' &amp;gt; shit.txt</summary></entry><entry><title type="html">Port Forwarding with Netgear Routers</title><link href="http://localhost:4000/port_forwarding_netgear.html" rel="alternate" type="text/html" title="Port Forwarding with Netgear Routers" /><published>2020-10-30T00:00:00-04:00</published><updated>2020-10-30T00:00:00-04:00</updated><id>http://localhost:4000/router-port-forwarding</id><content type="html" xml:base="http://localhost:4000/port_forwarding_netgear.html">&lt;p&gt;In order to forward a port in Netgear, first access your router. If you’re connected to the router already, you can either reach it via http://www.routerlogin.net or via http://192.168.1.1&lt;/p&gt;

&lt;p&gt;You’ll be on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BASIC&lt;/code&gt; tab. Click on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADVANCED&lt;/code&gt;. Now go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced Setup &amp;gt; Port Forwarding / Port Triggering&lt;/code&gt;. Here’s where you’ll be allowing ports in computers to be accessed for remote purposes.&lt;/p&gt;

&lt;p&gt;So click on the button &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+ Add Custom Service&lt;/code&gt;. There, enter the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Service Name&lt;/strong&gt;: Choose any name&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Service Type&lt;/strong&gt;: TCP/UDP&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;External Port Range&lt;/strong&gt;: Choose the port that you wish to forward (e.g. 8080)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Internal Port Range&lt;/strong&gt;: Use the same as the “External Port Range”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Internal IP Address&lt;/strong&gt;: Enter or choose the machine that you wish its port to be exposed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, click on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Apply&lt;/code&gt; button, and you should have access.&lt;/p&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Server" /><category term="Networking" /><summary type="html">In order to forward a port in Netgear, first access your router. If you’re connected to the router already, you can either reach it via http://www.routerlogin.net or via http://192.168.1.1 You’ll be on the BASIC tab. Click on the ADVANCED. Now go to Advanced Setup &amp;gt; Port Forwarding / Port Triggering. Here’s where you’ll be allowing ports in computers to be accessed for remote purposes. So click on the button + Add Custom Service. There, enter the following: Service Name: Choose any name Service Type: TCP/UDP External Port Range: Choose the port that you wish to forward (e.g. 8080) Internal Port Range: Use the same as the “External Port Range” Internal IP Address: Enter or choose the machine that you wish its port to be exposed. Finally, click on the Apply button, and you should have access.</summary></entry><entry><title type="html">Synology NAS Ports</title><link href="http://localhost:4000/synology_nas_ports.html" rel="alternate" type="text/html" title="Synology NAS Ports" /><published>2020-10-30T00:00:00-04:00</published><updated>2020-10-30T00:00:00-04:00</updated><id>http://localhost:4000/synology-nas-ports</id><content type="html" xml:base="http://localhost:4000/synology_nas_ports.html">&lt;p&gt;This is a list of ports for the Synology NAS&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Port Number&lt;/th&gt;
      &lt;th&gt;Software / Application&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2202/ubooquity&lt;/td&gt;
      &lt;td&gt;Ubooquity User&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2203&lt;/td&gt;
      &lt;td&gt;Ubooquity Admin&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;names&lt;/td&gt;
      &lt;td&gt;Synology&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;32400&lt;/td&gt;
      &lt;td&gt;Plex&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8096&lt;/td&gt;
      &lt;td&gt;Jellyfin&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8181&lt;/td&gt;
      &lt;td&gt;Tautulli&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9000&lt;/td&gt;
      &lt;td&gt;Portainer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7080&lt;/td&gt;
      &lt;td&gt;Calibre&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1643&lt;/td&gt;
      &lt;td&gt;wger&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8003&lt;/td&gt;
      &lt;td&gt;Firefly III&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;phpAdmin&lt;/td&gt;
      &lt;td&gt;phpAdmin&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8001&lt;/td&gt;
      &lt;td&gt;wger&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3000&lt;/td&gt;
      &lt;td&gt;LibrePhotos&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Server" /><summary type="html">This is a list of ports for the Synology NAS Port Number Software / Application 2202/ubooquity Ubooquity User 2203 Ubooquity Admin names Synology 32400 Plex 8096 Jellyfin 8181 Tautulli 9000 Portainer 7080 Calibre 1643 wger 8003 Firefly III phpAdmin phpAdmin 8001 wger 3000 LibrePhotos</summary></entry><entry><title type="html">Installing Ubooquity on Synology NAS via Docker</title><link href="http://localhost:4000/ubooquity_synology.html" rel="alternate" type="text/html" title="Installing Ubooquity on Synology NAS via Docker" /><published>2020-10-30T00:00:00-04:00</published><updated>2020-10-30T00:00:00-04:00</updated><id>http://localhost:4000/ubooquity-synology</id><content type="html" xml:base="http://localhost:4000/ubooquity_synology.html">&lt;p&gt;This is a guide to install &lt;a href=&quot;&quot;&gt;Ubooquity&lt;/a&gt; on a Synology NAS. There hasn’t been many good instructions to install it, and for fools like me, it’s good to sometimes go step by step. Note that this method is to install it using Docker.&lt;/p&gt;

&lt;p&gt;First off, SSH into your server. Ideally, your server will be something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.1.XX&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &amp;lt;username&amp;gt;@&amp;lt;server-ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, figure out your PGID and PUID by running the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;username&amp;gt;@&amp;lt;server-ip&amp;gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;uid&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&amp;lt;username&amp;gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;gid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;gid&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&amp;lt;username&amp;gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, assuming you’ve already installed Docker from the Package Manager, go to the Docker directory, and create a directory called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubooquity&lt;/code&gt;. Here is where all of the config files will be located. Next, open up Docker, and in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Registry&lt;/code&gt; tab, look up &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linuxserver/ubooquity&lt;/code&gt;. Once you find it, install it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, start to create a container by clicking on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced Settings&lt;/code&gt; button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;General Settings&lt;/code&gt; tab, check the checkbox on “Enable auto-restart” (Optional)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Volume&lt;/code&gt; tab, you’ll need to create at least two folders: a configuration folder and a comics/raw/books folder. You should put the path to the Comics folder and set the mount path as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/comics&lt;/code&gt;, and the configuration folder with the mount path &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/config&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, you may wish to bring everything a level up in the directory. In order to do so, here’s an example that you may wish to use:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;File/Folder&lt;/th&gt;
      &lt;th&gt;Mount path&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;docker/ubooquity&lt;/td&gt;
      &lt;td&gt;/config&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;my_files/Comics/Marvel&lt;/td&gt;
      &lt;td&gt;/Marvel&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;my_files/Comics/DC&lt;/td&gt;
      &lt;td&gt;/DC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;my_files/Comics/Others&lt;/td&gt;
      &lt;td&gt;/Others&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And then, later, when you are as the administrator, you need to go to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Comics&lt;/code&gt; tab, and under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shared folders&lt;/code&gt;, click on the button &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADD FOLDER&lt;/code&gt;, and add the folders &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/Marvel&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/DC&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Others&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Port Settings&lt;/code&gt; tab, there will already be two local ports. In my experience, I wasn’t able to get it to work properly with the defaults, so I simply replaced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Auto&lt;/code&gt; with the same port number&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Environment&lt;/code&gt; tab, add in the variables &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PGID&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PUID&lt;/code&gt; with the values obtained from running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt; command above (i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;uid&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;gid&amp;gt;&lt;/code&gt; respectively)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/ubooquity_install/step7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, you’ll have created a container. Next, you’ll need to go to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;server-ip&amp;gt;:2203/ubooquity/admin&lt;/code&gt; to set up an admin password, and then it you’ll be able to see its settings. In order to access your server, go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;server-ip&amp;gt;:2202/ubooquity&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;accessing-the-server&quot;&gt;Accessing the Server&lt;/h2&gt;

&lt;h3 id=&quot;android---kuboo&quot;&gt;Android - Kuboo&lt;/h3&gt;

&lt;p&gt;You can access your server via different ways now. One way is &lt;a href=&quot;https://github.com/sethchhim/Kuboo&quot;&gt;Kuboo&lt;/a&gt; developed by Seth Chhim&lt;/p&gt;

&lt;p&gt;First, in the admin comfiguration, in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced&lt;/code&gt; tab, you need to check the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enable OPDS feed&lt;/code&gt;, which will allow your books and comics to be accessed to the server via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;server-ip&amp;gt;:2202/opds-books&lt;/code&gt; in the application on Android.&lt;/p&gt;

&lt;p&gt;In order to access it remotely, ideally you should have already set a external website. In many cases, it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;chosen-name&amp;gt;.synology.me&lt;/code&gt;. In your router, you need to &lt;a href=&quot;nicolasshu.com/port_forwarding_netgear.html&quot;&gt;allow for the ports to be forwarded&lt;/a&gt;. In this case, you need to forward the port 2202. Once that has been done, you should be able to access it via 
https://choosename.synology.me:2202/ubooquity&lt;/p&gt;

&lt;h3 id=&quot;ipad---chunky-comic-reader&quot;&gt;iPad - Chunky Comic Reader&lt;/h3&gt;

&lt;p&gt;You can access it via &lt;a href=&quot;https://apps.apple.com/pe/app/chunky-comic-reader/id663567628?l=en&quot;&gt;Chunky Comic Reader&lt;/a&gt; for the iPad&lt;/p&gt;

&lt;h2 id=&quot;alternative&quot;&gt;Alternative&lt;/h2&gt;

&lt;p&gt;If you have enabled Samba or FTP on your server, you can access your files directly (or via FTP) via the application &lt;a href=&quot;https://www.cdisplayex.com/&quot;&gt;CDisplay Ex&lt;/a&gt;. I always loved using the program [CDisplay] by David Ayton, but he passed away in 2003, where Derek Quenneville continued hosting his website. Then a guy (unnamed) continued his work with CDisplay Ex. He actually does a really good job in bringing back CDisplay, and he made an Android application. This is in fact my application of choice for reading via SMB.&lt;/p&gt;

&lt;p&gt;In order to access the files via Samba, you need to set one (or both) of the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Samba (for local access) - This is nice so that you don’t have to download anything
    &lt;ul&gt;
      &lt;li&gt;Description: Anything&lt;/li&gt;
      &lt;li&gt;Scheme: Samba&lt;/li&gt;
      &lt;li&gt;Host name: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;name-of-server&amp;gt;&lt;/code&gt; (e.g. Brainiac)&lt;/li&gt;
      &lt;li&gt;Username: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;username-to-server&amp;gt;&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Password: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;password&amp;gt;&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Path: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;path/to/comics&amp;gt;&lt;/code&gt; (e.g. /avatar/Comics/)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FTP (for remote access)
    &lt;ul&gt;
      &lt;li&gt;Description: Anything&lt;/li&gt;
      &lt;li&gt;Scheme: Ftp&lt;/li&gt;
      &lt;li&gt;Host name: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;name&amp;gt;.synology.me&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Username: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;username-to-server&amp;gt;&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Password: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;password&amp;gt;&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Path: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;path/to/comics&amp;gt;&lt;/code&gt; (e.g. /avatar/Comics/)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><category term="Server" /><summary type="html">This is a guide to install Ubooquity on a Synology NAS. There hasn’t been many good instructions to install it, and for fools like me, it’s good to sometimes go step by step. Note that this method is to install it using Docker. First off, SSH into your server. Ideally, your server will be something like 192.168.1.XX ssh &amp;lt;username&amp;gt;@&amp;lt;server-ip&amp;gt; Next, figure out your PGID and PUID by running the command id &amp;lt;username&amp;gt;@&amp;lt;server-ip&amp;gt;$ id uid=&amp;lt;uid&amp;gt;(&amp;lt;username&amp;gt;) gid=&amp;lt;gid&amp;gt;(&amp;lt;username&amp;gt;) Next, assuming you’ve already installed Docker from the Package Manager, go to the Docker directory, and create a directory called ubooquity. Here is where all of the config files will be located. Next, open up Docker, and in the Registry tab, look up linuxserver/ubooquity. Once you find it, install it. Next, start to create a container by clicking on the Advanced Settings button. Under the General Settings tab, check the checkbox on “Enable auto-restart” (Optional) Under the Volume tab, you’ll need to create at least two folders: a configuration folder and a comics/raw/books folder. You should put the path to the Comics folder and set the mount path as /comics, and the configuration folder with the mount path /config. Alternatively, you may wish to bring everything a level up in the directory. In order to do so, here’s an example that you may wish to use: File/Folder Mount path docker/ubooquity /config my_files/Comics/Marvel /Marvel my_files/Comics/DC /DC my_files/Comics/Others /Others And then, later, when you are as the administrator, you need to go to the Comics tab, and under Shared folders, click on the button ADD FOLDER, and add the folders /Marvel, /DC, and Others. Under the Port Settings tab, there will already be two local ports. In my experience, I wasn’t able to get it to work properly with the defaults, so I simply replaced Auto with the same port number Finally, under the Environment tab, add in the variables PGID and PUID with the values obtained from running the id command above (i.e. &amp;lt;uid&amp;gt; and &amp;lt;gid&amp;gt; respectively) Now, you’ll have created a container. Next, you’ll need to go to the &amp;lt;server-ip&amp;gt;:2203/ubooquity/admin to set up an admin password, and then it you’ll be able to see its settings. In order to access your server, go to &amp;lt;server-ip&amp;gt;:2202/ubooquity Accessing the Server Android - Kuboo You can access your server via different ways now. One way is Kuboo developed by Seth Chhim First, in the admin comfiguration, in the Advanced tab, you need to check the Enable OPDS feed, which will allow your books and comics to be accessed to the server via &amp;lt;server-ip&amp;gt;:2202/opds-books in the application on Android. In order to access it remotely, ideally you should have already set a external website. In many cases, it is &amp;lt;chosen-name&amp;gt;.synology.me. In your router, you need to allow for the ports to be forwarded. In this case, you need to forward the port 2202. Once that has been done, you should be able to access it via https://choosename.synology.me:2202/ubooquity iPad - Chunky Comic Reader You can access it via Chunky Comic Reader for the iPad Alternative If you have enabled Samba or FTP on your server, you can access your files directly (or via FTP) via the application CDisplay Ex. I always loved using the program [CDisplay] by David Ayton, but he passed away in 2003, where Derek Quenneville continued hosting his website. Then a guy (unnamed) continued his work with CDisplay Ex. He actually does a really good job in bringing back CDisplay, and he made an Android application. This is in fact my application of choice for reading via SMB. In order to access the files via Samba, you need to set one (or both) of the following: Samba (for local access) - This is nice so that you don’t have to download anything Description: Anything Scheme: Samba Host name: &amp;lt;name-of-server&amp;gt; (e.g. Brainiac) Username: &amp;lt;username-to-server&amp;gt; Password: &amp;lt;password&amp;gt; Path: &amp;lt;path/to/comics&amp;gt; (e.g. /avatar/Comics/) FTP (for remote access) Description: Anything Scheme: Ftp Host name: &amp;lt;name&amp;gt;.synology.me Username: &amp;lt;username-to-server&amp;gt; Password: &amp;lt;password&amp;gt; Path: &amp;lt;path/to/comics&amp;gt; (e.g. /avatar/Comics/)</summary></entry><entry><title type="html">Pytorch_dropout</title><link href="http://localhost:4000/2020/10/13/pytorch_dropout.html" rel="alternate" type="text/html" title="Pytorch_dropout" /><published>2020-10-13T00:00:00-04:00</published><updated>2020-10-13T00:00:00-04:00</updated><id>http://localhost:4000/2020/10/13/pytorch_dropout</id><content type="html" xml:base="http://localhost:4000/2020/10/13/pytorch_dropout.html">&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# %%
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# %%
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# %%
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Your Name</name><email>nicolas.s.shu[at]gmail[dot]com</email></author><summary type="html"># %% drop = nn.Dropout() x = torch.ones(1,10) # %% drop.train() drop(x) # %% drop.eval() drop(x)</summary></entry></feed>