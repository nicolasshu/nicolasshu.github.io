<!DOCTYPE html><html lang="en">
  <head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><title>Probabilistic Linear Discriminant Analysis - Nick Shu. A Fool in the Making</title>

<meta name="description" content="I need to note that a lot of this post was inspired by RaviSoji’s PLDA implementationLet us say that we have a training dataset $X \in \mathbb{R}^{N\times F}...">
<link rel="canonical" href="http://localhost:4000/plda_python.html"><link rel="alternate" type="application/rss+xml" title="Nick Shu. A Fool in the Making" href="/feed.xml"><!-- start favicons snippet, use https://realfavicongenerator.net/ --><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#fc4d50"><link rel="shortcut icon" href="/assets/favicon.ico">

<meta name="msapplication-TileColor" content="#ffc40d"><meta name="msapplication-config" content="/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff">
<!-- end favicons snippet --><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" ><!-- start custom head snippets -->

<!-- end custom head snippets -->
<script>(function() {
  window.isArray = function(val) {
    return Object.prototype.toString.call(val) === '[object Array]';
  };
  window.isString = function(val) {
    return typeof val === 'string';
  };

  window.hasEvent = function(event) {
    return 'on'.concat(event) in window.document;
  };

  window.isOverallScroller = function(node) {
    return node === document.documentElement || node === document.body || node === window;
  };

  window.isFormElement = function(node) {
    var tagName = node.tagName;
    return tagName === 'INPUT' || tagName === 'SELECT' || tagName === 'TEXTAREA';
  };

  window.pageLoad = (function () {
    var loaded = false, cbs = [];
    window.addEventListener('load', function () {
      var i;
      loaded = true;
      if (cbs.length > 0) {
        for (i = 0; i < cbs.length; i++) {
          cbs[i]();
        }
      }
    });
    return {
      then: function(cb) {
        cb && (loaded ? cb() : (cbs.push(cb)));
      }
    };
  })();
})();
(function() {
  window.throttle = function(func, wait) {
    var args, result, thisArg, timeoutId, lastCalled = 0;

    function trailingCall() {
      lastCalled = new Date;
      timeoutId = null;
      result = func.apply(thisArg, args);
    }
    return function() {
      var now = new Date,
        remaining = wait - (now - lastCalled);

      args = arguments;
      thisArg = this;

      if (remaining <= 0) {
        clearTimeout(timeoutId);
        timeoutId = null;
        lastCalled = now;
        result = func.apply(thisArg, args);
      } else if (!timeoutId) {
        timeoutId = setTimeout(trailingCall, remaining);
      }
      return result;
    };
  };
})();
(function() {
  var Set = (function() {
    var add = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (data[i] === item) {
          return;
        }
      }
      this.size ++;
      data.push(item);
      return data;
    };

    var Set = function(data) {
      this.size = 0;
      this._data = [];
      var i;
      if (data.length > 0) {
        for (i = 0; i < data.length; i++) {
          add.call(this, data[i]);
        }
      }
    };
    Set.prototype.add = add;
    Set.prototype.get = function(index) { return this._data[index]; };
    Set.prototype.has = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (this.get(i) === item) {
          return true;
        }
      }
      return false;
    };
    Set.prototype.is = function(map) {
      if (map._data.length !== this._data.length) { return false; }
      var i, j, flag, tData = this._data, mData = map._data;
      for (i = 0; i < tData.length; i++) {
        for (flag = false, j = 0; j < mData.length; j++) {
          if (tData[i] === mData[j]) {
            flag = true;
            break;
          }
        }
        if (!flag) { return false; }
      }
      return true;
    };
    Set.prototype.values = function() {
      return this._data;
    };
    return Set;
  })();

  window.Lazyload = (function(doc) {
    var queue = {js: [], css: []}, sources = {js: {}, css: {}}, context = this;
    var createNode = function(name, attrs) {
      var node = doc.createElement(name), attr;
      for (attr in attrs) {
        if (attrs.hasOwnProperty(attr)) {
          node.setAttribute(attr, attrs[attr]);
        }
      }
      return node;
    };
    var end = function(type, url) {
      var s, q, qi, cbs, i, j, cur, val, flag;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        s[url] = true;
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (cur.urls.has(url)) {
            qi = cur, val = qi.urls.values();
            qi && (cbs = qi.callbacks);
            for (flag = true, j = 0; j < val.length; j++) {
              cur = val[j];
              if (!s[cur]) {
                flag = false;
              }
            }
            if (flag && cbs && cbs.length > 0) {
              for (j = 0; j < cbs.length; j++) {
                cbs[j].call(context);
              }
              qi.load = true;
            }
          }
        }
      }
    };
    var load = function(type, urls, callback) {
      var s, q, qi, node, i, cur,
        _urls = typeof urls === 'string' ? new Set([urls]) : new Set(urls), val, url;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (_urls.is(cur.urls)) {
            qi = cur;
            break;
          }
        }
        val = _urls.values();
        if (qi) {
          callback && (qi.load || qi.callbacks.push(callback));
          callback && (qi.load && callback());
        } else {
          q.push({
            urls: _urls,
            callbacks: callback ? [callback] : [],
            load: false
          });
          for (i = 0; i < val.length; i++) {
            node = null, url = val[i];
            if (s[url] === undefined) {
              (type === 'js' ) && (node = createNode('script', { src: url }));
              (type === 'css') && (node = createNode('link', { rel: 'stylesheet', href: url }));
              if (node) {
                node.onload = (function(type, url) {
                  return function() {
                    end(type, url);
                  };
                })(type, url);
                (doc.head || doc.body).appendChild(node);
                s[url] = false;
              }
            }
          }
        }
      }
    };
    return {
      js: function(url, callback) {
        load('js', url, callback);
      },
      css: function(url, callback) {
        load('css', url, callback);
      }
    };
  })(this.document);
})();
</script><script>
  (function() {
    var TEXT_VARIABLES = {
      version: '2.2.6',
      sources: {
        font_awesome: 'https://use.fontawesome.com/releases/v5.0.13/css/all.css',
        jquery: 'https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js',
        leancloud_js_sdk: '//cdn.jsdelivr.net/npm/leancloud-storage@3.13.2/dist/av-min.js',
        chart: 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js',
        gitalk: {
          js: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.js',
          css: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.css'
        },
        valine: 'https://unpkg.com/valine/dist/Valine.min.js',
        mathjax: 'https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML',
        mermaid: 'https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js'
      },
      site: {
        toc: {
          selectors: 'h1,h2,h3'
        }
      },
      paths: {
        search_js: '/assets/search.js'
      }
    };
    window.TEXT_VARIABLES = TEXT_VARIABLES;
  })();
</script>
</head>
  <body>
    <div class="root" data-is-touch="false">
      <div class="layout--page js-page-root"><div class="page__main js-page-main page__viewport has-aside cell cell--auto">

      <div class="page__main-inner"><div class="page__header d-print-none"><header class="header"><div class="main">
      <div class="header__title">
        <div class="header__brand"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="24px" height="24px" viewBox="0 0 24 24">
<style type="text/css">
	.st0{fill:#515151;}
</style>
<path class="st0" d="M1.7,22.3c5.7-5.7,11.3-5.7,17,0c3.3-3.3,3.5-5.3,0.8-6c2.7,0.7,3.5-1.1,2.3-5.6s-3.3-5.2-6.3-2.1
	c3-3,2.3-5.2-2.1-6.3S7,1.8,7.7,4.6C7,1.8,5,2.1,1.7,5.3C7.3,11,7.3,16.7,1.7,22.3"/>
</svg>
<a title="Nick's Personal Website
" href="/">Nick Shu. A Fool in the Making</a></div><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></div><nav class="navigation">
        <ul><li class="navigation__item"><a href="/">Home</a></li><li class="navigation__item"><a href="/archive.html">Archive</a></li><li class="navigation__item"><a href="/about.html">About</a></li><li class="navigation__item"><a href="/cv.html">CV</a></li><li><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></li></ul>
      </nav></div>
  </header>
</div><div class="page__content"><div class ="main"><div class="grid grid--reverse">

              <div class="col-aside d-print-none js-col-aside"><aside class="page__aside js-page-aside"><div class="toc-aside js-toc-root"></div>
</aside></div>

              <div class="col-main cell cell--auto"><!-- start custom main top snippet -->

<!-- end custom main top snippet -->
<article itemscope itemtype="http://schema.org/Article"><div class="article__header"><header><h1>Probabilistic Linear Discriminant Analysis</h1></header><span class="split-space">&nbsp;</span>
          <a class="edit-on-github"
            title="Edit on Github"
            href="https://github.com/nicolasshu/nicolasshu.github.io/tree/master/_posts/2021-01-30-plda.md">
            <i class="far fa-edit"></i></a></div><meta itemprop="headline" content="Probabilistic Linear Discriminant Analysis"><div class="article__info clearfix"><ul class="left-col menu"><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=Python">Python</a>
            </li><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=Machine+Learning">Machine Learning</a>
            </li></ul><ul class="right-col menu"><li><i class="far fa-calendar-alt"></i> <span>Jan 30, 2021</span>
            </li></ul></div><meta itemprop="author" content="Your Name"/><meta itemprop="datePublished" content="2021-01-30T00:00:00-05:00">
    <meta itemprop="keywords" content="Python,Machine Learning"><div class="js-article-content"><div class="layout--article"><!-- start custom article top snippet -->

<!-- end custom article top snippet -->
<div class="article__content" itemprop="articleBody"><p>I need to note that a lot of this post was inspired by RaviSoji’s <a href="https://github.com/RaviSoji/plda">PLDA implementation</a></p>

<p>Let us say that we have a training dataset $X \in \mathbb{R}^{N\times F}$ and $y \in \mathbb{R}^N$, where $N$ is the number of examples in the whole dataset, $F$ is the number of features in the dataset, $K$ specifies the number of classes that there are, and $\boldsymbol{m} \in \mathbb{R}^F$ represents the mean vector of the entire dataset $X$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">"/home/data/MNIST"</span><span class="p">)</span>
<span class="n">ds</span><span class="p">.</span><span class="n">gz</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="c1">#.reshape(-1,28,28)
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">F</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>For a classification problem we will create several subspaces:</p>
<ul>
  <li>$\mathcal{D}$ represents the space where the data lives in</li>
  <li>$\mathcal{X}$ represents the subspace found via PCA where the data in $\mathcal{D}$ gets transformed to the subspace $\mathcal{X}$. Here, you may consider $\mathcal{X}$ to be the space of the preprocessed state</li>
  <li>$\mathcal{U}$ represents the subspace found via PLDA, where the data will be ultimately transformed to, which is described in detail by Ioffe 2006</li>
  <li>$\mathcal{U}_{model}$ represents the subspace within $\mathcal{U}$ which contains only the dimensions that are relevant to the problem</li>
</ul>

<p>So, in general, the data will first flow in the following fashion
\(\mathcal{D} \leftrightarrow \mathcal{X} \leftrightarrow \mathcal{U} \leftrightarrow \mathcal{U}_{model}\)</p>

<p>First, for a classification problem, we perform a PCA to reduce the dimensionality, and get rid of features that may not be very important. In other words, we will bring the data from $\mathcal{D}$ to $\mathcal{X}$. For this one needs to determine the number of components that one would want to take into account for in its subspace, and set that as the matrix rank. This can be predefined or not. If it is not defined, we compute the covariance matrices for each of the classes (i.e. a between-class covariance matrix and a within-class covariance matrix per class $k$). In other words</p>

<p>For each class</p>
<ol>
  <li>Compute the mean vectors $m_k \in \mathbb{R}^F$</li>
  <li>Compute the covariance matrix $\sigma_k \in \mathbb{R}^{F\times F}$</li>
  <li>Compute the between-class covariance matrix per class $S_{b,k} \in \mathbb{R}^{F\times F}$
 \(S_{b,k} = \frac{n_k}{N}\underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times 1}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{1\times F}}\)</li>
  <li>Compute the within-class covariance matrix per class $S_{w,k} \in \mathbb{R}^{F\times F}$
 \(S_{w,k} = \frac{n_k-1}{N} \odot \sigma_{k}\)</li>
</ol>

<p>Then compute the within-class and between-class covariance matrices, $S_w \in \mathbb{R}^{F\times F}$ and $S_b \in \mathbb{R}^{F\times F}$ respectively. If one was to set \(\boldsymbol{m}_{ks} \in \mathbb{R}^{K\times F}\) as the matrix representing all of the mean vectors, $\boldsymbol{\sigma_{ks} \in \mathbb{R}^{K\times F \times F}}$ the tensor representing all of the class covariances, and $n_{ks}\in \mathbb{R}^K$ a vector representing all of the number of examples in each class, it is possible to vectorize it all as</p>

\[\begin{align}
    S_b &amp;= \underbrace{\frac{n_{ks}}{N} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})^T}_{\mathbb{R}^{F\times K}}}_{\mathbb{R}^{F\times K}} \underbrace{(\boldsymbol{m}_k - \boldsymbol{m})}_{\mathbb{R}^{K\times F}} \\
    S_w &amp;= \sum_{k} S_{w,k} \\ 
    &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \odot \underbrace{\frac{n_k-1}{N}}_{\mathbb{R}^K} \\
    &amp;= \sum_{k} \underbrace{\sigma_{ks}}_{\mathbb{R}^{K\times F \times F}} \cdot \underbrace{\frac{n_k-1}{N}\text{[:, None, None]}}_{\mathbb{R}^{K\times 1 \times 1}}
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_principal_components</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># This might overfit
</span>
<span class="k">if</span> <span class="n">n_principal_components</span><span class="p">:</span>
    <span class="n">matrix_rank</span> <span class="o">=</span> <span class="n">n_principal_components</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">m_ks</span><span class="p">,</span> <span class="n">sigma_ks</span><span class="p">,</span> <span class="n">n_ks</span> <span class="o">=</span> <span class="p">[],[],[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Get only the data associated with class k
</span>        <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
        
        <span class="c1"># Compute the mean, number of samples, and class covariance
</span>        <span class="n">m_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>
        <span class="n">sigma_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Append them all
</span>        <span class="n">m_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">m_k</span><span class="p">)</span>
        <span class="n">n_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_k</span><span class="p">)</span>
        <span class="n">sigma_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_k</span><span class="p">)</span>
    <span class="n">m_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">m_ks</span><span class="p">)</span>
    <span class="n">n_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_ks</span><span class="p">)</span>
    <span class="n">sigma_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sigma_ks</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">m_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">n_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">sigma_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    
    <span class="n">S_b</span> <span class="o">=</span>  <span class="p">((</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">T</span> <span class="o">*</span> <span class="n">n_ks</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>  <span class="o">@</span> <span class="p">(</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">S_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_ks</span> <span class="o">*</span> <span class="p">((</span><span class="n">n_ks</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">matrix_rank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">S_w</span><span class="p">)</span>

<span class="k">if</span> <span class="n">F</span> <span class="o">!=</span> <span class="n">matrix_rank</span><span class="p">:</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">matrix_rank</span><span class="p">)</span>
    <span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, there are going to be several transformations:</p>
<ul>
  <li>$\mathcal{D} \rightarrow \mathcal{X}$</li>
</ul>

<blockquote>
  <p>Here, there are two case scenarios. If PCA was defined in order to reduce the dimensions, then the data in $\mathcal{D}$ will be transformed via PCA. Otherwise, you can return the data itself</p>
</blockquote>

<ul>
  <li>$\mathcal{X} \rightarrow \mathcal{D}$</li>
</ul>

<blockquote>
  <p>In this case, it is very similar to the converse. If PCA was defined, in order to bring it back to the original data space $\mathcal{D}$, you need to inverse transform the data. Otherwise, just return the data itself</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_D_to_X</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">pca</span>
    <span class="k">if</span> <span class="n">pca</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">transform_from_X_to_D</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">pca</span>
    <span class="k">if</span> <span class="n">pca</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>So, at this point, we convert the training data from $\mathcal{D}$ to the space $\mathcal{X}$, having the data be represented as $X_{pca}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_pca</span> <span class="o">=</span> <span class="n">transform_from_D_to_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of X_pca ="</span><span class="p">,</span><span class="n">X_pca</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Shape of X_pca = (60000, 712)
</span></code></pre></div></div>

<p>In the PLDA, we use a Gaussian mixture model, where $\boldsymbol{x}$ retpresents a sample in the mixture, and $\mathcal{y}$ represents the center of a mixture component. In general, the class-conditional distributions is represented by</p>

\[P(\boldsymbol{x} | \boldsymbol{y}) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{y}, \Phi_w)\]

<p>where all of the class-conditional distributions share one common covariance 
[I DON’T KNOW WHY THEY ALL SHARE THE SAME COVARIANCE]</p>

<p>If we recall, the <strong>LDA formulation</strong> is the result if we were to set $\mu_k$ values to be constrainted to be in a lower dimension, and perform the likelihood maximization with respect to $\mu_k$, $\pi_k$, and $\Phi_w$, where the priord of the class variable $\boldsymbol{y}$ is set to put a probability mass on each of the points</p>

\[P_{LDA}(\boldsymbol{y}) = \sum_{k=1}^{K} \pi_k \delta(\boldsymbol{y}-\mu_k)\]

<p>But that won’t be the case in the <strong>PLDA formulation</strong>. Instead, PLDA sets it so taht the prior is not to be within a discrete set of values, but instead, sampled from a Gaussian prior.</p>

\[P_{PLDA}(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{m}, \Phi_b)\]

<p>Note that this normal distribution $P_{PLDA}(y)$ uses the mean of the full dataset. This formulation makes it such that $\Phi_w$ is positive definite, and $\Phi_b$ is positive semi-definite. Theoretically, it is possible to find a transformation $V$ which can simultaneously diagonalize $\Phi_b$ and $\Phi_w$</p>

\[\begin{align}
    V^T \Phi_b V &amp;= \Psi \\
    V^T \Phi_w V &amp;= I
\end{align}\]

<p>We can define $A = V^{-T} = \text{inv}(V^T)$, resulting in</p>

\[\begin{align}
    \Phi_w &amp;= AA^T \\
    \Phi_b &amp;= A\Psi A^T
\end{align}\]

<p>Thus, the <strong>PLDA model</strong> is defined as:</p>

\[\bbox[teal, 4pt]{\begin{align}
\boldsymbol{x} &amp;= \boldsymbol{m} + A \boldsymbol{u} \quad \text{where} \\
&amp; \boldsymbol{u} \sim \mathcal{N}(\cdot | \boldsymbol{v}, I) \\
&amp; \boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)
\end{align}}\]

<p>Here, $\boldsymbol{u}$ represents the sample data representation of $\boldsymbol{x}$, but projected in the lated projected space $\mathcal{U}$, and $\boldsymbol{v}$ represents the sample label in the lated projected space. These transformations can be computed via</p>

\[\boldsymbol{x} = \boldsymbol{m} + A \boldsymbol{u} \quad \leftrightarrow \quad \boldsymbol{u} = V^T (\boldsymbol{x} - \boldsymbol{m})\]

\[\boldsymbol{y} = \boldsymbol{m} + A \boldsymbol{v} \quad \leftrightarrow \quad \boldsymbol{v} = V^T (\boldsymbol{y} - \boldsymbol{m})\]

<p>And from this point on, we determine the optimal $\boldsymbol{m}$, $A$, and $\Psi$ are.</p>

<p>S. Ioffe: “<em>In the training data, the grouping of examples into clusters is given, and we learn the model parameters by maximizing the likelihood. If, instead, the model parameters are fixed, likelihood maximization with respect to the class assignment labels solves a clustering problem</em>”</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_X_to_U</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">m</span><span class="p">,</span> <span class="n">A</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">m</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transform_from_U_to_X</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">m</span><span class="p">,</span> <span class="n">A</span>
    <span class="k">return</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>But, at this point, we don’t know what the parameters $A$ or $\Psi$ are, so we can’t use these functions yet.</p>

<h3 id="learning-the-model-parameters-boldsymbolm-psi-a">Learning the Model Parameters ($\boldsymbol{m}, \Psi, A$)</h3>
<p>The loading matrix $A$ is essentially finding the variances $\Phi_b$ and $\Phi_w$, and all of the parameters can be defined using a maximum likelihood framework. Let us say that $D_k$ represents the dataset which contains only samples from the $k^{th}$ class, and $\boldsymbol{x}_k^i$ represents the $i^{th}$ sample from $D_k$, and it belongs to the $k^{th}$ class. Given $N$ training examples separated into $K$ classes, and assuming that they are all independently drawm from their respective class, the log likelihood is</p>

\[l(\boldsymbol{x}^{1 \cdots N}) = \sum_{k=1}^K \ln P(\boldsymbol{x}^i : i \in D_k) = \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k)\]

<p>where the joint probability distribution of a set of $n$ patterns (assuming all these $n$ patterns belong to the same class $k$) is:</p>

\[\begin{align}
P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) &amp;= \int \color{red}{P(\boldsymbol{x}^1_k | \boldsymbol{y})} \cdots \color{cyan}{P(\boldsymbol{x}^n_k | \boldsymbol{y})} \color{magenta}{P(\boldsymbol{y})} d\boldsymbol{y} \\
&amp;= \int \color{red}{\mathcal{N}(\boldsymbol{x}^1_k | \boldsymbol{y}, \Phi_w)} \cdots \color{cyan}{\mathcal{N}(\boldsymbol{x}^n_k | \boldsymbol{y}, \Phi_w)} \color{magenta}{\mathcal{N}(\boldsymbol{y} | 0, \Phi_b)} d\boldsymbol{y}
\end{align}\]

<p>By computing the integral, we obtain</p>

\[\color{red}{MAGIC}\]

\[\ln P(\boldsymbol{x}^1_k,\cdots, \boldsymbol{x}^n_k) = C - \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} (\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T\right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} ( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)\right)\right]\]

<p>where \(\bar{\boldsymbol{x}}_k = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}^i_k\), and $C$ is a constant that can be ignored</p>

<p>At this point, as a “hack”, it sets the number of examples for each class to be $n$. In other words, every class ends up having exactly $n$ examples to learn from. Now, if one were to maximize the equation $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\boldsymbol{m}$, one would obtain $\boldsymbol{m}^* = \frac{1}{N} \sum_i \boldsymbol{x}^i$. If one substitutes it back, one would get</p>

\[\begin{align}
l(\boldsymbol{x}^{1\cdots N})
&amp;= \sum_{k=1}^K \ln P(\boldsymbol{x}^1_k, \cdots, \boldsymbol{x}^n_k) \\
&amp;= -\sum_{k=1}^K \frac{1}{2}\left[\ln |\Phi_b + \frac{\Phi_w}{n}| + tr\left((\Phi_b+ \frac{\Phi_w}{n})^{-1} \color{cyan}{(\bar{\boldsymbol{x}}_k-\boldsymbol{m})(\bar{\boldsymbol{x}}_k-\boldsymbol{m})^T}  \right) + (n-1) \ln |\Phi_w| + tr\left(\Phi_w^{-1} \color{red}{( \sum_{i=1}^n (\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)(\boldsymbol{x}^i_k - \bar{\boldsymbol{x}}_k)^T)} \right)  \right] \\
&amp;= \cdots \\
&amp;= \color{red}{\text{MAGIC}} \\
&amp;= \cdots \\
&amp;= - \frac{c}{2} \left[ \ln |\Phi_b + \frac{\Phi_w}{n} | + \text{tr} \left( (\Phi_b + \frac{\Phi_w}{n})^{-1} \color{cyan}{S_b} \right) + (n-1) \ln |\Phi_w | + n \text{tr} (\Phi_w^{-1} \color{red}{S_w}) \right]
\end{align}\]

<p>Now, we need to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Phi_b$ and $\Phi_w$ subject to $\Phi_w$ being p.d. and $\Phi_b$ being p.s.d. <strong>Without</strong> these constraints, we would obtain</p>

\[\Phi_w = \frac{n}{n-1} S_w \quad \text{and} \quad \Phi_b = S_b - \frac{1}{n-1} S_w\]

<p>Therefore, if $S_w$ and $S_b$ are diagonal, then the covariances $\Phi_w$ and $\Phi_b$ will also be diagonal, and the diagonalization property holds as long as the contraints above are satisfied.</p>

<p>As we have previously stated, we know that</p>

\[\Phi_b = A \Psi A^T\]

<p>If you fix $\Psi$ and maximize $l(\boldsymbol{x}^{1\cdots N})$ via unconstrained optimization with respect to $A^{-1}$ will make $A^{-1}S_b A^{-T}$ and $A^{-1}S_w A^{-T}$, making the $A^{-T}$ to be the solution of the generalized eigenvector problem involving $S_b$ and $S_w$, where $S_b \boldsymbol{v} = \lambda S_w \boldsymbol{v}$. Then, the projection of the data to the latent space with the LDA projection. [REVIEW]</p>

<p>Then, if you were to optimize $l(\boldsymbol{x}^{1\cdots N})$ with respect to $\Psi$ subject to $\Psi \geq 0$ and $\text{rank}(\Psi) \leq \hat{F}$, then we’ll get the method to optimize the model [REVIEW]</p>

<p>Ioffe 2006: “<em>Our method was derived for the case where each class in the training data is represented by the same number $n$ of examples. This may not be true in practice, in which case, we can resample the data to make the number of examples the same, use EM (as shown in section 5), or use approximations. We took the latter approach, using the closed-form solution in Fig. 2, where $n$ was taken to be the average number of examples per class</em>”</p>

<h2 id="algorithm-plda-optimization"><strong>Algorithm</strong>: PLDA Optimization</h2>
<blockquote>
  <p><strong><em>Input</em>:</strong> Training $N$ examples from $K$ classes, with $n = N/K$ per class<br />
<strong><em>Output</em>:</strong> Parameters $\boldsymbol{m}, A, \Psi$, maximizing the likelihood of the PLDA model</p>
</blockquote>

<blockquote>
  <ol>
    <li>Compute the covariance matrices $S_b$, and $S_w$</li>
    <li>Compute the transformation matrix $W$ such that $S_b \boldsymbol{w} = \lambda S_w \boldsymbol{w}$ (i.e. $eig(S_w^{-1}S_b)$)</li>
    <li>Compute the covariance matrices in the latent space
      <ul>
        <li>$\Lambda_b = W^T S_b W$</li>
        <li>$\Lambda_w = W^T S_w W$</li>
      </ul>
    </li>
    <li>Determine the following parameters
      <ul>
        <li>$\boldsymbol{m} = \frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$</li>
        <li>$A = W^{-T} \left( \frac{n}{n-1} \Lambda_w \right)^{1/2}$</li>
        <li>$\Psi = \max \left( 0, \frac{n-1}{n} (\Lambda_b / \Lambda_w) - \frac{1}{n} \right)$</li>
      </ul>
    </li>
    <li>Reduce the dimensionality to $\hat{F}$ by keeping the largest elements of $\Psi$, while setting the rest to zero.</li>
    <li>In the latent space $\boldsymbol{u} = A^{-1}(\boldsymbol{x}-\boldsymbol{m})$, only the features for non-zero entries are needed for recognition</li>
  </ol>
</blockquote>

<p>Note:</p>
<blockquote>
  <p>Scipy’s <code class="language-plaintext highlighter-rouge">eigh(A,B)</code> function in <code class="language-plaintext highlighter-rouge">linalg</code> solves the generalized eigenvalue problem for a complex Hermittian or a real symmetric matrix, so that $A\boldsymbol{v} = \lambda B \boldsymbol{v}$. Otherwise, if <code class="language-plaintext highlighter-rouge">B</code> is ommited, then it is assumed that $B=I$<br />
Also note that <code class="language-plaintext highlighter-rouge">scipy.linalg.eigh != np.linalg.eigh</code></p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_Sb_Sw</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span><span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">m_ks</span><span class="p">,</span> <span class="n">sigma_ks</span><span class="p">,</span> <span class="n">n_ks</span> <span class="o">=</span> <span class="p">[],[],[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Get only the data associated with class k
</span>        <span class="n">X_k</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
        
        <span class="c1"># Compute the mean, number of samples, and class covariance
</span>        <span class="n">m_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_k</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>
        <span class="n">sigma_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_k</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Append them all
</span>        <span class="n">m_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">m_k</span><span class="p">)</span>
        <span class="n">n_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_k</span><span class="p">)</span>
        <span class="n">sigma_ks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_k</span><span class="p">)</span>
    <span class="n">m_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">m_ks</span><span class="p">)</span>
    <span class="n">n_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_ks</span><span class="p">)</span>
    <span class="n">sigma_ks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sigma_ks</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">m_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">n_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">sigma_ks</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">F</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
    
    <span class="n">S_b</span> <span class="o">=</span>  <span class="p">((</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">).</span><span class="n">T</span> <span class="o">*</span> <span class="n">n_ks</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>  <span class="o">@</span> <span class="p">(</span><span class="n">m_ks</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">S_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_ks</span> <span class="o">*</span> <span class="p">((</span><span class="n">n_ks</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">N</span><span class="o">/</span><span class="n">K</span>
<span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span> <span class="o">=</span> <span class="n">compute_Sb_Sw</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute W
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">S_b</span><span class="p">,</span> <span class="n">S_w</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">eigvecs</span>

<span class="c1"># Compute Lambdas
</span><span class="n">Lambda_b</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_b</span> <span class="o">@</span> <span class="n">W</span>
<span class="n">Lambda_w</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_w</span> <span class="o">@</span> <span class="n">W</span>

<span class="c1"># Compute A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Lambda_w</span><span class="p">))</span><span class="o">**</span><span class="mf">0.5</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Compute Psi
</span><span class="n">diag_Lambda_w</span> <span class="o">=</span> <span class="n">Lambda_w</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span>
<span class="n">diag_Lambda_b</span> <span class="o">=</span> <span class="n">Lambda_b</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span>

<span class="n">Psi</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">diag_Lambda_b</span><span class="o">/</span><span class="n">diag_Lambda_w</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="n">Psi</span><span class="p">[</span> <span class="n">Psi</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Psi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Psi</span><span class="p">)</span>
</code></pre></div></div>

<p>From this point you can transform the data that is in the PCA subspace $\mathcal{X}$ to the $\mathcal{U}$ subspace, having the data be represented as $U$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span> <span class="o">=</span> <span class="n">transform_from_X_to_U</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, again, not every dimension will be relevant in the $\mathcal{U}$ subspace, and that is why we reduce the $\mathcal{U}$ to \(\mathcal{U}_{model}\) , which only contains the relevant dimensions of $\mathcal{U}$. Therefore, in order to go back and forth in between the two subspaces, simply drop them the irrelevant dimensions or add the relevant dimensions to a zero matrix. This new data will be represented as $U_{model}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform_from_U_to_Umodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dims</span><span class="p">):</span>
    <span class="n">u_model</span> <span class="o">=</span> <span class="n">u</span><span class="p">[...,</span><span class="n">dims</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">u_model</span> 

<span class="k">def</span> <span class="nf">transform_from_Umodel_to_U</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dims</span><span class="p">,</span><span class="n">u_dim</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">u_dim</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">u</span><span class="p">[...,</span> <span class="n">dims</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">u</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute the relevant dimensions of Psi
</span><span class="n">relevant_dims</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Psi</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="n">relevant_dims</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">relevant_dims</span> <span class="o">=</span> <span class="n">relevant_dims</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

<span class="n">U_model</span> <span class="o">=</span> <span class="n">transform_from_U_to_Umodel</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span><span class="n">relevant_dims</span><span class="p">)</span>
</code></pre></div></div>

<p>Great! Now we we have the data in the $\mathcal{U}_{model}$ space. Now, all that there is left is to understand how to perform inference, and in order to do that, one needs to find the prior parameters for $\boldsymbol{v}$, the posterior parameters, and the posterior predictive parameters.  Let’s discuss now how to find the probability parameters.</p>

<h2 id="inference-on-the-latent-space">Inference on the Latent Space</h2>

<p>First, if you need some review on Bayesian inference for Gaussian distributions, you may check the other <a href="https://nicolasshu.com/bayesian_inference_for_gaussian.html">post</a> to understand priors, posteriors, posterior predictives, and marginal probability. For this problem, note that the different dimensions have been decorrelated (i.e. the covariances have been decorrelated), thus the different dimensions could be treated as univariate problems.</p>

<p>We need to determing the prior parameters of $\boldsymbol{v}$, which leads to the probability distribution $P(\boldsymbol{v})$, the posterior parameters, which leads to \(P(\boldsymbol{v} \vert \boldsymbol{u})\) , and the posterior predictive parameters, for \(P(\boldsymbol{u}^p \vert \boldsymbol{u}^g_{1\cdots n})\)</p>

<p>The easiest to determine right off the bat are the prior parameters. For the prior parameters, as one may recall in the model formulation</p>

\[\boldsymbol{v} \sim \mathcal{N}(\cdot | 0, \Psi)\]

<p>which, in turn, are simple to compute. Here we’ll call $\mu_{prior}$ the prior mean, and $\Sigma_{prior}$ the prior covariance, making</p>

\[\bbox[teal, 4pt]{\begin{align}
\mu^{prior} &amp;= \boldsymbol{0} \\
\Sigma^{prior} &amp;= \Psi_{\forall d \in D}
\end{align}}\]

<p>where $D$ represents all of the relevant dimensions, which are all that the variances are not zero. Then from this point, we’ll use the notation setting $\hat{\Psi} = \Psi_{\forall d \in D}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"mean"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">relevant_dims</span><span class="p">),</span>
    <span class="s">"cov"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Psi</span><span class="p">)[</span><span class="n">relevant_dims</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now, the more involved ones are the posteriors parameters.</p>

<p><strong>One advantage that PLDA has is that it allows one to make inferences about classes not present during training.</strong> Let us consider the following case of classification first. We are given a set of data to learn from, which Ioffe refers to as a “gallery”. This set ${ \boldsymbol{x}^1, \cdots,\boldsymbol{x}^k, \cdots, \boldsymbol{x}^K  }$ contains $K$ examples, with one example from each of the $K$ classes. We are also given a probe example $\boldsymbol{x}^p$, and assume that it belongs to one of the $K$ classes. If we are to determine to which class it belongs, maximizing the likelihood will do the job. This can be more easily accomplised in the lated space by performing the trnasformation $\boldsymbol{u} = A^{-1}(\boldsymbol{x} - \boldsymbol{m})$, since it will decorrelate the data. For this example, $\boldsymbol{x}^p$ will be transformed to $\boldsymbol{u}^p$</p>

<h3 id="single-training-example-per-class">Single Training Example per Class</h3>
<p>Let us consider an example $\boldsymbol{u}^g$ from the training set (i.e. gallery), where, again, it belongs to some class between $1 \cdots K$ The probability that the probe $\boldsymbol{u}^p$ belongs to the same class as $\boldsymbol{u}^g$ is defined by the probability $P(\boldsymbol{u}^p | \boldsymbol{u}^g)$.</p>

<p>So, the posterior probability will provide a way to perform inference on the class variable $\boldsymbol{v}$ (i.e. the transformed version of $\boldsymbol{y}$). As we may remember, the parameters for a posterior Gaussian are</p>

\[\begin{align}
\mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Now, since we only have a single sample $\bar{x} = \boldsymbol{u}$. Additionally, we know that the mean for the prior $\mu^{prior}$ is all zeros (since the data has been centralized), then</p>

\[\begin{align}
\mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Additionally, since we would be looking at a single class at this point and the covariances have been diagonalized, the within-class covariance is an identity matrix, making the $(\sigma^{prior})^2 = 1$. Therefore the parameters turn into</p>

\[\begin{align}
\mu^{post} &amp;= \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{1(\sigma^{prior})^2}{N(\sigma^{prior})^2 + 1}
\end{align}\]

<p>And, once again, since we are dealing with a single sample, $N=1$</p>

\[\begin{align}
\mu^{post} &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1} \boldsymbol{u} \\
(\sigma^{post})^2 &amp;= \frac{(\sigma^{prior})^2}{(\sigma^{prior})^2 + 1}
\end{align}\]

<p>The posterior can then be defined as</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}) = \mathcal{N}\left(\boldsymbol{v} \bigg| \frac{\hat{\Psi}}{\hat{\Psi} + I}\boldsymbol{u}, \frac{\hat{\Psi}}{\hat{\Psi} + I}\right)}\]

<p>Now, if we see how this flows, the class variable $\boldsymbol{v}$ will be used to determine $\boldsymbol{u}$ examples, which are then used to determine the data $\boldsymbol{x}$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">()</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'v'</span><span class="p">,</span><span class="s">"v"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'1'</span><span class="p">,</span><span class="s">"u1"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">'2'</span><span class="p">,</span><span class="s">"u2"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">"a"</span><span class="p">,</span><span class="s">"x1"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span><span class="s">"x2"</span><span class="p">)</span>
<span class="n">dot</span><span class="p">.</span><span class="n">edges</span><span class="p">([</span><span class="s">'v1'</span><span class="p">,</span><span class="s">"v2"</span><span class="p">,</span><span class="s">"1a"</span><span class="p">,</span><span class="s">"2b"</span><span class="p">])</span>
<span class="n">dot</span>
</code></pre></div></div>
<p><img src="assets/images/lda_plda/v_u_x.png" alt="drawing" align="middle" style="width:300px;" /></p>

<p>From probabilistic graphical models, if we observe $\boldsymbol{v}$ (i.e. $\boldsymbol{v}$ is given), then $\boldsymbol{u}^p$ and $\boldsymbol{u}^g$ are conditionally independent. We also know that the posterior predictive probability is</p>

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu^{post}, (\sigma^{post})^2 + \sigma^2)\]

<p>And since the within class variance has been diagonalized to an identity matrix ($\sigma^2 = 1$), we’ll obtain</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}^g) = \mathcal{N}\left(\boldsymbol{u}^p \bigg| \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \boldsymbol{u}^g, I + \dfrac{\hat{\Psi}}{\hat{\Psi} + I} \right)}\]

<p>In order to classify $\boldsymbol{u}^p$, then we compute $P(\boldsymbol{u}^p \vert \boldsymbol{u}^g) \forall g \in {1,\cdots,M }$, and pick the maximum.</p>

<p>Ioffe 2006: “<em>With PLDA, we were able to combine the knowledge about the general structure of the data, obtained during training, and the examples of new classes, yielding a principled way to perform classification</em>”</p>

<h3 id="multiple-training-examples-per-class">Multiple Training Examples Per Class</h3>
<p>We can improve the recognition performance by using more examples. Let us say that we have $n_k$ examples from class $k$, making</p>

\[n_k = |U_{model,k}|\]

<p>These examples are all independent examples $\boldsymbol{u}_{1\cdots n}^g$. Just as before, we know that, here, we are looking at a single class $k$. We have previously diagonalized all of the covariance matrices, making all of the dimensions (i.e. features) decorrelated, thus they can be worked on individually as if they were univariate features. This also means that the within-class covariance is an identity matrix, making $\sigma^2 = 1$. As we may remember, the parameters for a posterior Gaussian are</p>

\[\begin{align}
\mu^{post} &amp;= \frac{\sigma^2}{N(\sigma^{prior})^2 + \sigma^2}\mu^{prior} + \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Again, since our model estimates that the prior mean $\mu^{prior}$ is all zeros, then</p>

\[\begin{align}
\mu^{post} &amp;=  \frac{N(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2} \bar{x} \\
(\sigma^{post})^2 &amp;= \frac{\sigma^2(\sigma^{prior})^2}{N(\sigma^{prior})^2 + \sigma^2}
\end{align}\]

<p>Since we’re looking at the class $k$, then $N=n_k$ and $\bar{x} = \bar{\boldsymbol{u}}_k$. Thus the posterior for multiple samples is</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{v} | \boldsymbol{u}_k^{1\cdots n_k} ) = \mathcal{N}\left(\boldsymbol{v} \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi}+I}\bar{\boldsymbol{u}}_k, \dfrac{\hat{\Psi}}{n_k \hat{\Psi}+I} \right) }\]

<p>Therefore, in order to compute the posterior parameters, where $\mu_k^{post}$ and $\Sigma_k^{post}$ are the mean posterior and the covariance posterior for each class $k$, we have</p>

\[\bbox[teal, 4pt]{\begin{align}
\Sigma^{post}_k &amp;= \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = \Sigma^{prior} \odot \frac{1}{1 + n_k \cdot \Sigma^{prior}} \\
\mu_k^{post} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \underbrace{\left( \sum_{\boldsymbol{u} \in U_{model,k}} \boldsymbol{u} \right)}_{n_k \bar{\boldsymbol{u}}_k} \cdot \Sigma^{post}_k 
\end{align}}\]

<p>where we can recall that $\Sigma^{prior} = \hat{\Psi}$, and $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posterior_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">u_model_k</span> <span class="o">=</span> <span class="n">u_model</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">k</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_model_k</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">n_k</span> <span class="o">*</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">])</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u_model_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cov</span>
    <span class="n">posterior_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s">"mean"</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s">"cov"</span><span class="p">:</span><span class="n">cov</span><span class="p">}</span>
</code></pre></div></div>

<p>As per page 535 by Ioffe 2006, if one sets multiple examples of a class to a single model, assuming we have $n_k$ independent examples \(\{ \boldsymbol{u}_k^i \}_{i=1}^{n_k}\), then the probability of obtaining a sample $\boldsymbol{u}^p$, given the set above, can be obtained from the posterior predictive</p>

\[P(x|\boldsymbol{X}) = \mathcal{N} (x|\mu_N, \sigma_N^2 + \sigma^2)\]

<p>Once again, since the within-class covariance has been diagonalized to an identity matrix, then $\sigma^2 = 1$</p>

\[\bbox[teal, 4pt]{P(\boldsymbol{u}^p | \boldsymbol{u}_k^{1\cdots n_k}) = P(\boldsymbol{u}^p | \boldsymbol{u}_k^1, \cdots, \boldsymbol{u}_k^{n_k}) = \mathcal{N} \left( \boldsymbol{u}^p \bigg| \dfrac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k, I + \dfrac{\hat{\Psi}}{n_k \hat{\Psi} + I} \right) }\]

<p>where $\bar{\boldsymbol{u}}_k = \frac{1}{n_k}(\boldsymbol{u}_k^1 + \cdots + \boldsymbol{u}_k^{n_k})$</p>

<p>This means that, you may compute the predictive parameters as copying the posterior parameters ($\mu_k^{postpred}$, $\Sigma_k^{postpred}$), and adding an identity matrix to the covariance</p>

\[\bbox[teal, 4pt]{\begin{align}
\mu_k^{postpred} &amp;= \frac{n_k \hat{\Psi}}{n_k \hat{\Psi} + I} \bar{\boldsymbol{u}}_k = \mu_k^{post} \\
\Sigma_k^{postpred} &amp;= I + \frac{\hat{\Psi}}{n_k \hat{\Psi} + I} = I + \Sigma_k^{post}
\end{align}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">post_pred_params</span> <span class="o">=</span> <span class="n">posterior_params</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">params</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>Great! This is all that we need to precompute before performing any inference! At this point, you have determined all of the parameters which describe you data. As a review, in order to prepare this algorithm, we have received a training dataset $X$ and its corresponding labels $y$. Then, we have decided on a certain number of components to be used for a PCA. This PCA will bring the data from the space $\mathcal{D}$ to the space $\mathcal{X}$, obtaining the data $X_{pca}$. Then we found the parameters $\boldsymbol{m}$, $A$, and $\hat{\Psi}$, which optimize the PLDA formulation. These would then allow us to bring the data from the $\mathcal{X}$ subspace to the $\mathcal{U}$ latent space, obtaining the data $U$. Then, we reduced the dimensions of $U$ by discarding any dimensions which had a zero variance in $\hat{\Psi}$, yielding the data $U_{model}$. Finally, using these same parameters, we obtain the prior parameters ($\mu^{prior}$ and $\Sigma^{prior}$), the posterior parameters ($\mu^{post}_k$ and $\Sigma^{post}_k$), and the posterior predictive parameters ($\mu^{postpred}_k$ and $\Sigma^{postpred}_k$)</p>

<p>Now, we’re ready to do the inference on the latent space</p>

<p>We have previously established that we have the classes (i.e. categories) $1,\cdots, K$. What we will do is to iterate through each of the possible classes, and compute its posterior probabilities by using the parameters computed for the posterior predictive probabilities and creating a Gaussian distribution. Finally, we can obtain the probability on that Gaussian at each of those locations. We can also use the log probability for each of the samples on each of the classes</p>

\[\bbox[teal, 4pt]{\begin{align}
P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k) &amp;= \mathcal{N}(\boldsymbol{u}_{model} | \mu_k^{postpred}, \Sigma_k^{postpred}) \\ 
\boldsymbol{y}^* &amp;= \arg\max_{k} P_k(\boldsymbol{u}^p | \boldsymbol{u}^{1\cdots n_k}_k)
\end{align}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span> <span class="k">as</span> <span class="n">gaussian</span>
<span class="n">log_prob_post</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">mean</span><span class="p">,</span><span class="n">cov</span> <span class="o">=</span> <span class="n">param</span><span class="p">[</span><span class="s">"mean"</span><span class="p">],</span> <span class="n">param</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span>
    <span class="n">log_probs_k</span> <span class="o">=</span> <span class="n">gaussian</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">)).</span><span class="n">logpdf</span><span class="p">(</span><span class="n">U_model</span><span class="p">)</span>
    <span class="n">log_prob_post</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_probs_k</span><span class="p">)</span>
<span class="n">log_prob_post</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_prob_post</span><span class="p">).</span><span class="n">T</span>
</code></pre></div></div>

<p>Here, you may choose to normalize the probabilities</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob_post</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_prob_post</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_prob_post</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categories</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">post_pred_params</span><span class="p">.</span><span class="n">keys</span><span class="p">()])</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">categories</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">count</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        <span class="n">title</span> <span class="o">=</span> <span class="s">"True: {} | Pred: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">count</span><span class="p">],</span><span class="n">predictions</span><span class="p">[</span><span class="n">count</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">count</span><span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>If you wanted to extract the LDA features, you could simply use the transformation functions to convert the data from some space to another space.</p>

<p>So this is the case when we have a classification problem where the probes are assumed to belong to one of the trained classes. Now, let us look at the case where the probes belong to classes not yet seen.</p>

<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<p>Here, we try to determine whether two samples belong to the same class or not. For that, we can compute the following likelihoods</p>

\[\begin{align}
P(\boldsymbol{u}^p)P(\boldsymbol{u}^g) &amp;= \text{likelihood of examples belonging to different classes} \\
P(\boldsymbol{u}^p, \boldsymbol{u}^g) &amp;= \int P(\boldsymbol{u}^p | \boldsymbol{v}) P(\boldsymbol{u}^g|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\ 
&amp;= \text{likelihood of examples belonging to the same class}
\end{align}\]

<p>As a generalized formulation where there are multiple examples, the likelihood ratio is</p>

\[\begin{align}
R(\{\boldsymbol{u}^{1\cdots m}_p\},\{\boldsymbol{u}^{1\cdots n}_g\}) &amp;= \frac{\text{likelihood(same)}}{\text{likelihood(diff)}} = \frac{P(\boldsymbol{u}^{1\cdots m}_p,\boldsymbol{u}^{1\cdots n}_g)}{P(\boldsymbol{u}^{1\cdots m}_p)P(\boldsymbol{u}^{1\cdots n}_g)} \\
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;= \int P(\boldsymbol{u}^1 | \boldsymbol{v}) \cdots P(\boldsymbol{u}^n|\boldsymbol{v}) P(\boldsymbol{v}) d\boldsymbol{v} \\
&amp;= \prod_{t=1}^d \frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} \exp \left( - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} - \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} \right)
\end{align}\]

<p>where</p>

\[\bar{u}_t = \frac{1}{n}\sum_{i=1}^n u_t^i\]

<p>For the priors $\pi_{\text{same}}$ and $\pi_{\text{diff}}$, the probability that all of the examples are in the same class is</p>

\[\left(1 + \dfrac{\pi_{\text{diff}} / \pi_{\text{same}} }{R} \right)^{-1} = \dfrac{R}{R+\pi_{\text{diff}} / \pi_{\text{same}}}\]

<p><span style="color:red">
If $R &gt; \pi_{\text{diff}} / \pi_{\text{same}}$, the two groups of examples belong to the same class; otherwise they do not.</span></p>

<p>The between-class feature variances $\psi_t$ indicate how discriminative the features are. For example, if $\psi=0$, then it is a completely non-discriminative feature.</p>

<p>Therefore, we can compute the marginal likelihoods for each of those possibilities:</p>
<ul>
  <li>Marginal probability of them being from same class, i.e. $P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n})$
    <blockquote>
      <p>Note that, for the marginal probability that they are from the same class, we will are treating it as a single marginal probability
\(P(\boldsymbol{u}_p^{1\cdots m},\boldsymbol{u}_g^{1\cdots n}) =P(\underbrace{\boldsymbol{u}_p^1, \cdots, \boldsymbol{u}_p^m}_{\boldsymbol{u}_p^{1\cdots m}},\underbrace{\boldsymbol{u}_g^1, \cdots, \boldsymbol{u}_g^n}_{\boldsymbol{u}_g^{1\cdots n}})\)</p>
    </blockquote>
  </li>
  <li>Marginal probability of them being from different classes, i.e. $P(\boldsymbol{u}_p^{1\cdots m}) \cdot P(\boldsymbol{u}_g^{1\cdots n})$</li>
</ul>

<p>By taking the logarithmic, we can can more easily deal with infinitesimal probabilities and helps us make multiplications into additions. Therefore, let us consider</p>

\[\begin{align}
P(\boldsymbol{u}^{1\cdots n}) = P(\boldsymbol{u}^1, \boldsymbol{u}^2, \cdots, \boldsymbol{u}^n) &amp;= \prod_{t=1}^d \underbrace{\frac{1}{\sqrt{(2\pi)^n (\psi_t + \frac{1}{n})}} }_{C} \exp \left( \underbrace{- \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})}}_{E_1} \underbrace{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }_{E_2} \right) \\
&amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
\log (P(\boldsymbol{u}^{1\cdots n})) &amp;= \sum_{t=1}^d \color{red}{\log (C)} +\color{cyan}{ \log(e^{E_1})} + \color{magenta}{\log(e^{E_2})} \\
&amp;= \sum_{t=1}^d \color{red}{\log (C)} + \color{cyan}{E_1} + \color{magenta}{E_2} \\
&amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(\psi_t + \frac{1}{n} \right)} \color{cyan}{ - \frac{\bar{u}_t^2}{2(\psi_t + \frac{1}{n})} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} } \\
&amp;= \sum_{t=1}^d \color{red}{-\frac{n}{2} \log (2\pi) - \frac{1}{2} \log \left(n\psi_t + 1 \right) + \frac{1}{2} \log(n)} \color{cyan}{ - \frac{n \bar{u}_t^2}{2(n\psi_t + 1)} } \color{magenta}{- \frac{\sum_{i=1}^n (u_t^i - \bar{u}_t)^2}{2} }
\end{align}\]

<blockquote>
  <p>It is ideal to work in the logarithmic space. With some magic, we get</p>
</blockquote>

<blockquote>
\[\color{red}{\text{Can't get my math to align with this}}\]
</blockquote>

<blockquote>
\[\begin{align}
P(\boldsymbol{u}^{1\cdots n}) &amp;= \prod_{t=1}^d C \exp (E_1 + E_2) \\ 
\log (C) &amp;= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log(n\Sigma^{prior} + I) \\
\log (e^{E_1}) &amp;= E_1 = \frac{n^2 \Sigma^{prior} \bar{\boldsymbol{u}}^2}{2 (n\Sigma^{prior} + I)} \\
\log (e^{E_2}) &amp;= E_2 = -\frac{1}{2} \sum_{\boldsymbol{u} \in U_{model}} \boldsymbol{u}^2 \\
\log(P(\boldsymbol{u}^{1\cdots n})) &amp;= \sum_{t=1}^d \log(C) + E_1 + E_2 \\ 
\end{align}\]
</blockquote>

<p>Now, note that because the data in $U_{model}$ has been normalized $mean(U_{model}) = \boldsymbol{0}$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">marginal_logprob</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">S_prior</span> <span class="o">=</span> <span class="n">prior_params</span><span class="p">[</span><span class="s">"cov"</span><span class="p">]</span>
    <span class="n">log_C</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">S_prior</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">E1</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">S_prior</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">S_prior</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">E2</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">U</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">logP_t</span> <span class="o">=</span> <span class="n">log_C</span> <span class="o">+</span> <span class="n">E1</span> <span class="o">+</span> <span class="n">E2</span> 
    <span class="n">logP</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">logP_t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logP</span>
</code></pre></div></div>

<p>This way, if we were to receive two sets of data</p>

\[\begin{align}
\boldsymbol{u}_p &amp;\in \mathbb{R}^{m\times \hat{F}} \\
\boldsymbol{u}_g &amp;\in \mathbb{R}^{n\times \hat{F}}
\end{align}\]

<p>We can set a set by concatenating both of them
\(\boldsymbol{u}_{pg} \in \mathbb{R}^{(m+n) \times \hat{F}}\)</p>

<p>And then we can pass them through the computation above</p>

\[\begin{align}
log(P(\boldsymbol{u}_p^{1\cdots m}) &amp;= \text{log likelihood for probe set} \\ 
log(P(\boldsymbol{u}_g^{1\cdots n}) &amp;= \text{log likelihood for gallery set} \\ 
log(P(\boldsymbol{u}_{pg}^{1\cdots m+n}) &amp;= \text{log likelihood for combined set}\\ 
\end{align}\]

<p>Finally, instead of the ratio, we compute the log of the ratio</p>

\[\log(R) = log[P(\boldsymbol{u}_{pg}^{1\cdots m+n})] - [log(P(\boldsymbol{u}_p^{1\cdots m}) + log(P(\boldsymbol{u}_g^{1\cdots n})]\]

<p>In such case, since we are dealing with log ratios, negative values mean that the model believes the two datapoints are from different categories, where as positive values indicate that the model believes that the two data points are from the same category.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logprob_p</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">u_p</span><span class="p">)</span>
<span class="n">logprob_g</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">u_g</span><span class="p">)</span>
<span class="n">logprob_pg</span> <span class="o">=</span> <span class="n">marginal_logprob</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">u_p</span><span class="p">,</span><span class="n">u_g</span><span class="p">]))</span>
<span class="n">log_Ratio</span> <span class="o">=</span> <span class="n">logprob_pg</span> <span class="o">-</span> <span class="p">(</span><span class="n">logprob_p</span> <span class="o">+</span> <span class="n">logprob_g</span><span class="p">)</span>

<span class="k">if</span> <span class="n">log_Ratio</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Belong to the same class"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Belong to different classes"</span><span class="p">)</span>
</code></pre></div></div>
</div><section class="article__sharing d-print-none"></section><div class="d-print-none"><footer class="article__footer"><meta itemprop="dateModified" content="2021-01-30T00:00:00-05:00"><!-- start custom article footer snippet -->

<!-- end custom article footer snippet -->
<div class="article__subscribe"><div class="subscribe"><i class="fas fa-rss"></i> <a type="application/rss+xml" href="/feed.xml">Subscribe</a></div>
</div><div class="article__license"><div class="license">
    <p>This work is licensed under a <a itemprop="license" rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International</a> license.
      <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">
        <img alt="Attribution-NonCommercial 4.0 International" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
      </a>
    </p>
  </div></div></footer>
<div class="article__section-navigator clearfix"><div class="previous"><span>PREVIOUS</span><a href="/lda_python.html">Linear Discriminant Analysis</a></div><div class="next"><span>NEXT</span><a href="/bayesian_inference_for_gaussian.html">Bayesian Inference on Gaussian Distributions</a></div></div></div>

</div>

<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    $(function() {
      var $this ,$scroll;
      var $articleContent = $('.js-article-content');
      var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
      var scroll = hasSidebar ? '.js-page-main' : 'html, body';
      $scroll = $(scroll);

      $articleContent.find('.highlight').each(function() {
        $this = $(this);
        $this.attr('data-lang', $this.find('code').attr('data-lang'));
      });
      $articleContent.find('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]').each(function() {
        $this = $(this);
        $this.append($('<a class="anchor d-print-none" aria-hidden="true"></a>').html('<i class="fas fa-anchor"></i>'));
      });
      $articleContent.on('click', '.anchor', function() {
        $scroll.scrollToAnchor('#' + $(this).parent().attr('id'), 400);
      });
    });
  });
})();
</script>
</div><section class="page__comments d-print-none"></section></article><!-- start custom main bottom snippet -->

<!-- end custom main bottom snippet -->
</div>
            </div></div></div><div class="page__footer d-print-none">
<footer class="footer py-4 js-page-footer">
  <div class="main"><div itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Your Name"><meta itemprop="url" content="/"><div class="footer__author-links"><div class="author-links">
  <ul class="menu menu--nowrap menu--inline"><li title="Send me an Email.">
      <a class="button button--circle mail-button" itemprop="email" href="mailto:nicolas.s.shu[at]gmail[dot]com" target="_blank">
        <i class="fas fa-envelope"></i>
      </a><li title="Follow me on Twitter.">
        <a class="button button--circle twitter-button" itemprop="sameAs" href="https://twitter.com/shunicolas" target="_blank">
          <div class="icon"><svg fill="#000000" width="24px" height="24px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M1024.032 194.432c-37.664 16.704-78.176 28-120.672 33.088 43.36-26.016 76.672-67.168 92.384-116.224-40.608 24.064-85.568 41.568-133.408 50.976-38.336-40.832-92.928-66.336-153.344-66.336-116.032 0-210.08 94.048-210.08 210.08 0 16.48 1.856 32.512 5.44 47.872-174.592-8.768-329.408-92.416-433.024-219.52-18.08 31.04-28.448 67.104-28.448 105.632 0 72.896 37.088 137.184 93.472 174.88-34.432-1.088-66.816-10.528-95.168-26.272-0.032 0.864-0.032 1.76-0.032 2.656 0 101.792 72.416 186.688 168.512 205.984-17.632 4.8-36.192 7.36-55.36 7.36-13.536 0-26.688-1.312-39.52-3.776 26.72 83.456 104.32 144.192 196.256 145.888-71.904 56.352-162.496 89.92-260.928 89.92-16.96 0-33.664-0.992-50.112-2.944 92.96 59.616 203.392 94.4 322.048 94.4 386.432 0 597.728-320.128 597.728-597.76 0-9.12-0.192-18.176-0.608-27.168 41.056-29.632 76.672-66.624 104.832-108.736z" />
</svg>
</div>
        </a>
      </li><li title="Follow me on Linkedin.">
        <a class="button button--circle linkedin-button" itemprop="sameAs" href="https://www.linkedin.com/in/nicolasshu" target="_blank">
          <div class="icon"><svg fill="#000000" width="24px" height="24px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M260.096 155.648c0 27.307008-9.899008 50.516992-29.696 69.632-19.796992 19.115008-45.396992 28.672-76.8 28.672-30.036992 0-54.612992-9.556992-73.728-28.672-19.115008-19.115008-28.672-42.324992-28.672-69.632 0-28.672 9.556992-52.224 28.672-70.656 19.115008-18.432 44.372992-27.648 75.776-27.648 31.403008 0 56.32 9.216 74.752 27.648 18.432 18.432 28.331008 41.984 29.696 70.656 0 0 0 0 0 0m-202.752 808.96c0 0 0-632.832 0-632.832 0 0 196.608 0 196.608 0 0 0 0 632.832 0 632.832 0 0-196.608 0-196.608 0 0 0 0 0 0 0m313.344-430.08c0-58.708992-1.364992-126.292992-4.096-202.752 0 0 169.984 0 169.984 0 0 0 10.24 88.064 10.24 88.064 0 0 4.096 0 4.096 0 40.96-68.267008 105.812992-102.4 194.56-102.4 68.267008 0 123.220992 22.868992 164.864 68.608 41.643008 45.739008 62.464 113.664 62.464 203.776 0 0 0 374.784 0 374.784 0 0-196.608 0-196.608 0 0 0 0-350.208 0-350.208 0-91.476992-33.451008-137.216-100.352-137.216-47.787008 0-81.236992 24.576-100.352 73.728-4.096 8.192-6.144 24.576-6.144 49.152 0 0 0 364.544 0 364.544 0 0-198.656 0-198.656 0 0 0 0-430.08 0-430.08 0 0 0 0 0 0" />
</svg>
</div>
        </a>
      </li><li title="Follow me on Github.">
        <a class="button button--circle github-button" itemprop="sameAs" href="https://github.com/nicolasshu" target="_blank">
          <div class="icon"><svg fill="#000000" width="24px" height="24px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path class="svgpath" data-index="path_0" fill="#272636" d="M0 525.2c0 223.6 143.3 413.7 343 483.5 26.9 6.8 22.8-12.4 22.8-25.4l0-88.7c-155.3 18.2-161.5-84.6-172-101.7-21.1-36-70.8-45.2-56-62.3 35.4-18.2 71.4 4.6 113.1 66.3 30.2 44.7 89.1 37.2 119 29.7 6.5-26.9 20.5-50.9 39.7-69.6C248.8 728.2 181.7 630 181.7 513.2c0-56.6 18.7-108.7 55.3-150.7-23.3-69.3 2.2-128.5 5.6-137.3 66.5-6 135.5 47.6 140.9 51.8 37.8-10.2 80.9-15.6 129.1-15.6 48.5 0 91.8 5.6 129.8 15.9 12.9-9.8 77-55.8 138.8-50.2 3.3 8.8 28.2 66.7 6.3 135 37.1 42.1 56 94.6 56 151.4 0 117-67.5 215.3-228.8 243.7 26.9 26.6 43.6 63.4 43.6 104.2l0 128.8c0.9 10.3 0 20.5 17.2 20.5C878.1 942.4 1024 750.9 1024 525.3c0-282.9-229.3-512-512-512C229.1 13.2 0 242.3 0 525.2L0 525.2z" />
</svg>
</div>
        </a>
      </li></ul>
</div>
</div>
    </div><div class="site-info mt-2">
      <div>© Nick Shu. A Fool in the Making 2020,
        Powered by <a title="Jekyll is a simple, blog-aware, static site generator." href="http://jekyllrb.com/">Jekyll</a> & <a
        title="TeXt is a super customizable Jekyll theme." href="https://github.com/kitian616/jekyll-TeXt-theme">TeXt Theme</a>.
      </div>
    </div>
  </div>
</footer>
</div></div>
    </div><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $body = $('body'), $window = $(window);
    var $pageRoot = $('.js-page-root'), $pageMain = $('.js-page-main');
    var activeCount = 0;
    function modal(options) {
      var $root = this, visible, onChange, hideWhenWindowScroll = false;
      var scrollTop;
      function setOptions(options) {
        var _options = options || {};
        visible = _options.initialVisible === undefined ? false : show;
        onChange = _options.onChange;
        hideWhenWindowScroll = _options.hideWhenWindowScroll;
      }
      function init() {
        setState(visible);
      }
      function setState(isShow) {
        if (isShow === visible) {
          return;
        }
        visible = isShow;
        if (visible) {
          activeCount++;
          scrollTop = $(window).scrollTop() || $pageMain.scrollTop();
          $root.addClass('modal--show');
          $pageMain.scrollTop(scrollTop);
          activeCount === 1 && ($pageRoot.addClass('show-modal'), $body.addClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.on('scroll', hide);
          $window.on('keyup', handleKeyup);
        } else {
          activeCount > 0 && activeCount--;
          $root.removeClass('modal--show');
          $window.scrollTop(scrollTop);
          activeCount === 0 && ($pageRoot.removeClass('show-modal'), $body.removeClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.off('scroll', hide);
          $window.off('keyup', handleKeyup);
        }
        onChange && onChange(visible);
      }
      function show() {
        setState(true);
      }
      function hide() {
        setState(false);
      }
      function handleKeyup(e) {
        // Char Code: 27  ESC
        if (e.which ===  27) {
          hide();
        }
      }
      setOptions(options);
      init();
      return {
        show: show,
        hide: hide,
        $el: $root
      };
    }
    $.fn.modal = modal;
  });
})();
</script><div class="modal modal--overflow page__search-modal d-print-none js-page-search-modal"><script>
(function () {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    // search panel
    var search = (window.search || (window.search = {}));
    var useDefaultSearchBox = window.useDefaultSearchBox === undefined ?
      true : window.useDefaultSearchBox ;

    var $searchModal = $('.js-page-search-modal');
    var $searchToggle = $('.js-search-toggle');
    var searchModal = $searchModal.modal({ onChange: handleModalChange, hideWhenWindowScroll: true });
    var modalVisible = false;
    search.searchModal = searchModal;

    var $searchBox = null;
    var $searchInput = null;
    var $searchClear = null;

    function getModalVisible() {
      return modalVisible;
    }
    search.getModalVisible = getModalVisible;

    function handleModalChange(visible) {
      modalVisible = visible;
      if (visible) {
        search.onShow && search.onShow();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].focus();
      } else {
        search.onShow && search.onHide();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].blur();
        setTimeout(function() {
          useDefaultSearchBox && ($searchInput.val(''), $searchBox.removeClass('not-empty'));
          search.clear && search.clear();
          window.pageAsideAffix && window.pageAsideAffix.refresh();
        }, 400);
      }
    }

    $searchToggle.on('click', function() {
      modalVisible ? searchModal.hide() : searchModal.show();
    });
    // Char Code: 83  S, 191 /
    $(window).on('keyup', function(e) {
      if (!modalVisible && !window.isFormElement(e.target || e.srcElement) && (e.which === 83 || e.which === 191)) {
        modalVisible || searchModal.show();
      }
    });

    if (useDefaultSearchBox) {
      $searchBox = $('.js-search-box');
      $searchInput = $searchBox.children('input');
      $searchClear = $searchBox.children('.js-icon-clear');
      search.getSearchInput = function() {
        return $searchInput.get(0);
      };
      search.getVal = function() {
        return $searchInput.val();
      };
      search.setVal = function(val) {
        $searchInput.val(val);
      };

      $searchInput.on('focus', function() {
        $(this).addClass('focus');
      });
      $searchInput.on('blur', function() {
        $(this).removeClass('focus');
      });
      $searchInput.on('input', window.throttle(function() {
        var val = $(this).val();
        if (val === '' || typeof val !== 'string') {
          search.clear && search.clear();
        } else {
          $searchBox.addClass('not-empty');
          search.onInputNotEmpty && search.onInputNotEmpty(val);
        }
      }, 400));
      $searchClear.on('click', function() {
        $searchInput.val(''); $searchBox.removeClass('not-empty');
        search.clear && search.clear();
      });
    }
  });
})();
</script><div class="search search--dark">
  <div class="main">
    <div class="search__header">Search</div>
    <div class="search-bar">
      <div class="search-box js-search-box">
        <div class="search-box__icon-search"><i class="fas fa-search"></i></div>
        <input type="text" />
        <div class="search-box__icon-clear js-icon-clear">
          <a><i class="fas fa-times"></i></a>
        </div>
      </div>
      <button class="button button--theme-dark button--pill search__cancel js-search-toggle">
        Cancel</button>
    </div>
    <div class="search-result js-search-result"></div>
  </div>
</div>
<script>var SOURCES = window.TEXT_VARIABLES.sources;
var PAHTS = window.TEXT_VARIABLES.paths;
window.Lazyload.js([SOURCES.jquery, PAHTS.search_js], function() {
  var search = (window.search || (window.search = {}));
  var searchData = window.TEXT_SEARCH_DATA || {};

  function memorize(f) {
    var cache = {};
    return function () {
      var key = Array.prototype.join.call(arguments, ',');
      if (key in cache) return cache[key];
      else return cache[key] = f.apply(this, arguments);
    };
  }

  /// search
  function searchByQuery(query) {
    var i, j, key, keys, cur, _title, result = {};
    keys = Object.keys(searchData);
    for (i = 0; i < keys.length; i++) {
      key = keys[i];
      for (j = 0; j < searchData[key].length; j++) {
        cur = searchData[key][j], _title = cur.title;
        if ((result[key] === undefined || result[key] && result[key].length < 4 )
          && _title.toLowerCase().indexOf(query.toLowerCase()) >= 0) {
          if (result[key] === undefined) {
            result[key] = [];
          }
          result[key].push(cur);
        }
      }
    }
    return result;
  }

  var renderHeader = memorize(function(header) {
    return $('<p class="search-result__header">' + header + '</p>');
  });

  var renderItem = function(index, title, url) {
    return $('<li class="search-result__item" data-index="' + index + '"><a class="button" href="' + url + '">' + title + '</a></li>');
  };

  function render(data) {
    if (!data) { return null; }
    var $root = $('<ul></ul>'), i, j, key, keys, cur, itemIndex = 0;
    keys = Object.keys(data);
    for (i = 0; i < keys.length; i++) {
      key = keys[i];
      $root.append(renderHeader(key));
      for (j = 0; j < data[key].length; j++) {
        cur = data[key][j];
        $root.append(renderItem(itemIndex++, cur.title, cur.url));
      }
    }
    return $root;
  }

  // search box
  var $result = $('.js-search-result'), $resultItems;
  var lastActiveIndex, activeIndex;

  function clear() {
    $result.html(null);
    $resultItems = $('.search-result__item'); activeIndex = 0;
  }
  function onInputNotEmpty(val) {
    $result.html(render(searchByQuery(val)));
    $resultItems = $('.search-result__item'); activeIndex = 0;
    $resultItems.eq(0).addClass('active');
  }

  search.clear = clear;
  search.onInputNotEmpty = onInputNotEmpty;

  function updateResultItems() {
    lastActiveIndex >= 0 && $resultItems.eq(lastActiveIndex).removeClass('active');
    activeIndex >= 0 && $resultItems.eq(activeIndex).addClass('active');
  }

  function moveActiveIndex(direction) {
    var itemsCount = $resultItems ? $resultItems.length : 0;
    if (itemsCount > 1) {
      lastActiveIndex = activeIndex;
      if (direction === 'up') {
        activeIndex = (activeIndex - 1 + itemsCount) % itemsCount;
      } else if (direction === 'down') {
        activeIndex = (activeIndex + 1 + itemsCount) % itemsCount;
      }
      updateResultItems();
    }
  }

  // Char Code: 13  Enter, 37  ⬅, 38  ⬆, 39  ➡, 40  ⬇
  $(window).on('keyup', function(e) {
    var modalVisible = search.getModalVisible && search.getModalVisible();
    if (modalVisible) {
      if (e.which === 38) {
        modalVisible && moveActiveIndex('up');
      } else if (e.which === 40) {
        modalVisible && moveActiveIndex('down');
      } else if (e.which === 13) {
        modalVisible && $resultItems && activeIndex >= 0 && $resultItems.eq(activeIndex).children('a')[0].click();
      }
    }
  });

  $result.on('mouseover', '.search-result__item > a', function() {
    var itemIndex = $(this).parent().data('index');
    itemIndex >= 0 && (lastActiveIndex = activeIndex, activeIndex = itemIndex, updateResultItems());
  });
});
</script>
</div></div>


<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function scrollToAnchor(anchor, duration, callback) {
      var $root = this;
      $root.animate({ scrollTop: $(anchor).position().top }, duration, function() {
        window.history.replaceState(null, '', window.location.href.split('#')[0] + anchor);
        callback && callback();
      });
    }
    $.fn.scrollToAnchor = scrollToAnchor;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function affix(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroll,
        offsetBottom = 0, scrollTarget = window, scroll = window.document, disabled = false, isOverallScroller = true,
        rootTop, rootLeft, rootHeight, scrollBottom, rootBottomTop,
        hasInit = false, curState;

      function setOptions(options) {
        var _options = options || {};
        _options.offsetBottom && (offsetBottom = _options.offsetBottom);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroll && (scroll = _options.scroll);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $scrollTarget = $(scrollTarget);
        isOverallScroller = window.isOverallScroller($scrollTarget[0]);
        $scroll = $(scroll);
      }
      function preCalc() {
        top();
        rootHeight = $root.outerHeight();
        rootTop = $root.offset().top + (isOverallScroller ? 0 :  $scrollTarget.scrollTop());
        rootLeft = $root.offset().left;
      }
      function calc(needPreCalc) {
        needPreCalc && preCalc();
        scrollBottom = $scroll.outerHeight() - offsetBottom - rootHeight;
        rootBottomTop = scrollBottom - rootTop;
      }
      function top() {
        if (curState !== 'top') {
          $root.removeClass('fixed').css({
            left: 0,
            top: 0
          });
          curState = 'top';
        }
      }
      function fixed() {
        if (curState !== 'fixed') {
          $root.addClass('fixed').css({
            left: rootLeft + 'px',
            top: 0
          });
          curState = 'fixed';
        }
      }
      function bottom() {
        if (curState !== 'bottom') {
          $root.removeClass('fixed').css({
            left: 0,
            top: rootBottomTop + 'px'
          });
          curState = 'bottom';
        }
      }
      function setState() {
        var scrollTop = $scrollTarget.scrollTop();
        if (scrollTop >= rootTop && scrollTop <= scrollBottom) {
          fixed();
        } else if (scrollTop < rootTop) {
          top();
        } else {
          bottom();
        }
      }
      function init() {
        if(!hasInit) {
          var interval, timeout;
          calc(true); setState();
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState();
          });
          $window.on('resize', function() {
            disabled || (calc(true), setState());
          });
          hasInit = true;
        }
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions,
        refresh: function() {
          calc(true, { animation: false }); setState();
        }
      };
    }
    $.fn.affix = affix;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function toc(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroller, $tocUl = $('<ul class="toc toc--ellipsis"></ul>'), $tocLi, $headings, $activeLast, $activeCur,
        selectors = 'h1,h2,h3', container = 'body', scrollTarget = window, scroller = 'html, body', disabled = false,
        headingsPos, scrolling = false, hasRendered = false, hasInit = false;

      function setOptions(options) {
        var _options = options || {};
        _options.selectors && (selectors = _options.selectors);
        _options.container && (container = _options.container);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroller && (scroller = _options.scroller);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $headings = $(container).find(selectors).filter('[id]');
        $scrollTarget = $(scrollTarget);
        $scroller = $(scroller);
      }
      function calc() {
        headingsPos = [];
        $headings.each(function() {
          headingsPos.push(Math.floor($(this).position().top));
        });
      }
      function setState(element, disabled) {
        var scrollTop = $scrollTarget.scrollTop(), i;
        if (disabled || !headingsPos || headingsPos.length < 1) { return; }
        if (element) {
          $activeCur = element;
        } else {
          for (i = 0; i < headingsPos.length; i++) {
            if (scrollTop >= headingsPos[i]) {
              $activeCur = $tocLi.eq(i);
            } else {
              $activeCur || ($activeCur = $tocLi.eq(i));
              break;
            }
          }
        }
        $activeLast && $activeLast.removeClass('active');
        ($activeLast = $activeCur).addClass('active');
      }
      function render() {
        if(!hasRendered) {
          $root.append($tocUl);
          $headings.each(function() {
            var $this = $(this);
            $tocUl.append($('<li></li>').addClass('toc-' + $this.prop('tagName').toLowerCase())
              .append($('<a></a>').text($this.text()).attr('href', '#' + $this.prop('id'))));
          });
          $tocLi = $tocUl.children('li');
          $tocUl.on('click', 'a', function(e) {
            e.preventDefault();
            var $this = $(this);
            scrolling = true;
            setState($this.parent());
            $scroller.scrollToAnchor($this.attr('href'), 400, function() {
              scrolling = false;
            });
          });
        }
        hasRendered = true;
      }
      function init() {
        var interval, timeout;
        if(!hasInit) {
          render(); calc(); setState(null, scrolling);
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState(null, scrolling);
          });
          $window.on('resize', window.throttle(function() {
            if (!disabled) {
              render(); calc(); setState(null, scrolling);
            }
          }, 100));
        }
        hasInit = true;
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions
      };
    }
    $.fn.toc = toc;
  });
})();
/*(function () {

})();*/
</script><script>
  /* toc must before affix, since affix need to konw toc' height. */(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  var TOC_SELECTOR = window.TEXT_VARIABLES.site.toc.selectors;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window);
    var $articleContent = $('.js-article-content');
    var $tocRoot = $('.js-toc-root'), $col2 = $('.js-col-aside');
    var toc;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
    var hasToc = $articleContent.find(TOC_SELECTOR).length > 0;

    function disabled() {
      return $col2.css('display') === 'none' || !hasToc;
    }

    tocDisabled = disabled();

    toc = $tocRoot.toc({
      selectors: TOC_SELECTOR,
      container: $articleContent,
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      tocDisabled = disabled();
      toc && toc.setOptions({
        disabled: tocDisabled
      });
    }, 100));

  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window), $pageFooter = $('.js-page-footer');
    var $pageAside = $('.js-page-aside');
    var affix;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');

    affix = $pageAside.affix({
      offsetBottom: $pageFooter.outerHeight(),
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      scroll: hasSidebar ? $('.js-page-main').children() : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      affix && affix.setOptions({
        disabled: tocDisabled
      });
    }, 100));

    window.pageAsideAffix = affix;
  });
})();
</script><script>
  window.Lazyload.js(['https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js', 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js'], function() {
    var $canvas = null, $this = null, _ctx = null, _text = '';
    $('.language-chart').each(function(){
      $this = $(this);
      $canvas = $('<canvas></canvas>');
      _text = $this.text();
      $this.text('').append($canvas);
      _ctx = $canvas.get(0).getContext('2d');
      (_ctx && _text) && (new Chart(_ctx, JSON.parse(_text)) && $this.attr('data-processed', true));
    });
  });
</script>
<script type="text/x-mathjax-config">
	var _config = { 
		tex2jax: {
			inlineMath: [['$','$'], ['\\(','\\)']]
		}, 
		TeX: {
			extensions: ["action.js"]
		},
	};MathJax.Hub.Config(_config);
</script>
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<script>
  window.Lazyload.js('https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js', function() {
    mermaid.initialize({
      startOnLoad: true
    });
    mermaid.init(undefined, '.language-mermaid');
  });
</script>

    </div>
    <script>(function () {
  var $root = document.getElementsByClassName('root')[0];
  if (window.hasEvent('touchstart')) {
    $root.dataset.isTouch = true;
    document.addEventListener('touchstart', function(){}, false);
  }
})();
</script>
  </body>
</html>

